[{"title":"我是如何成为TA的","path":"/2025/03/15/我是如何成为TA的/","content":"唠嗑 必须承认，我是一个想法很多但行动不足的懒人。这有时候给我带来好处：身边的人不觉得我无趣，还恰当地因为我的懒而觉得我有分寸；有时候让我痛苦：抓不住想法的感觉就像溺水，不仅来不及大声呼救，更不知道向哪个方向求救。 说了这么多，其实只是为自己迟缓的更新做一点小小的辩解。写博客本来是件快乐的事情，尤其是当我得知我的破博客还有不少读者的时候。这时候敲下的文字就好像不只是某种排泄物，而是掏出来的心肝，或者是别的什么不那么恶心的东西。 最近发生了太多事情，距离上一次更新也过去了太久。2024年好像什么也没发生就已经过完了，但仔细想想还是做了不少事情，这之中又尤其有那么一两件值得我自豪地说道说道。于是我决定拿最不起眼的一件事情说说——我终于做了一次TA。 前因后果 在我还没有看清大学课程的时候总觉得助教是牛逼的——不大我几岁的人就能对课程内容和课外拓展信手拈来，写OJ搭平台有如家常便饭，改作业出lab还能带着小朋友玩梗吹牛——这实在是太酷了！ 于是在大学第一个学期结束的时候我悄悄决心要做一次TA，既是为了体验一下高人的感觉，也是怀着一点点能做一个有用的人的虚荣心。 当然，到了研究生，做事情就没那么酷了。在认领这个工作之前我列了一份pros&amp;cons： 当助教爽的地方 当助教不爽的地方 可以光明正大地水群 每周要跨校区通勤 可以接触最有活力的年轻人 可能会被私信轰炸 有钱拿 要干活 但我是一个很酷的人，很酷的人做事情是不会考虑后果的，所以我就接下来了。 感想 成就感 虽然本科都是上四年，但并不是每一年都有同样的地位。作为大一新生上的第一门编程课，我觉得做这门课的助教是有些讨巧的。很多时候我需要解决的疑惑都是很小的问题，不外乎编程的时候出现的各种逻辑问题和新手错误。很多时候即便我什么也没有做，同学们也都还是能想办法把事情给搞明白，最后还会感谢啥也没做的我。 大一新生身上总有一种奇妙的陌生感——在大学生中他们最不像大学生，既没有完全变成大学生，也不能完全不算是。和他们交谈我会想到我大一的窘态：不知道问什么，不知道做什么，也不知道怎么办。一些人来到陌生的环境后第一反应是寻求某种既存框架下的最优路径，他们和我谈论怎么选课，怎么刷分，怎么免修，找我要笔记（虽然我没有，也不会四处宣扬我的破博客）；更多的人则是想做些什么，却发现学校并不教这些东西。幸好我上的只是基础课程，还可以用打基础云云糊弄过去，然后尽可能在实验课里夹带一些比较有用的东西。 同事 当TA的另一个好处是可以接触到比较牛逼的同事。虽然不知道我有没有看起来像个能混入其中的牛逼的人，但至少我的同事们还是非常牛逼的。 虽然很多人都说读PhD需要多社交（虽然我不是），需要多了解别人的情况和课题，但只有真正和人交流之后我才知道即便是方向非常接近的同行也会有截然不同的烦恼，并且有时候这种烦恼在我看来是无法接受的；当然他们也会有让我羡慕的好，不过相比较起来就没有了解之前的那么好了。 这门课的另一个特色就是会有本科生助教，因此和小朋友聊天还是很好玩的。偶尔他也会问一些不知道如何是好的问题，幸好我已经长大到能够熟练地应付这种情况了，偶尔还能给出像模像样的回答，不致于像个什么都不懂的草包。 修锅 作为一门涉及非常多实验的课程，助教的活也是非常多的。通常来讲，我们需要负责作业、上实验课、课程网站的布置、群里发公告、出考试卷、改试卷，偶尔还要代一两节正课。 由于这门课已经上了六年，许多基础设施已经不是那么先进了（也缺乏维护），处处可以看到妥协的痕迹。并且由于之前接手的人不少已经跑路了，我也没法很好联系上。一些问题往往在发布前晚才被发掘出来，然后通过紧张而刺激的现场读log和上手改代码让整个屎山不至于垮掉，又不需要引入太多的workaround。我相信即便是这样的屎山也是由一代一代助教workaround的智慧凝结而成的。 关于本科一年级教育的思考 这个问题我和许多人都讲过，但总觉得只是涉及到皮毛，而我不过是个打工的硕鼠，也没有什么资格谈论教育这样的话题。 虽然很残酷，但绝大多数老师并不会与时俱进地改进他们的课程。往往是第一年迫于考核压力和某种“信念”把一门课弄出来，后面的几年就变成了按部就班。也许一门课在最初的几年会有些新意，但到了后面就变味了——太难的讲不懂，砍；太长的讲起来费力，砍；太杂的不好考试，砍。加之上课和教授的考核并不直接挂钩（很难想象名为“教授”的职业竟然不需要任何教学相关的资格证），自然也就没什么人在乎了。 当然也许有人要这么反驳我：上课本来就不能指望学到什么，真本事都是自己搞懂的。我没法说这种想法是错的，但谁又不想有个激情又前沿的领路人呢？","tags":["Eureka Moments"]},{"title":"DOM","path":"/2024/04/12/DOM/","content":""},{"title":"Futamura Projections","path":"/2024/03/19/Futamura-Projections/","content":"看论文时发现的潮词 对于一个程序 \\(f\\)，它的输入是 \\((a,b)\\)，输出是 \\(o\\)，我们写作 \\(f\\colon A\\times B\\to O\\) 如果存在一个奇妙程序 \\(s\\)，它能把给定的程序 \\(f\\) 和一部分输入 \\(a\\) 结合在一起，得到一个新的程序 \\(f_a\\)，那么我们就把 \\(s\\) 叫做 specializer，这里 \\(s\\colon F\\times A\\to F\\) 从几何的角度看，这就是一个高维向量在 \\(a\\) 处的投影，所以世人也叫 \\(s\\) 为 projection 如果 \\(f\\) 是一个解释器，\\(a\\) 是一段代码，\\(b\\) 是程序的输入，那么 type I \\[ \\forall b\\in B, s(f,a)(b)\\overset{\\text{def}}= f_a(b)=f(a, b) \\] 利用 specializer \\(s\\)，我们就从解释器 \\(f\\) 和代码 \\(a\\) 得到了可执行文件 \\(s(f, a) = f_a\\) type II \\[ \\forall a\\in A, s(s, f)(a)\\overset{\\text{def}}= s_f(a)=s(f, a)=f_a \\] 根据 type I，\\(f_a\\) 是可执行文件，所以这里 \\(s(s, f) = s_f\\) 是一个只要给出代码就能产生可执行文件的——编译器 我们就从 specializer \\(s\\) 和解释器 $ f$ 得到了编译器 \\(s_f\\) type III \\[ \\forall f\\in F,s(s, s)(f)\\overset{\\text{def}}= s_s(f)=s(s, f)\\overset{\\text{def}}=s_f \\] 根据 type II，\\(s_f\\) 是编译器，所以这里的 \\(s_s\\) 是一个只要给出解释器就能产生编译器的——元编译器！ 我们就从两个 specializer \\(s\\) 得到了 compiler generator \\(s_s\\)","tags":["Eureka Moments"]},{"title":"DB04 Storage","path":"/2024/03/01/DB04-Storage/","content":"开头讲了一些硬件小常识 一个有意思的表达： strawman idea: 翻译过来大概是“抛砖引玉”或者“初步的想法” Storage Hierarchy 简单来说，数据库的数据是这么存的： 一个数据库的数据划分存在许多文件里 一个文件里有许多 block/page 一个 block 里有许多 tuple 为了实现的方便，书上又做了一些假设： tuple 不会跨 block 储存 block 不会跨文件储存 Tuple 最简单的实现就是照猫画虎地序列化一个 C struct 需要注意： tuple 内部的值可以不定长 tuple 可以有 null value 解法也很简单，只需要加一个 tuple header 来存各个 attribute 的信息就行。通常这个 header 是定长的 Block 实际上要处理两个问题： 怎么访问有用的数据（比如遍历 block 内的所有 tuple） 怎么管理空闲的位置（比如找到任意一个空闲位置） 解法也很简单： 用数据结构组织已分配的位置（链表/数组/...） 用数据结构组织未分配的位置（链表/树/...） 因为插入和删除的位置可以是任意的，所以用数组就会很呆。如果做过 oslab 就会想到加 header 各种挤 metadata 的空间 fixed-length 这时候 fixed-length tuple 的假设就非常好了，回想 slab-page + bit flag 的设计，这里对应就是 block-tuple + bit flag variable-length 最简单的办法就是像 slab 一样把 block 分类，每类只装大小为 4B 8B 16B 32B... 的 tuple（大小不足的向上取整），这样就变成 fixed-length 的问题了 书上还列举了很多比较具体的做法，都不是很难 File 和 block 管理是类似的，不同之处在于一些假设 Unordered 即 tuple 的顺序无所谓，每个 block 都是同等地位的。这个和内存分配是一样的 Ordered 要求 tuple 之间按照某个 key 排序。由于目前的架构是 tuple-block-file 三层的，这实际上提出了两个要求： block 内部的 tuple 要(逻辑上)有序 file 内部的 block 要(逻辑上)有序 解决办法： 如果用数组就可以直接移动位置排序，如果用链表就可以操作引用来排序。总之是排个序 数组可以定时合并，链表可以横跨 block 排序 如果比我聪明的话应该马上就能想到用 B 树这样的数据结构来维护 树-结点-数据 这样的三层结构了。 Tricks prejoin：意思是把某些实际上相关的表提前 join 在一起 partitioning：意思是把某些表划分成子表，这些子表内部满足一些性质(比如有序)，以此利用查询时的局部性","tags":["Database"]},{"title":"DB03 SQL","path":"/2024/02/05/DB03-SQL/","content":"其实有点无聊，不过俺还从来没写过SQL，所以记一记 Basics 常用的语法是 SELECT expr1, expr2 ... FROM table1, table2 ... WHERE SEL-predicate GROUP BY col1, col2 ... HAVING GB-predicate 语义写成伪代码是 product(tables) |&gt; (eval exprs &lt;$&gt;) |&gt; (filter SEL-predicate) |&gt; (toMap columns) |&gt; (filter GB-predicate) 注意两个 filter 的顺序 还有一些集合上的操作和类似let binding的语法，以及一些表达副作用的更新操作 Others 上面感觉就已经足够cover绝大部分日常工作了 View 我的理解就是带 lazy evaluation 的全局 let binding，因为 SELECT 非常的纯真所以可以延迟求值的同时做一些优化 这里把 cached view 叫做 materialized view Transaction 我的理解是副作用需要一个东西来管理，SQL 就提供了最简单的类似 TX-begin TX-end 的结构来保证若干个副作用满足原子性和顺序 Constraints 书上说只有开销比较小的检查才支持，我感觉怪怪的.jpg not null 类似于类型修饰符，加在类型后面 unique 表达 super key 限制，和 primary 的区别在于可以 null constraint check(Predicate) 相当于一些简单的 assertion constraint 可以有自己的名字，并且可以随时添加/删除 Foreign key 可以用 foreign key (col_name) references talbe_name(ref_col_name) 来表达引用的取值范围（也就是一个泛型 ref&lt;T&gt;） 可以用 on delete/update cascade 来保证当被引用的 tuple 发生变更时，对应的引用也一起变更（也就是一个GC） Cyclic reference 这个问题还挺常见的，办法有这么几个 constraint 检查可以被推迟，例如出现相互引用的 foreign key 时可以在一个 transaction 内暂时不顾 referential integrity foreign key 可以设置为 null，变成 mutable 的形状 User Defined Types 看关键字的意思是还可以 non final(有 subtype)。很裤，不说话 Domain 我的理解就是 duck typing，更裤了 Others 还有很多很多细碎的 PL+DB 一句话：在通用编程语言(PL)里访问和操作数据库(DB) 按照我的理解粗暴分成两类 static/compiled/PreparedStatement：传给 DBMS 的查询是某种解析/优化过后的 IR dynamic/interpretive/Statement：直接给 DBMS 传 SQL 语句 这里的 static/dynamic 是相对于 DBMS 而言的，在 host 语言里可能全是动态行为(比如 JDBC 的字符串)，也可能是编译期行为(比如大名鼎鼎的 LINQ) 传 IR 的好处显而易见： 可以做优化 可以做安全性检查 对着这张图看 最左是 LINQ 那种编译期可以做 lint 检查的 embedded SQL 做法 中间是 JDBC 那种运行期间拼接字符串产生 SQL 查询，调用 API 的做法 最右的两个箭头展示了 static 和 dynamic 的区别(以 JDBC 为例，就是 PreparedStatement 和 Statement 的区别)","tags":["Database"]},{"title":"浅谈Verilog中的X","path":"/2024/01/30/浅谈Verilog中的X/","content":"打算做成一个系列，希望不要变成征讨檄文 做个标题党 不知从那里听来的……他认识一种值，名曰“X”，混沌所化，用if一算，就消释了…… X的作用似乎很多，可以表示未知、don't care、注入错误，但许多仿真器不带X玩也可以足够流行（说的就是你，verilator）。作为一个比较独特的语言特性，X必然要硅农花费时间熟悉。这篇文章希望带你10分钟知道和X有关的一切。 X 是什么 仿真 天地浑沌如鸡子，盘古生其中。万八千岁，天地开辟，阳清为天，阴浊为地。 知识告诉我们单纯的数字电路中只有0和1，实践告诉我们电路在刚上电（开机）的时候具有一些不确定性，这是电路物理特性决定的。 在编程语言中表达这种初始值的不确定性一般有这么几种路子 UB派 比如大家都喜欢的C 未经初始化的变量的值是未定义的 这句话中译中后是这样的 如果变量 v 没有被初始化，那么 v 的值可能是 0 也可能是 1 这么做的好处首先是简单，我们甚至不需要定义什么，只需要说它是未定义的 unknown派 比如大家最后都在写的SQL 未经初始化的变量的值是特殊值UNKNOWN 这句话定义了除0, 1外的新值，UNKNOWN，并规定未初始化的变量将持有UNKNOWN 敏锐的童鞋可能会问：引入了新的值，那么运算要怎么办呢？ 答案是传播UNKNOWN。简单来说，只要有一个操作数带了 UNKNOWN，那么结果就要体现出 UNKOWN 显然，verilog 选择了 UNKNOWN。 为什么 我斗胆猜测了一下原因 verilog 最早是用来验证和仿真的语言，引入X可以方便验证（写断言/写性质） 选择UB会引入额外的不确定性 X可以覆盖更多的情况，更加general 综合 “不知道！”他似乎很不高兴，脸上还有怒色了。 表达 do not care 的需求在手画电路图的工匠时代就已经产生了。某些综合工具会利用X的仿真语义，认为这代表了硅农在表示 do not care，进而做一些优化 always_comb begin if (sel) out = A; else out = 1&#39;bx; end 上面这个例子就可以被优化成 assign out = A; 验证 由于X在运算中会传播，因此X可以作为“污点”。当电路进入非法状态时，可以通过注入X来起警示作用。如果观察到了X，说明电路很可能出了问题 always_comb begin if (sanity_check_fail(current_state)) begin next_state = 32&#39;bx; end else begin // ... end end 结论 X表示不确定的值——“0或1” X不是仿真必须的——只是verilog选择了UNKOWN语义 X需要有配套的运算语义 X怎么样 X的语义 因为X是一个抽象值，所以我们需要给X运算的语义 定义 \\(\\gamma(\\texttt{X})=\\{0,1\\},\\gamma(\\texttt{0})=\\{0\\},\\gamma(\\texttt{1})=\\{1\\},\\gamma(s)=\\prod_{i}\\gamma(s_i)\\)，我们称 \\(\\gamma\\) 是一个具体函数，它将抽象的比特串映射回可能的01比特串。例如 \\(\\gamma(\\texttt{01X0})=\\{\\texttt{0110\\),\\(0100}\\}\\) 我们希望对于任意含X的比特\\(b_1,b_2\\)，应该有 \\(\\gamma(b_1\\oplus b_2)=\\{c_1\\oplus c_2\\mid c_1\\in\\gamma(b_1),c_2\\in\\gamma(b_2)\\}\\) 这给出了理论上运算符精确性的极限，我们当然希望越接近这个极限越好。 这里从语言标准里摘给几个运算让大家感受一下，可以验证哪些满足最优，哪些不满足 and initial begin A = 3&#39;bxxx; B = 3&#39;b01x; $displayb(A &amp; B); // 0xx end 可以发现标准还是做了一些bit操作的优化的，例如0&amp;X=0，值的表扬 add initial begin A = 3&#39;b00x; B = 3&#39;b110; $displayb(A + B); // xxx end 注意到无论A是000还是001，它和B作加法都不会影响高位，因此最好的结果应该是11X 手册中规定只要有一位是X，那么加法的结果都将是X array // store initial begin for (i = 0; i &lt; 256; i = i + 1) arr[i] = 0; arr[32&#39;bx] = 114514; for (i = 0; i &lt; 256; i = i + 1) $display(arr[i]); // 0 end // load initial begin for (i = 0; i &lt; 256; i = i + 1) arr[i] = 114514; $display(arr[32&#39;x]); // x end 手册规定： store的index带X，则忽略这次store load的index带X，则产生一个X 严格来说，arr[X]=val 应当导致后续所有对 arr 的读取都产生X condition always @* begin if (sel) out = A; else out = B; // (sel=X, A=0, B=1) -&gt; 1 end assign out = sel ? A : B; // (sel=X, A=0, B=1) -&gt; X 这就是比较著名的 X-optimism，难以想象一个 fundamentally flawed implementation 竟然也能给一个这么积极的名字，大伙真是起名的大师 pessimism assign out = sel ? A : B; // (sel=X,A=1,B=1) -&gt; 1 assign out =(sel &amp; A) | (~sel &amp; B); // (sel=X,A=1,B=1) -&gt; X 虽然二者功能等价，但在仿真时结果精度不一致，即使它们都完整地覆盖了所有可能的结果 结论 X作为仿真抽象值，其上定义的运算并非最精确（某些情况下是具体语义的over approximation） X作为仿真抽象值，其上定义的运算可能遗漏了具体执行时可能的结果（并非全都是safe approximation） 想法 开始夹带私货：假如要设计一个新的HDL，或者H别的什么L（说的就是你，chisel），我们要如何对待X？ 要不要 X 不要 我个人觉得 X 更像是 meta level 的东西。verilog之所以复杂，是因为它兼具仿真、综合、验证三种功能。 把 X作为值放进标准里，定死的运算语义使得仿真器不能做精确优化；验证和仿真都用同一套语言让具体值和抽象值只能搅在一起；引入的X-optimism还会隐藏某些bug，得不偿失 如果X作为了语言本身的一部分，那么任何针对X的分析都将变成 partial evaluation 怎么描述不确定性 UB人 把描述不确定性交给 meta level 的工具，例如linter/formal checker/analyzer。仿真只负责处理01值，又快又好 这也是目前verilator的思路。不过verilog仍然允许显式赋值X，因此verilator还需要打一个x-assign的补丁 怎么解决X-optimism 这其实是个抽象解释理论能解决的问题","tags":["Eureka Moments"]},{"title":"DB02 Relational","path":"/2024/01/23/DB02-Relational/","content":"Def 简单来说，我们称 \\(R\\subseteq D_1\\times D_2\\cdots\\times D_n\\) 这样一个 \\(n\\)-tuple 构成的集合 \\(R\\) 为为 \\(n\\)-relation over domain \\(D_1\\ldots D_n\\)，简称为 \\(n\\)-relation。简单地说，一个 relation 包含了所有使这个关系成立的元素（组），因此在数据库的语境下“关系”与“集合”是等价的。 直观来看，一个关系实为一张巨大的 \\(n\\) 列表，每行为一个满足该关系的元素，每列代表了一个属性。 需要注意的是，在数据库的语境下，“集合”通常指“无序可重集合”，即两个元素的比较是 identity equality 而非 domain-induced equality。因此后面讲到的 primary key/super key 的重要性也就很直观了，因为这里（可能）需要统计重复出现的元素。 这两个视角没什么区别，感觉也没啥好说的。 一些边角要求 通常 domain 有一个特殊的占位值，即取 \\(D_\\bot=D\\cup\\{\\bot\\}\\) 作为真正使用的 domain（群友最爱的 optional&lt;T&gt;） 一般要求 \\(D\\) 中的元素是原子的，例如 int；非原子的例子有 varchar、逻辑上的集合 这一切都看起来和命题演算那么熟悉，像啊，很像啊 Strict structure 关系这样非常固化的结构带来了一些后果： 处理起来非常简单 用起来也非常简单 不够灵活多变 Keys 数据库语境下的关系 \\((R,\\equiv)\\) 实际上是二元组，分别由定义域和 tuple 上的等价关系组成 以网课签到为例，一个签到记录由三元组 (学号，名称，地点) 组成。 一个人可以在不同地方签到多次，但只应被记录一次 两个人只要学号和名称不同时相等，那么他们的签到就应当分开算 primary 这里 (学号，名称) 就是一组 primary key，它唯一确定了一个 tuple 的 identity 比我聪明的读者会发现这也说明 签到表本质上是一个函数，即 (学号，名称) -&gt; 地点 以及 primary key constraint 本质上是 deterministic function constraint 接上条，数据的本质是一堆 primary key 间的映射 candidate 指极小的列集合 \\(K\\) 满足 \\(\\forall r_1,r_2\\in R,\\forall k\\in K, r_1\\text{.}k e r_2\\text{.}k\\rightarrow r_1\\text{.}k ot\\equiv r_2\\text{.}k\\) 说人话：只要 \\(K\\) 中的属性不全相同，就认为是不同的 tuple 此处 candidate 的意思是可以作为 primary key 的备胎 去掉极小限制后的 candidate 叫做 super key foreign/referential 考虑 tuple \\(t_1\\in R_1,t_2\\in R_2\\)，如果 \\(\\text{$t_1$.$k_1$=$t_2$.$k_2$}\\)，那么我们称 \\(k_1,k_2\\) 是 \\(R_1,R_2\\) 之间的 referential key。不失一般性地，如果 \\(k_1\\) 是 primary key，那么 \\(k_2\\) 就称为 \\(R_2\\) 到 \\(R_1\\) 的 foreign key 如果用 \\(\\sigma_k\\) 表示投影到 key \\(k\\)，那么约束写出来就是 \\(\\sigma_{k_1}(R_1)\\subseteq \\sigma_{k_2}(R_2)\\)，so easy 需要注意的是，某个 foreign key 可能也是 primary key，这二者并不矛盾 联谊 以大学生喜闻乐见的脱单为例，不妨假设有一个男女配对环节，表 B=(名字，身高)， G=(名字，院系) 和 N=(名字，twitter_id) 分别表示男女和非二元性别，并且名字都是 primary key 一张用来表示心动嘉宾的表 L=(名字，名字) 就由两个 foreign key 组成，foreign key constraint 的意义在于每个人的心动嘉宾应当出席了活动 一张用来表示心动身高的表 D=(名字，身高) 就由 foreign key 和 referential key 组成，因为身高并非 primary key。referential key constraint 的意义在于活动中确实有人长这么高 不难发现 foreign key 是 referential key 的一个特例。直观上看，foreign key 更好实现（目标明确），referential key 适用范围更广（海王） Relational algebra 非常的简单，书上介绍了这么几种： project：删去一些列 select：filter by predicate product：就是你想的那个 product join：product + select 的语法糖 rename/assign：term rewriting 的语法糖","tags":["Database"]},{"title":"DB01 Intro","path":"/2024/01/23/DB01-Intro/","content":"鸽了很久，再一次打算补上了。去年在家看网课的时候遇上了手机脱焊，希望今年能平安无事地搞完。 Intro DB 一般指有组织的数据，组织数据的软件则叫 DBMS。简单来说，DBMS 就是用户和数据之间的交互接口。 不难想象 DBMS 有很多应用，通常对 DBMS 的要求有这么几方面： consistency：对同一数据的多个拷贝修改，应当使结果保持一致 integrity：修改操作不应该破坏数据原有的语义 confidentiality：机密数据不应泄露 还有一些别的可以畅想的 Data model 讲的是用户和 DBMS 如何看待和描述 DB 中的数据。 以关系模型为例，DB 中的数据就是若干“关系”（或者有序集合），这些“关系”（集合）可以通过操作得到新的“关系”（集合）。用户通过提供“关系操作”来描述期望的查询结果。 relation 也叫 table，后者是更直观的名字。 Layering 就是你想的那样 物理层 逻辑层 用户层 DDL/DML Data Definition Language，说的是用来定义和描述数据的语言。用 DDL 而不是直接写裸数据有这么些原因： 描述数据的语义约束，例如身份证校验、邮箱格式 提供一些数据的元信息，例如权限、名称 易用 Data Manipulation Language，说的是用来操作数据的语言。操作包括大家都知道的增删查改，类型有大家喜爱的 declarative 和 procedural。 Schema/Instance 出现在这里显得很奇怪的两个词，找到的各种解释都不太满意 schema 给人的感觉是数据的组织结构，instance 就是你想的那个意思 DBMS structure 粗略地看，DBMS 和 OS 没有太大差别，都是一个 shell 用来交互用户和 kernel 状态，然后还要上并发和文件系统 Storage Management 负责 和存储设备交互 做一些 caching 权限检查 通常要实现一些特别的数据结构来提供这些服务 Query Management 负责 把查询翻译成数据查询操作 把 DDL 翻译成数据表示 做一些查询/修改的（语言上的）优化 简单地说，就是一个 SQL 编译器 Transaction Management 网红概念辨析 transaction 说的是实现了某个单一目标的一组操作。通常要求 Atomic：这一组操作要么全部完成，要么全部失败（实质是单一目标的原子性：要么成功，要么失败，没有中间态） Consistent：应该叫 consistency-preservation，即 consistent 的数据库在操作后仍然 consistent Isolation：多个目标对应的多组操作互不影响 Durability：操作的结果不会丢失 所以 Transaction Manager 需要负责 错误恢复 并发控制 和一些你懂得的活","tags":["Database"]},{"title":"SA04 Abstract interpretation","path":"/2024/01/02/SA04-Abstract-interpretation/","content":"整点抽象的 下面默认 \\(x\\le y\\) 意味着 \\(x\\) 比 \\(y\\) 更精确，\\(y\\) 比 \\(x\\) 更保守。 Galois connection 说的是这么一件事情： 如果给定了 程序的语义 \\(f_{s}\\) 和所有可能的执行状态 \\(\\Sigma_s=\\Set{\\sigma_s}\\) 分析的语义 \\(f_{a}\\) 和所有可能的分析状态 \\(\\Sigma_a=\\Set{\\sigma_{a}}\\) 指派函数 \\(\\alpha\\colon2^{\\Sigma_s}\\to\\Sigma_a\\)，将给定的执行状态集抽象成分析状态 指派函数 \\(\\gamma\\colon\\Sigma_a\\to2^{\\Sigma_s}\\)，将给定的分析状态还原为可能的执行状态集 并且有如下性质： \\(\\forall x,x\\subseteq\\gamma(\\alpha(x))\\)：抽象后再还原可能丢精度，但仍是保守估计（不会低估） \\(\\forall y,\\alpha(\\gamma(y))\\le y\\)：还原后再抽象不会损失精度 那么 \\((\\alpha,\\gamma)\\) 就称为一对 galois connection 需要注意，通常 \\(\\alpha\\) 是构造的，\\(\\gamma\\) 往往是 \\(\\alpha\\) 的伴随产物（这也意味着通常有 \\(\\alpha\\circ\\gamma=\\text{id}\\)），因此上面两个条件也在说： 抽象必须保守 抽象要尽可能避免丢精度 随机附赠的性质有： \\(\\alpha(\\bigcup A)=\\bigcup_{a\\in A}\\alpha(a)\\) \\(\\gamma(\\bigcap A)=\\bigcap_{a\\in A}\\gamma(a)\\) \\(\\alpha(\\bot)=\\bot\\)，\\(\\gamma(\\top)=\\top\\) Soundness 当我们说一个分析 sound 的时候，说的就是 \\(\\forall \\sigma, \\alpha(f_s(\\sigma))\\le f_a(\\alpha(\\sigma))\\) Optimality 对于给定的 \\(\\Sigma_a\\)，最精确的分析 \\(f_a\\) 由 \\(f_a=\\alpha\\circ f_s\\circ\\gamma\\) 给出 简单证明一下： 任取单调、sound \\(g\\colon \\Sigma_a\\to\\Sigma_a\\)，由 \\(g\\) sound 立刻有 \\(\\forall \\sigma_a\\in\\Sigma_a, \\alpha(f_s(\\sigma_a))\\le g(\\alpha(\\sigma_a))\\) 这说明 \\(\\forall \\sigma_s\\in\\Sigma_s, \\alpha(f_s(\\gamma(\\sigma_s)))\\le g(\\alpha(\\gamma(\\sigma_s)))\\)。根据定义，\\(\\alpha(\\gamma(\\sigma_s))\\le \\sigma_s\\)；\\(g\\) 单调，\\(g(\\alpha(\\gamma(\\sigma_s)))\\le g(\\sigma_s)\\) 串起来就是 \\(\\alpha\\circ f_s\\circ\\gamma\\le g\\)，由 \\(g\\) 的任意性可知 \\(\\alpha\\circ f_s\\circ\\gamma\\) 最优。 这告诉我们对于给定的格抽象，理论上总是存在一个最优的抽象解释器：粗暴地把抽象状态具体化，交给具体语义的解释器执行，最后把可能的结果收集起来再抽象。 这里有个比较重要的点是这样的：即便所有的原子操作语义都做到了最优抽象，它们的组合也可能达不到最优。 办法： 对组合后的原子操作建模，扩大上下文。比如直接对数组（ptr offset deref =&gt; array load）和对象（ptr offset deref =&gt; load field）操作整体建模 做一些语义等价但利于分析的程序变换。比如用易于分析的 IR、做优化","tags":["Static Analysis"]},{"title":"Tour de SAT","path":"/2023/12/30/Tour-de-SAT/","content":"最近对 SAT 求解器很感兴趣，来个三天带我入门 Basics why SAT 通用，意味着解决了它就能解决很多问题 重要，例如做硬件验证大量用到 SAT 建模问题 高情商：有理论上的意义 why PL 简单，存在判定算法 和一阶逻辑比起来虽然表现力不够，但足够解决一些实际问题 与有限结构的一阶逻辑等价 Quantified Propositional logic 除去大家都懂的部分，额外多了 \\(\\gamma\\mid R\\) \\(\\theta\\mid eg R\\) \\(\\forall P,\\alpha\\) \\(\\exists Q,\\beta\\) 它们分别被定义为 \\(\\gamma\\mid R=\\gamma[R\\mapsto \\bold{true}]\\) \\(\\theta\\mid eg R=\\theta[R\\mapsto \\bold{false}]\\) \\(\\forall P,\\alpha=(\\alpha\\mid P)\\wedge (\\alpha\\mid eg P)\\) \\(\\exists Q,\\beta=(\\beta\\mid P)\\vee(\\beta\\mid eg P)\\) 这方面一些关于知识表示/AI 的术语非常高大上 CNF \\[ \\bigwedge_{i}\\left(\\bigvee_{j} C_{j}\\right) \\] 一些术语： atom/literal：\\(P\\) or \\( eg P\\) clause：\\(\\vee\\) of atoms term：\\(\\wedge\\) of terms \\(\\alpha\\) satisfiable：存在一组解 \\(\\sigma\\) 使得 \\(\\sigma\\models\\alpha\\) Operations Normalization 说人话：把任给的公式“编译”到特定形式，一般默认是 CNF。这就是逻辑人的 IR。 CNF 是 universal 的，这意味着它有足够的表现力 可以通过一些基本操作得到 CNF \\( eg\\) 分配 \\(\\vee\\) 分配 \\( eg eg\\) 消去 也可以通过引入新变量构造 CNF why CNF： 简单 相较而言 DNF 下的 SAT 问题是平凡的 Decomposition 对于公式 \\(\\alpha\\)，可以提取变量得到 \\((P\\wedge(\\alpha\\mid P))\\vee( eg P\\wedge(\\alpha\\mid eg P))\\) 相当于把一个公式拆成并行的两份分别运算，最后用一个 MUX 挑出真正的结果 Resolution 对于形如 \\((P\\vee\\alpha)\\wedge( eg P\\vee\\beta)\\) 的公式，可以推导得到 \\((\\alpha\\vee\\beta)\\) 这样的操作叫 resolution between \\((P\\vee \\alpha)\\) and \\(( eg P\\vee\\beta)\\) over \\(P\\)，得到的结果 \\((\\alpha\\vee\\beta)\\) 叫 \\(P\\)-resolvent Completeness 就是你想的那个 completeness。 Resolution 并非 complete，考虑 \\(\\alpha=A\\)，它并不能 resolve 得到 \\( eg eg A\\) 但 resolution is refutation-complete，意味着如果 \\(\\alpha\\) 不可满足，那么对 \\(\\alpha\\) 做 resolution 将得到形如 \\(P\\wedge eg P\\) 的结果 这说明可以利用 resolution 判定公式是否不可满足 Unit resolution 也叫 Boolean propagation。对于只含一个变量的 clause \\(P\\)，为了保证可满足需要赋予 \\(\\sigma(P)=\\textbf{true}\\)。这里的 \\(P\\) 就叫 unit clause。 有点 simpl. 的感觉 Strategies 现在问题变成了： 可以挑一个变量对 CNF 做 resolution 可以挑一个变量猜一个值，猜错了回溯 由于所有的 solver 本质上都是爆搜+回溯，那么搜索策略就变得比较有讲究了： 挑哪个变量做 resolution？ 猜什么值？ Linear resolution resolve \\(\\alpha,\\beta\\) iff \\(\\alpha,\\beta\\) are roots, or \\(\\alpha\\) is the ancestor of \\(\\beta\\) Directed resolution(DP) 大名鼎鼎，而且很古老 指的是按照一个顺序 resolve 变量，每次只 resolve 那些没有被动过的 clause（不走回头路） 更具体地，给定变量集 \\(V\\) 上的顺序 \\(\\preceq\\)，定义 \\(c(P)=\\Set{\\text{$\\alpha$ is a clause}\\mid \\forall R\\in var(\\alpha), P\\preceq R}\\)，即所有出现了 \\(P\\) 但没有出现比 \\(P\\) 更小的变量的 clause。 算法是这样的： 初始化 \\(c\\) 按顺序枚举变量 \\(P\\) 两两 resolve \\(c(P)\\) 中的 clause 将结果更新到对应的 \\(c\\) 中 DPLL 名气更大 其实就是爆搜。枚举每个变量的取值后将得到一棵 dfs 树 每次搜完都利用 unit resolution 做一步化简，通过花费一点线性代价来换取搜索空间变小 对决策树做子树合并就得到了大名鼎鼎的 BDD 按照 DP 逆序操作带入也能还原出这棵决策树","tags":["SAT"]},{"title":"A subtle bug in a static analysis framework","path":"/2023/12/26/A-subtle-bug-in-a-static-analysis-framework/","content":"编译器的bug已经让人痛苦了，作为抽象的编译器——静态分析器里的bug只会更微妙、更隐蔽。 Intro 最近在关心基于抽象解释的单调分析框架和基于这类框架的分析算法设计，于是很自然地参考了一些著名的开源静态分析框架算法，在这期间我发现了一个这样的问题： 某个常量传播 evaluator 简化后的设计如下： def evalBinary(lhs, rhs, op): match (lhs, op, rhs) with | _, DIV, (Const 0) =&gt; UNDEF | (Const c1), _, (Const c2) =&gt; op.apply(c1, c2) | (Const 0), MUL, _ =&gt; (Const 0) | _, MUL, (Const 0) =&gt; (Const 0) | NAC, _, NAC =&gt; NAC | _, _, _ =&gt; UNDEF end. 可以发现这个函数不单调 这里于某次 commit 中引入了一个基于代数恒等式的简单优化，即 \\(\\forall x, 0\\times x=0\\)。简单思索之后可以发现这是不单调的！只需要考虑输入 \\((NAC, UNDEF), (NAC, 0)\\) 即可。 一个自然的疑问是：我们能利用上这个 bug 来做点有趣的事情吗？ A quick dive into monotonicity 首先需要知道为什么我能 claim 这是一个 bug 基于格抽象的静态分析框架通过抽象程序的执行状态、定义抽象状态上的状态转换、迭代执行抽象操作直至收敛 这三步完成一个抽象解释执行程序的艰巨任务。 而收敛（算法终止）的关键则在于对程序状态的抽象上——我们需要保证抽象状态的转换“不走回头路”，也就是不会兜圈子。 这种性质在数学上的刻画就叫单调性。如果失去了单调性，分析算法可能就不能结束输出结果，而是进入一个死循环。 Yes, but how? 再回头看我们究竟发现什么破坏了单调性：当某个变量的抽象值从 UNDEF 变为 0 时，它与 NAC 的乘积会从 NAC 变为 0。 想要构造出死循环，意味着我们需要构造至少两个彼此依赖的变量 \\(\\Set{x_1,x_2}\\) 使得： \\(x_1\\) UNDEF-&gt;0，\\(x_{2}\\) NAC-&gt;0 \\(x_{2}\\) NAC-&gt;0，\\(x_{1}\\) 0-&gt;UNDEF 一个个看怎么办： 当然是用 x2 = x1 * NAC 看起来有点麻烦 柳暗花明又一村 在找到反例之前我也以为这样的例子是不存在的，直到我的朋友告诉了我一个分析器中常见的优化小技巧：edge transfer 考虑如下代码： if (x == 1) &#123; return x; &#125; else &#123; return 1; &#125; if 的判断条件是带有额外信息的，这意味着在 true branch 内我们都可以安全地假设 x 的值要么是 UNDEF，要么是 1，而不可能是其它值。 形式化地讲，就是 \\([\\![x&#39;]\\!]=[\\![x]\\!]\\sqcap[\\![1]\\!]\\)，一个格上的 meet 操作。 利用这个 trick，我们就可以构造出程序使得某个变量的值发生 0 -&gt; UNDEF 的变化——又一处不单调！ 一个例子是这样的： if (x == 1) &#123; y = x - 1; &#125; else &#123; y = y / 0; &#125; 这里额外用到了一个 x / 0 = UNDEF 的 trick 当 x 在 0 和 NAC 之间震荡时，根据 meet 会有 x' - 1 在 UNDEF 和 0 之间震荡，这恰好就是我们想要的2 Putting all together 最终人肉综合出来的程序大概长这样 x = 1 / 0, y = NAC; while (true) &#123; z = x * y; if (z == 1) &#123; x = z - 1; &#125; else &#123; x = 1 / 0; &#125; &#125; 可以跑一跑得到这样的 trace： [x: UNDEF, y: NAC, z: UNDEF]@2 // 第一轮 [x: UNDEF, y: NAC, z: NAC] @3 [x: UNDEF, y: NAC, z: 1] @4 [x: 0, y: NAC, z: 1] @5 [x: UNDEF, y: NAC, z: NAC] @7 [x: 0, y: NAC, z: NAC] @8 // 第二轮 [x: 0, y: NAC, z: 0] @3 [x: 0, y: NAC, z: UNDEF] @4 [x: UNDEF, y: NAC, z: UNDEF]@5 [x: UNDEF, y: NAC, z: NAC] @7 [x: UNDEF, y: NAC, z: NAC] @8 对比就能发现循环两次之后的抽象状态完全相等，但每次迭代之后每个基本块对应的状态都会发生变化，因此不会停机 ...吗？ Analysis Framework: A bottom-up perspective 在进行了详尽的测试之后，存在这个 bug 的框架并没有如预想般死在我面前，而是成功停机了。 读了代码之后有如下观察： 框架通过一个 map 给每个基本块的入口和出口都分配了一个 AbsEnv 每次合并前驱计算转换时，通过枚举计算结果（Var-Value mapping）来更新出口 AbsEnv，此为 update 方法 AbsEnv 中通过 absent key 来表示 UNDEF 这意味着： 当出现了 0-&gt;UNDEF 的突变时，AbsEnv 会直接删掉这个元素 枚举更新的方式则会直接忽视所有 UNDEF，进而保证了与 UNDEF 相关的非单调性不会影响终止 所以很遗憾，这是一个被框架设计兜底了的 bug 想法 抽象解释的理论很简洁，但实践很复杂 静态分析器也可能会有微妙的 bug 并非所有 bug 都会显现，有时是框架擦了屁股 我们能像测试编译器一样测试静态分析器吗？","tags":["Eureka Moments"]},{"title":"SA03 narrowing & widening","path":"/2023/11/29/SA03-narrowing-widening/","content":"一句话：Widening 和 Narrowing 分别提供了对格上数据流分析的加速trick和精度trick。 Widening 某些格无限高，迭代算法不一定停机（即便最小不动点存在）。 数学意义上（超穷构造）的存在并不意味着可以通过停机的算法构造，一个经典的例子是收敛数列的极限。 修改格抽象 朴实的想法是对于太高的格 \\(S_h\\)，作一个新的更实际的格抽象 \\(S_l\\)，通过一个简单函数翻译 \\(w\\colon S_h\\to S_l\\) 修改转移函数 另一个想法是让迭代的转移函数跑得快些，这样可以得到一个收敛但不太精确的解。对于原本的转移函数 \\(t\\)，构造一个修正函数 \\(w\\)，使得 \\(w\\circ f\\) 的不动点更好算，并且 \\(w\\circ f\\) 的不动点是 \\(f\\) 不动点的一个安全估计。 聪明的读者可以发现第二种方法中的 \\(w\\) 类型会是 \\(S\\to S\\)，也是一个翻译函数 Requirements 回忆我们的目的： 构造一个修正函数 \\(w\\) \\(w\\circ t\\) 单调，可以迭代求不动点 \\(w\\circ t\\) 的不动点应当停机（或更快停机） \\(w\\circ t\\) 的不动点应当是 \\(t\\) 不动点的安全估计 这要求 \\(w\\colon S\\to S\\) \\(\\forall x, y\\in S, x\\le y\\Rightarrow w(x)\\le w(y)\\) \\(w(S)=\\Set{w(s)\\mid s\\in S}\\) 尽可能小 \\(\\forall x\\in S, x\\le w(x)\\)，这样才有 \\(\\forall x\\in S, t(x)\\le w(t(x))\\) 不妨记 \\(f_t, f_w\\) 分别为 \\(t, w\\circ t\\) 的最小不动点 一个简单粗暴的实现是——截断。就像模拟信号到数字信号的采样一样，多余的部分直接提升到 \\(\\top\\)，非常直观 于是就获得了一个 \\(w\\circ t\\) 这样好算简单的新函数，对它迭代就得到了一个原函数不动点的安全估计 Narrowing 指的是得到 \\(f_w\\) 后，继续计算 \\(t(f_w)\\) 的过程 注意到 \\(f_t=t(f_t)\\le t(f_w)\\le w(t(f_w))=f_w\\) 这说明 \\(t(f_w)\\) 仍然是 \\(f_t\\) 的安全估计，但不会比 \\(f_w\\) 更大。 实际上这样的迭代是是单调不增的，即从 \\(f_w\\) 出发求一个尽可能小的不动点，但并不一定收敛（此处迭代的是 \\(t\\)）。 这个过程精度不断提升，属于 more pay more gain 怎么样，是不是非常简单呢？","tags":["Static Analysis"]},{"title":"Cpp Lambda Quirks","path":"/2023/08/23/Cpp-Lambda-Captures/","content":"C++11 引入了匿名函数。类似 Java，C++ 中的匿名函数也是通过构造匿名类来实现的，不同之处在于 C++ 允许重载 operator()，因此可以实现看上去和函数一样的调用语法。本文主要关注 lambda 函数中一些反直觉行为，以及这些反直觉背后的直觉。 文章里关于手册的转述可能不够严谨，部分名词不知道怎么翻译，也可能存在时效问题，建议优先读一手资料。 例1 先来看一段代码 int g = 0; void ex1() &#123; auto l1 = []() &#123; return g + 1; &#125; auto l2 = [=]() &#123; return g + 1; &#125; auto l3 = [g=g]() &#123; return g + 1; &#125; g = 20; std::cout &lt;&lt; l1() &lt;&lt; &quot; &quot; // 21 &lt;&lt; l2() &lt;&lt; &quot; &quot; // 21 &lt;&lt; l3() &lt;&lt; &quot; &quot; // 11 &lt;&lt; std::endl; &#125; 乍一看这三个匿名函数没有区别，但注释里的输出表明它们是有细微的区别的。 查 cppreference 之后可以知道： 对应 l1()：匿名函数体内部可以直接引用全局变量而无需捕获 对应 l2()：对于默认值捕获，捕获对象为包裹匿名函数的 enclosing scope 中的 automatic variables，故代码中的 g 并没有进入捕获列表，此时等价于 l1() 对应 l3()：显式地捕获了全局变量 g 的值，内部对 g 的引用并非全局 这么做背后的原因有几个猜测： 全局变量在各个函数间都能访问，无需默认捕获，但需要的时候可以显式指明。此为“不需要则默认不做” 匿名函数的函数体实质上是一个单独的函数 operator()，在 callee 内无法引用 caller 的局部变量，但可以将这些局部变量作为函数参数传入，恰好对应按值/按引用捕获。此为“必须做的不做不行” 引用捕获则要麻烦得多，需要能证明生命周期的嵌套。更多时候更无脑的办法是用 shared_ptr 来辅助管理 例2 auto factory1(int x) &#123; return [=] &#123; static int y = 0; return (++ y) + x; &#125;; &#125; void ex2_1() &#123; auto l1 = factory1(114); auto l2 = factory1(514); std::cout &lt;&lt; l1() &lt;&lt; &quot; &quot; // 114 &lt;&lt; l2() &lt;&lt; &quot; &quot; // 515 &lt;&lt; std::endl; &#125; 这个例子表明 static 关键字的含义仍然没有发生变化。由于匿名函数的实现是匿名类，因此 static int y 仍然是相对于 operator() 这个函数而言的静态变量，y 是在多个匿名函数之间共享的 一个更能佐证的例子是这样的： auto factory2(int x) &#123; return [=] &lt;typename T&gt; (T z) &#123; static T y = 0; return (++ y) + x; &#125;; &#125; void ex2_2() &#123; auto l1 = factory2(114); auto l2 = factory2(514); std::cout &lt;&lt; l1(0) &lt;&lt; &quot; &quot; // 114 &lt;&lt; l2(0.) &lt;&lt; &quot; &quot; // 514 &lt;&lt; std::endl; &#125; 这里的两个调用会产生两个匿名函数，它们对应于两个不同的实例化后的匿名类，因此会分别有 &lt;int&gt;::operator() 和 &lt;double&gt;::operator() 两个函数体，自然也就有两份静态变量 例3 如果希望向 SICP 里那样用闭包保存状态该怎么做？ void ex4() &#123; auto l1 = [state=INITIAL_STATE] (int input) mutable &#123; switch (input) &#123; // transitions ... &#125; &#125;; &#125; 匿名函数的 operator() 默认是 const 的，即内部不能更改捕获列表中的内容，如果希望有状态的跳转需要标记 mutable 值捕获本质上是用当前作用域中的表达式初始化匿名类的成员，此处相当于声明储存状态的成员 不能用 static，原因你懂的","tags":["Cpp"]},{"title":"SA02 lattice","path":"/2023/08/11/SA02-lattice/","content":"因为懒，所以具体的含义请读之前仔细认领一下，不对符号重载滥用造成的误解负责，但可以戳我... Def posets &amp; bounds 给定集合 \\(S\\) 与其上一个二元偏序关系 \\(\\le\\)，这样的结构 \\((S,\\le)\\) 称为偏序集（poset） 通常这样的偏序关系是带有信息（方向）的，即规定 \\(x\\le y\\) 表明 \\(x\\) 比 \\(y\\) 更精确（或 \\(y\\) 是 \\(x\\) 的估计）。 元素间的偏序关系可以推广到集合上，即对于 \\(X\\subseteq S\\)，\\(X\\le y\\) 定义为 \\(\\forall x\\in X, x\\le y\\)，此时称 \\(y\\) 为集合 \\(X\\) 的一个上界；关于下界的讨论是类似的。 上界可能有零个或多个。如果存在上界，那么可以讨论最小上界。将 \\(X\\) 的最小上界记为 \\(\\text{lub}(X)\\)，它应当满足 \\(\\forall y\\in S, X\\le y\\Rightarrow \\text{lub}(X)\\le y\\)。 lattice 如果任取 \\(x,y\\in S\\) 都满足： 存在 \\(z_1\\in S\\) 使得 \\(z_1=\\text{lub}(\\Set{x,y})\\) 存在 \\(z_2\\in S\\) 使得 \\(z_2=\\text{glb}(\\Set{x,y})\\) 即 \\(\\text{lub, glb}\\) 是 \\(\\Set{X\\in 2^S\\mid |X|=2}\\) 上的全函数，那么称 \\((S,\\le)\\) 是格（lattice），并规定 \\(x\\cup y=\\text{lub}(\\Set{x,y})\\) \\(x\\cap y=\\text{glb}(\\Set{x,y})\\) 如果对于任意的 \\(X\\subseteq S\\) 都存在 \\(\\text{lub}(X)\\) 和 \\(\\text{glb}(X)\\)，即 \\(\\text{lub, glb}\\) 是定义在 \\(2^S\\) 上的全函数，那么称 \\((S,\\le)\\) 是全格。 注意这里并没有构造地给出 join 和 meet，而只是表明了它们应有的性质 有限格必然是全格 任取 \\(x, y, z\\)，我们 claim \\(\\text{lub}(\\Set{x,y,z})=(x\\cup y)\\cup z\\)。 首先证明这是良定义的。注意到 \\(\\text{lub}\\) 是定义在集合上的，因此 \\(x,y,z\\) 应当有轮换对称性。而 \\((x\\cup y)\\cup z\\) 是 \\(\\Set{x,y,z}\\) 的一个上界，因此也是 \\(\\Set{y,z}\\) 的一个上界，有 \\(y\\cup z\\le (x\\cup y)\\cup z\\) 和 \\(x\\le (x\\cup y)\\cup z\\)，这说明 $ x( y z)(x y)z$，类似可以证得 \\(x\\cup(y\\cup z)=(x\\cup y)\\cup z=(x\\cup z)\\cup y\\)，故这个取法是良定义的 接着证明这是最小上界。任取 \\(\\Set{x,y,z}\\) 的上界 \\(t\\)，根据定义有 \\(x\\le t, y\\le t,z\\le t\\)，因此有 \\((x\\cup y)\\cup z\\le t\\)。由 \\(t\\) 的任意性可知 \\((x\\cup y)\\cup z\\) 是三者最小上界。 然后就可以愉快地根据集合大小归纳了，\\(S\\) 的有限性保证了归纳法的正确。 更进一步，\\(\\text{glb, lub}\\) 二者只需要其一就能导出另一个 若偏序集 \\((S,\\le)\\) 上有全函数 \\(\\cup\\)，那么必然也存在全函数 \\(\\cap\\) 记 \\(L=\\Set{y\\in S\\mid y\\le X}\\)，取 \\(\\bigcap X=\\bigcup L\\)，即取出集合 \\(X\\) 所有的下界，然后找到这些下界的最小上界，它就是 \\(X\\) 的最大下界。 最大是显然的。下界：\\(\\bigcap X\\) 是 \\(X\\) 所有下界的最小上界，这说明 \\(\\forall z\\in S, L\\le z\\Rightarrow \\bigcap X\\le z\\)。可以发现此处满足条件的 \\(z\\) 包含了全部 \\(X\\) 中的元素，这说明 \\(\\bigcap X\\) 是 \\(X\\) 的一个下界。 如果是有限全格，那么必然存在全局最大和最小值，通常记为 \\(\\top, \\bot\\)。最长的形如 \\(\\bot\\le\\cdots\\le\\top\\) 的链的长度称为格的高度。 Common lattices 鉴于这本书实际上想讲的是静态分析，所以通常只关心有限格，而通常它们又都是全格。可以开动脑筋想想介绍这些特殊格的动机。 任给集合 \\(S\\) 都自动构成一个格 \\((2^S,\\subseteq)\\)。 定义 \\(flat(S)=(S\\cup\\Set{\\bot,\\top}, (\\Set{\\bot}\\times S)\\cup (S\\times\\Set{\\top}))\\)，即只有 \\(\\top, \\bot\\) 可以比较的格。 给定若干格 \\(\\Set{(L_n,\\le_n)}\\)，那么 \\((\\prod_{i=1}^n L_i,\\le&#39;)\\) 也是格，其中 \\((\\ldots x_i\\ldots)\\le&#39;(\\ldots y_i\\ldots)\\) 定义为 \\(\\forall i, x_i\\le_i y_i\\)。这个叫 product lattice 给定集合 \\(D,S\\) 和函数集 \\(F= S^D\\)，若 \\((S,\\le)\\) 是全格，那么 \\((F, \\le&#39;)\\) 也是全格，此处 \\(f\\le&#39; g\\) 定义为 \\(\\forall x\\in D, f(x)\\le g(x)\\)。这个叫 map lattice 给定全格 \\((S,\\le)\\)，那么 \\((S\\cup\\Set{\\bot}, \\le&#39;)\\) 也是全格，此处 \\(\\le&#39;\\) 的定义就是你想的那样。这个叫 lift lattice Properties 下面默认 \\(f\\colon S\\mapsto T\\)，\\((S,\\le)\\) 和 \\((T,\\le&#39;)\\) 都是全格 Distributive 若 \\(\\forall x, y \\in S\\)，\\(f(x\\cup y)=f(x)\\cup f(y)\\)，那么称 \\(f\\) 可分配。 Extensive 任取 \\(x\\in S\\)，\\(x\\le f(x)\\)。 注意和单调性作区分。 Monotone 若 \\(\\forall x, y\\in S, x\\le y\\Rightarrow f(x)\\le f(y)\\)，则称 \\(f\\) 是单调的，含义为给 \\(f\\) 更精确的输入不会让结果更不精确。 单调的定义可以拓展到多参输入，这里只需要对单个参数单调即可。 单调函数在复合运算下封闭 这个是显然的。 常函数单调 这个也是显然的。 集合差操作 \\(\\backslash\\) 非单调 这个也是显然的。 话说这个能不能叫协单调/逆单调（认真脸 \\(\\cap, \\cup\\) 都单调 固定 \\(x, y\\)，任取 \\(z\\) 假设 \\(x\\le z\\)，那么 \\(x\\le z\\le z\\cup y, y\\le z\\cup y\\) ，故 \\(z\\cup y\\) 是 \\(\\Set{x, y}\\) 的上界，由 \\(x\\cup y\\) 的定义即得 \\(x\\cup y\\le z\\cup y\\) 由对称性直接得到 \\(\\cap\\) 的讨论是类似的。 可分配函数单调 若 \\(x\\le y\\)，则 \\(x\\cup y=y\\) 带入定义有 \\(f(y)=f(x\\cup y)=f(x)\\cup f(y)\\)，而 \\(f(x)\\le f(x)\\cup f(y)=f(x\\cup y)=f(y)\\) \\(f\\) 单调的一个充要条件是 \\(\\forall x,y\\in S\\)，\\(f(x)\\cup f(y)\\le f(x\\cup y)\\) \\(\\Rightarrow\\)： 显然有 \\(x\\le x\\cup y, y\\le x\\cup y\\)，由单调性得 \\(f(x)\\le f(x\\cup y), f(y)\\le f(x\\cup y)\\) 又 \\(f(x)\\cup f(y)\\) 是 \\(\\Set{f(x), f(y)}\\) 的最小上界，因此 \\(f(x)\\cup f(y)\\le f(x\\cup y)\\) \\(\\Leftarrow\\)： 不妨设 \\(x\\le y\\)，那么 \\(x\\cup y = y\\)，带入有 \\(f(x)\\cup f(y)\\le f(x\\cup y)=f(y)\\) 这说明 \\(f(y)\\) 是 \\(\\Set{f(x), f(y)}\\) 的上界，根据定义 \\(f(x)\\le f(y)\\)。 给定单调函数 \\(f\\colon L_1\\mapsto(A\\mapsto L_2)\\)，\\(g\\colon A\\mapsto L_2\\)，其中 \\(L_1, L_2\\) 都是全格，\\(A\\) 是定义域，\\(A\\mapsto L_2\\) 是 map lattice。对于给定的 \\(a\\in A\\)，证明 \\(h(x)=f(x)[a\\mapsto g(x)]\\) 单调 不妨假设 \\(x\\le y\\)，我们需要证明 \\(f(x)[a\\mapsto g(x)]\\le f(y)[a\\mapsto g(y)]\\)，而它的定义则是 \\(\\forall w\\in A, f(x)[a\\mapsto g(x)](w)\\le f(y)[a\\mapsto g(y)](w)\\) 如果 \\(a=w\\)，那么我们只需要证明 \\(g(x)\\le g(y)\\)，而 \\(g\\) 单调； 如果 \\(a e w\\)，那么我们只需要证明 \\(f(x)(w)\\le f(y)(w)\\)，而 \\(f\\) 单调； 综上，\\(h\\) 单调。 这个的证明像在写 coq Fixed point 抽象解释本质上是在构造一组抽象映射 \\(\\sigma\\) 的方程，最后我们希望能求出一个在精确意义上最优的解。 即对于一系列方程 \\[ \\left\\{ \\begin{align*} \\sigma_1&amp;=f_1(\\sigma) \\\\ &amp;\\cdots \\\\ \\sigma_n&amp;=f_n(\\sigma) \\\\ \\end{align*} \\right. \\] 其中 \\[ \\sigma=(\\ldots \\sigma_i \\ldots) \\] \\(\\sigma_i\\) 表示在 program point \\(i\\) 处的抽象映射。把 \\(\\Set{f_i}\\) 视为一整个大函数 \\(F\\)，则上面的方程组实质上是 \\[ \\sigma=F(\\sigma) \\] 即我们要求 \\(F\\) 的不动点。解可能有多个，最好是精确意义上最优的解。 Iteration 喜闻乐见的迭代，考虑 \\(\\bigcup_{i&gt;0} f^i(\\bot)\\)，收敛性、正确性和解的最优性质都是比较简单的。解方程的部分还有一些和线性规划类似的把不等号改写成等号的技巧。 实际的 solver 写起来有各种各样的技巧，比如利用单调性避免重复计算/根据CFG的结构选择计算顺序/SCC缩点后按照拓扑序计算等等","tags":["Static Analysis"]},{"title":"SA01 Type","path":"/2023/08/04/spa01-Type/","content":"看的是Anders Moller那本教材，感觉还挺全面的 Background 对一段程序赋予类型的过程也可以视为一种静态分析 可能出现错误（后面详细解释）的程序将无法赋予类型/通过类型检查 程序本质是图灵机（partial function），错误本质是未定义的转移或状态（stuck）、人为规定的转移和状态（比如invalid information flow） Rice定理、不可判定、approximation Describing types 简单的有穷类型很容易定义，考虑怎么定义递归类型。一个程序例子如下： def foo(ptr, n): if n == 0: return 1 return ptr(ptr, n - 1) + 1 foo 的类型理应有如下形式，框表示还不知道是个啥： \\[ \\text{foo}\\colon\\square\\to\\bf{int}\\to\\bf{int} \\] 注意到这里的 ptr 实质上可以指向 foo 本身，令 \\(\\square\\) 为 foo 的类型作一次替换即得到： \\[ \\text{foo}\\colon(\\square\\to\\bf{int}\\to\\bf{int})\\to\\bf{int}\\to\\bf{int} \\] 可以发现这样的替换能一直做下去。这样虽然无穷、但本质只有有限种子项类型（finite subterms）的类型称为正则类型，名字源于正则语言和正则树。 通常用 \\(\\mu\\) constructor 来表达这样的正则项，即上面的类型可以写成 \\[ \\mu \\tau.\\tau\\to\\bf{int}\\to\\bf{int} \\] Constraint-solving typing procedure Well-typed programs cannot go wrong. 针对每一种 AST 节点都产生一种类型约束，以表达式的语义动作为例： E : E1 + E2 &#123;type(E1) = type(E2) &amp;&amp; type(E1) = int&#125; | E1 &amp; E2 &#123;type(E1) = type(E2) &amp;&amp; type(E1) = bool&#125; | E1 [ E2 ] &#123;type(E1) = array &amp;&amp; type(E2) = int&#125; 约束 \\(C\\) 是形如 \\(\\tau_1=\\tau_2\\) 的等式集 一组替换（substitution）是 \\(TVar\\mapsto T\\) 的偏函数,如果 \\(\\forall \\tau_1=\\tau_2\\in C, \\sigma(\\tau_1)=\\sigma(\\tau_2)\\)，那么称替换 \\(\\sigma\\) 是 \\(C\\) 的解 此处 \\(\\sigma(\\tau)\\) 定义为将 \\(\\tau\\) 中自由出现的类型替换为映射后的像 考虑两组解 \\(\\sigma_1,\\sigma_2\\)，若存在序列 \\((V_1,\\tau_1)\\ldots\\) 使得 \\(\\sigma_1[V_1\\mapsto \\tau_1]\\cdots=\\sigma_2\\)，则称 \\(\\sigma_1\\) 比 \\(\\sigma_2\\) 更通用（general）。直观的解释就是“\\(\\sigma_1\\)能作为解的地方，\\(\\sigma_2\\)也能；反过来则不一定成立” Unification 简单的并查集应用 注意到约束实质上是等价关系、约束要求两侧的对应子项仍然保持约束，因此最终解必然是若干类型上的等价类，并且代表元都是具体类型 此时把每个类型变元的解设置为其所属等价类的代表元就行 Recursive types 上述基于并查集的解法并不能处理递归类型，一个例子如下： p = alloc null *p = p 这段代码的含义为： 为 p 分配一段地址，这段地址被初始化为空指针 null 将 p 的值（一个指针）存入 p 所指向的内存中 分析一下就会得到长成这样的约束集 \\[ \\left\\{ \\begin{align*} \\text{null}&amp;={\\bf{ptr}\\text{ }} \\alpha \\\\ \\text{p}&amp;=\\text{null}\\\\ \\text{p}&amp;={\\bf{ptr}}\\text{ p} \\end{align*} \\right. \\] 考虑这个约束： \\[ \\alpha={\\bf{ptr}}\\text{ }\\alpha \\] 想想我们都干了什么：把一个指针作为值存入了它指向的内存，这说明它是一个指向自己所属类型的值的指针，套娃了 可以发现这样的约束在求解过程中不会影响其它约束（why?）且会保留到最后（why?），因此只需要先解一遍，然后找到这种形式的解人为改写成 \\(\\mu\\) 构造子的形式就好了 Limitations 定义 Slack 为无法通过检查但仍然不会发生错误的程序集，根据 Rice 定理此处 Slack 必然非空，我们来看看它里面都会有哪些情况。 Flow-sensitive typing var x var y = input() if y == 1 &#123; x = &quot;1&quot; &#125; else &#123; x = 2 &#125; 对于不同的控制流分支，x 将拥有不同的类型。上述类型检查算法并没有考虑顺序和控制分支的情形。 let-polymorphism first :: a -&gt; a -&gt; a first x _ = x append (first [1, 2] [&quot;1&quot;, &quot;2&quot;]) (first 3 &quot;3&quot;) 对于不同的调用点，我们希望实例化出不同的类型：此处的两个 first 实质上拥有完全不同的类型","tags":["Static Analysis"]},{"title":"大三下荒唐实录","path":"/2023/08/03/大三下荒唐实录/","content":"今年非常懒，决定不再按照月份写流水帐了。 上课 中了非常热门的协议开发 渲染和组合数学，又选上了tls的编译，可以说是人品大爆发了 在四月初的时候因为要赶别的事情退了几门，于是成绩单就出现了很难看的无效分数 编译的实验有点东西但不多。渲染的实验能感受到是纯粹凑数赶出来的，无论是debug还是测试都很痛苦，只能嗯推式子+干瞪眼。协议开发的lab内容很多（所以我跑路了）。组合的作业确实有难度，但感觉分数被期末考试支配，只要别空题就行。 大三的课程大多很随意，大家也都很忙，没必要死磕或者找不愉快，毕竟分数不会算进申请GPA排名、全勤影响实习面试、刷lab也不怎么能提升水平了。一个字：摸 打工 写论文和做实验的时候学了很多，想了很多，但感觉都不太能说，最后也没有坚定地选择读博。 学期中的时候参加了一个不大的会议，印象是盒饭还不错，公司很热情，展示的水平就参差不齐，拉胯者居多。于是又想了很多。 生活 这学期出门吃饭和玩耍的次数显著增多了，由此可见2h门禁的取消确实带来了更多自由。 也体验了几次彻夜不归的鬼混（其实只是为了唱歌打折和吃特价海底捞），觉得某术壬鼓吹的所谓“刷夜”不过是精神小伙日常。 人际关系有一些处理不好的地方，但是也不是很有动力处理好，拉倒吧 保研梦 以下内容都是我在梦境中的体验，因这份体验过于真实，特此与诸位有缘人分享。如与现实中的人或事有雷同，纯属巧合。 后文会按照梦中的时间顺序回忆这两个月内梦境的重要节点，准确性不保证，真实性为零 仲夏夜营梦 从一些陶瓷壬那里听来说今年直博/硕的夏令营要分开，往年的“读硕不成转直博”将不会是可选项。 仲夏营报名通道开放，导梦员的口径是班上政策未知，先报名了再说。某的意思是推荐大家都勃。 报名结束前突然通知勃需要导师签字，并且在端午前就要。 某开会表示已经勃了的可以在考完后再参加班上的考核，同时强调班上的考核只发硕鼠，并且是计/软各一半。还提出班内没勃的需要单独考核，不能食我粟。 参加完因不存在的疫情而被迫线上的笔试后全班没人收到入营通知，随后一直不通知具体的考核日期，也没有相关的文件发布，这样的情形一直拖到了硕鼠面试结束。某开疼疼会议确定了GPA计算范围，同时发布了考核的指导文件（5分制GPA与5分制现场考核按照70%和30%的比例算分排名，线下考核包括面试和机试）。当晚班上开启了一轮冲锋，争议的焦点主要在于换算后机试5分即可翻盘，这样的比例是否区随机程度太大。 次日某宣布鸡鸡硕鼠名额5个，其中至多2个学鼠，班上开启了第二轮冲锋，争议的焦点主要在于学鼠名额是否太少。某提出这是因为班里的同学“鼠源甚至不如大班”因此“不好意思要名额”。 考核前两天某宣布机试为1h2题，额外加一场开卷笔试，内容为栎散鼠穴。 梦醒时分 机试1h2题的sb操作不评价，考场没有gdb的操作也不评价。相比较而言，硕鼠营是2h3题（实际执行是2.5h3题），勃营也是2h3题。 最后考完出来问了一波分数，top3的差距都在10分以内 面试感觉就是临时拉壮丁，甚至找了两个软软的叫兽，问了一些意义不明的问题。根据我手机上的梦话记录，推断如下： 英文介绍最喜欢的课程 嗯背了一段，没太听清梦里说的啥 直观上来说，一张图中有些节点位于比较中间的位置，你觉得有哪些衡量一个点位于中心的程度的度量？ 我说离心率、是不是割点应该都能衡量。出来之后想想这个“中间”应该也可以指“重要程度”，那么page rank这种也应该可以说 怎么求割点？ 我想着总不能上来就放大招，就说删掉看是不是联通。对面嫌弃这个太慢了能不能更快，就把tarjan背了一遍。 面向过程的语言和面向对象的语言能不能互相模拟？ 背了一段cfront 函数式和命令式能力是否等价？ 背了一段邱奇图灵论题 讲讲你的科研经历 又背了一段稿子 讲讲你的一个项目 把英文介绍那里的又讲了一遍，但感觉对面没咋听懂 如果读研后工作没及时完成咋办 我心想如果做研究都有个ddl的话那岂不是P=NP问题的解决指日可待了？ 但是客套话该说还得说，就背了一段客套话 戒之慎勿忘 说穿了自己还是不敢读直博，别的都没啥了。这个sb班虽然sb，但对铁了心要读博的人都还是很优惠的。 人事就是人事，20个人的班说大不大，说小也不小，背后没人硬挺自然是爹不疼娘不爱的。孤儿！ 班里的全体水平确实不如大班，但这话轮不到我说。对这些铁了心要做软软的同学我能做什么呢？可恨！对那些不得不去做软软的同学我又能做什么呢？可悲！","tags":["Eureka Moments"]},{"title":"LLHD 踩坑记","path":"/2023/06/14/LLHD-踩坑记/","content":"最近要用到这个 Moore Moore 是生成 LLHD 代码的一个前端，支持一部分的 SystemVerilog 语法，用 rust 写的 Build rustc 首先根据 issue #251，我们需要切换到 1.62.0 的rustc 具体操作是这样的(默认用rustup管理你的rust工具链) cd $MOORE_PROJECT_DIR rustup override set 1.62.0 如果希望全局修改，那么就用 rustup default 1.62.0 Fibers ... exceptions ... 这个问题 CIRCT 仓库已经修了，参照这个 PR #3576 比较坑跌的地方在于这个 submodule 的依赖是和日期相关的，而这个问题在依赖的不久后就给修了，非常的无语 &lt;cstdio&gt; 编译 CIRCT 的时候会报几个错，include 对应的头文件就行","tags":["踩坑"]},{"title":"Compiler04 语法制导翻译","path":"/2023/03/18/Compiler04-语法制导翻译/","content":"Intro 经过语法分析后的代码将会变成一棵具有内部结构的语法树，SDT 让我们能够在这棵树上做一些事情。 事实上我感觉基于语义动作和属性文法的翻译本身已经比较老，并且讲起来非常绕。现在有更简单也更可操作的翻译策略（例如 OO 的 Visitor Pattern, FP 里说的 Pattern Matching）。龙书提到这个设定也更像是为了引入后续各种 AST 上的算法，同时提供一种 formal 的描述基于 AST 算法的语言，nothing more。 Definition 定义了一套 SDT 的文法叫 SDD （Syntax Directed Definition） 主要是定义两个东西： 属性。即可以在 AST 的节点上记录信息 语义动作。即在什么时候执行什么代码 Attributes 龙书又给属性分了两大类： Inherited Attribute，向下传递的属性。例：某个参数化泛型函数的某次调用的类型，这个类型是从环境中继承来的 Synthesized Attribute，向上计算的属性。例：某个复杂表达式的类型，这个类型是从子表达式计算得来的 这里的属性类型区别本质上在于它们的依赖关系方向不同。综合属性依赖于节点的所有孩子，继承属性依赖于节点的父亲。 给属性分类意味着显式地画出数据依赖图，给出一个属性的计算顺序则意味着给依赖图定向。 属性让我们可以一定程度上“改变” AST 的形状。例如可以在非左递归的中缀表达式文法中利用属性来进行信息的传递，在一定程度上填补了 CST 和 AST 之间的 gap。 只有综合属性的文法叫 S-attributed SDD，代码片段没有副作用的 S-attributed SDD 又叫属性文法，意思是我们无需考虑属性之间的计算顺序。 聪明的读者可以发现这实际上是在区分 DFS 中的两个阶段。在 ANTLR 中我们可以通过在 visit 方法前后添加对应的代码来实现等价的功能。","tags":["Compiler"]},{"title":"Network06 Wireless","path":"/2023/02/15/Network06-Wireless/","content":"Intro 无线局域网的特征： 主机都是无线设备（废话） 节点之间通过无线介质通信 存在特殊的设备基站，责任包括： 与覆盖范围内的无线设备通信 协调接入设备对共享介质的使用 终端可能会移动，从一个接入点到达另一个接入点（如何保证通信中的 TCP 连接不断开） 通常又会有两种工作模式： infrastructure mode，即这些无线设备最终会接入互联网，它们只是普通的终端+无线网卡 ad hoc mode，这些无线设备彼此组成了一个整体网络，每个节点要实现完整的网络协议栈 无线链路（传输媒介）的特征： 信号强度衰减，也叫 path loss 会受到信号干扰，例如工作频率为 2.4GHz 的蓝牙和 WiFi Multipath Propagation 问题，即信号在反射后，沿着多条路径到达目的地（可能存在先后） 信噪比（SNR-Signal to Noise Ratio）指的是传输中信号与环境噪声之比，单位是 dB。信噪比越大，则越容易从噪声中分离出传输的信息。 误码率（BER-Bit Error Rate）指的是传输过程中出现比特错误的概率。这个值越小越好。 它们和传输带宽之间的关系可以粗略地看成 \\(\\text{SNR$\\times$BER=Bandwidth}\\) Hidden Terminal Problem hidden 说的是三个节点，其中两个无法感知到彼此，但它们同时传输造成的冲突又会干扰到第三个。 CDMA 后续讨论的比特实际上是 \\(\\mathbb{F}_2=\\Set{1,-1}\\) 先确定一个长度 \\(M\\)，每个节点发送一个比特的时间就会被划分成 \\(M\\) 份 此后可以选取出至多 \\(M\\) 个互相正交的 \\(\\mathbb{F}_2\\) 上的 \\(M\\) 维向量 \\(\\Set{\\vec{v}}_m\\) 以节点 \\(i\\) 为例，假设当前要传输的比特是 \\(d\\)，它想发给 \\(j\\)，那么最终它会被编码成 \\(d\\times \\vec{v}_j\\)，然后发出去 接收方收到之后再计算 \\(\\frac{d\\times \\vec{v}_j\\times \\vec{v}_j}{M}=d\\)，这样就完成了 \\(i\\to j\\) 的传输。而其它节点即使收到了信号，也会因为编码彼此正交而得到 \\(0\\)。 802.11 802.11 在链路层实现了可靠传输，靠的是 ACK。 每个 AP 都有一个 SSID，有一个工作频道。 被动探测： 每个AP定时发送 beacon 帧，包含 MAC 地址和 SSID 终端扫描信道监听 beacon，选择一个 AP 接入 主动探测： 终端主动广播一个帧 AP收到之后回应一个帧，终端选一个接入 探测完之后还需要终端发给 AP 确认，AP 再回一个确认。这是因为在探测完后，AP 仍然不知道终端会不会选它。 接入之后通常会发一个 DHCP 请求来获取 IP，然后就和有线网络几乎每区别了。 验证可以看 MAC 地址，也可以用密码 冲突避免vs冲突检测 有线，检测到了冲突立即停止传输，这是为了减少冲突； 无线，没法检测只能预先避免冲突，这也是为了减少冲突； CSMA/CA 即带冲突避免的 CSMA。无线局域网没有冲突检测，因为： 隐藏终端问题导致不一定检测得到 无线信号的冲突检测很困难 没有冲突检测的意思是：一旦一个节点开始传输，它将传输完整个帧。 流程如下： 先检测冲突，如果没有冲突的话等待一个固定的时间后开始传输 如果冲突了就 back-off 一下再传输 传完了等 ACK 确认了，那么就回到1 否则回到2，重新 back-off RTS CTS 解决隐藏终端的步骤： 某个节点n想要发数据，向 AP 发一个 RTS（Request To Send） AP 收到后广播一个 CTS（Clear To Send），表示： 同意n的发送 通知其它节点暂停发送 节点n开始发 发完之后终端广播一个 ACK，所有节点继续工作。 速率自适应 802.11 可以探测带宽，通过检测连续的ACK、连续的未ACK、超时等事件来进行决策 Power Management 每个节点有两种状态：sleep, wake 节点可以通过设置帧中的位，通知 AP 它将要省电了 AP 将会缓存需要发给它的帧 节点会在 AP 发送 beacon 之前唤醒，然后被 AP 告知是否有缓存的帧 节点可以向 AP 获取之前缓存的帧，也可以继续省电","tags":["Network"]},{"title":"Network05 Link","path":"/2023/02/15/Network05-Link/","content":"Intro 链路层的意义是补全网络层剩下要做的事情，主要有两种类型： 共享介质，广播传输 点对点传输 一般在链路层把设备叫做节点，连接节点的东西叫做链路。链路层就是通过相邻的节点、沿着连续的链路向上提供服务的，包括： 把IP数据包封装成帧 提供 MAC（Medium Access Control）以利用共享的传输介质（如果是点对点就不需要） 可靠传输，例如 WiFi 就是在链路层保证可靠性的 错误检测和恢复，重点是恢复 链路层的大部分功能是用专用硬件实现的，链路层可以看到软硬件交互的接口。 校验码/纠错码 一般的模式是给定数据 D，计算校验码 EDC 一起组成数据 (D,EDC) 传输。接收方收到 (D',EDC') 后进行校验或纠错，如果判定为不可恢复就丢弃这个帧。 奇偶校验 EDC 为一个比特，使得 (D,EDC) 所有位的异或和是 0。可以查出奇数个比特翻转，别的就查不到了。 可以把奇偶校验拓展到二维，这样就可以用出错的行和列定位到出错的比特，从而实现一位的纠错。 校验和 只是提一嘴，就是酱油 CRC CRC（Cyclic Redundancy Check）是一种比较高级的校验码。 不妨设数据 D 有 d 位。 收发双方首先要确定一个 r+1 位生成元 G，并保证 G 的最高位是 1 发送方要给出 r 位数据 R 使得 DR 拼接是 G 的整倍。这里 R=(D&lt;&lt;r)%G 接收方收到 D‘R' 后除一下 G 看看是否整除 需要注意的是，这里的运算均不考虑借位。即这里本质上都是 \\(\\mathbb{F}_2\\) 上的多项式运算。 如果连续出错的比特数不超过 r 个，那么 CRC 是可以检测出来的。 共享介质协议 在共享介质 广播链路中，每个发送方发送的数据都会被所有该链路连接的节点收到，同时刻多方传输则会产生信号干扰，因此需要协议来协调各方的行为。这样的协议也叫 MAC（Multiple Access Control） 理想的协议应当： 实现简单 当 \\(M\\) 个设备同时工作时，它们的长期平均带宽都应该是总带宽的 \\(\\frac{1}{M}\\) 最好是去中心化的，足够鲁棒 这里对共享介质的假设包括： 多方同时传输会冲突，使得数据无效 节点可以检测到冲突的发生 任意节点的发送都可以被所有节点收到 Channel Partition 通过 FDM 或者 TDM 将共享介质进行划分，保证任意发送的主机对不会冲突，可以实现 \\(\\frac{1}{N}\\) 的公平带宽。 问题在于 \\(N\\) 通常是固定的值，因此少数设备活动时可能没法得到全部的带宽。 也可以利用 CDMA（Code Division Multiple Access）。CDMA 给每个节点一个编号（code），每个节点发送时会利用 code 处理一下信息。如果选择恰当的 code 分配算法，就能保证即使多个节点同时传输也不会发生碰撞。 Random Access 每个节点都会全速（满带宽）发送帧 如果节点检测到了冲突，就停止传输 以某种策略重试（ALOHA用概率重传、CSMA/CD用随机back-off） Slotted-ALOHA 有如下规定： 所有帧大小相等，含 \\(L\\) 比特 时间被划分成时间片，每个时间片大小为 \\(L/R\\)。即用满带宽传输一个帧花费的时间 每个节点只会在时间片的开始进行传输 所有节点拥有同步时钟 每个发送的节点能在当前时间片立即检测到冲突 定义参数 \\(p\\)，冲突检测后以 \\(p\\) 的概率在下一个时间片立即重传 可以算一下 s-ALOHA 的效率极限。 注意到一个时间片内，某个节点成功的概率为 \\(p(1-p)^{N-1}\\)，故 \\(N\\) 个节点期望有 \\(Np(1-p)^{N-1}\\) 个节点传输成功 对于给定的 \\(N\\)，我们可以算出最优的 \\(p^*\\) 使得 \\(Np(1-p)^{N-1}\\) 最大，即 \\(p^*=\\frac{1}{N}\\)，此时单个时间片内成功节点数期望为 \\((1-\\frac{1}{N})^{N-1}\\) 这个函数是单调递减的，当 \\(N\\to\\infty\\)，可以知道 \\((1-\\frac{1}{N})^{N-1}\\to e^{-1}\\approx 37\\%\\) ALOHA 不要求所有节点的时钟同步 分析是类似的，区别在于某个时间片内单个节点成功的概率为 \\(p(1-p)^{2(N-1)}\\)。不妨假设讨论的时间片为 \\([s,s+len]\\)，那么不冲突要求 \\([s-len, s]\\) 以及 \\([s,s+len]\\) 这两个时间片内都不能有节点传输。而每个节点在任意时间片内恰好传输一次，这里就是剩下的节点的两次都必须不传输。 算出来是 \\(\\frac{1}{2}e^{-1}\\) CSMA/CD CSMA/CD（Carrier Sense Multiple Access with Collision Detection）这一类协议要求： 在发送之前先检测是否有人在发送 检测到冲突后立刻停止发送 等待一个随机的时间，重试 目的是减少冲突 back-off 的策略也可以操作。例如在检测到第 \\(n\\) 次冲突后，节点将在 \\(\\set{0,1,2\\ldots 2^n-1}\\) 中随机挑一个数 \\(K\\)，然后等待 \\(K\\) 个时间单位。而在 \\(n\\) 达到 \\(N\\) 次后将不再继续增长。 这样的好处： 首先这仍然是随机的 其次有一个探测的过程，back-off 的时间逐渐指数级变长 有一个上界 课本上给了一个效率，不会推。 Take turns Master node 即选出一个 master 节点用于协调传输。每次 master 会发送令牌给一个节点，这个节点就开始广播。令牌用完之后就停止传输，由 master 继续发令牌。 这个协议最大的问题还是鲁棒性，如果 master 挂掉了就整个网络都挂掉了。 Token passing 也就是击鼓传花，手上拿着令牌的节点可以传输，时间一到就传给下一个节点。 问题还是鲁棒性，这个协议要求每个节点都必须正确地操作（让出令牌）。 MAC 地址/交换机 48位，一般每个设备的 MAC 是不变的（回忆 IP，主机的 IP 要设置为子网内有效 IP 地址，DHCP） 需要注意的是，交换机与主机和路由器直接相连的端口，是没有 MAC 地址的。这是因为这些链路上的主机和路由器不需要寻址，直接发就能被交换机收到（但主机和路由器的端口需要 MAC 地址，这是为了交换机发送数据） 链路层端口工作流程如下： 把 IP 数据包封装成帧，填入目标 MAC 地址，交给物理层打出去 收到帧后查看目标 MAC 地址 如果目标 MAC 地址就是当前端口的 MAC 地址，接收 丢掉这个帧 也可以广播，FF-FF-FF-FF-FF-FF 就是特殊的广播 MAC 地址 ARP ARP（Address Resolution Protocol）是用来计算 IP 地址到 MAC 地址的映射的 计算 ARP 的模块通常包含若干表项，每个表项的内容为： IP 地址 MAC 地址 TTL 查询过程如下： 看看是否存在目标 IP 的映射 存在，直接返回 MAC 地址 不存在 构造 ARP 包，目标 MAC 是广播地址，包含要查询的 IP 地址和源 MAC 地址 收到 ARP 包的节点会比较查询 IP 地址和自己的 IP 地址，然后给源 MAC 发一个确认包 根据 TTL 去掉超时映射 需要注意的是，查询是广播而应答是点对点的。 以太网 以太网有这么几个优点： 发明得很早，大家都用 很简单，成本低 速度还不错 交换机 交换机对于主机和路由器而言是透明的，即主机只会标注目标路由器/主机的 MAC 地址。 交换机消除了冲突 引入交换机可以让不同链路跑在不同的速率 能够提高网络的鲁棒性，例如过滤某些可疑的帧 交换机的功能主要包括过滤和转发。不妨假设交换机的 x 端口收到了一个目标 MAC 地址为 M 的帧： 如果交换机没有 M 的转发表，那么就广播这个帧（x 端口除外） 交换机中记录 M 的出端口为 x，那么就丢掉这个帧 记录 M 的出端口为 y，那么就从 y 转发 而交换机中的转发表则是通过预定算法求出来的 每从端口 x 收到一个源 MAC 地址为 S 的帧，就记录一个表项 (S,x,TTL) 固定一段时间根据 TTL 清理表项 VLAN 即一个 LAN 仍然可以继续划分成若干个 VLAN。VLAN 的功能需要交换机额外提供，可以提供更好的安全性和资源管理 vlan 如上图，所有的端口的划分构成了若干个不相交的广播域。即这个交换机被虚拟化成了好几个交换机，分管不同的广播域。 此时这些分属不同广播域的子网要怎么互联呢？解决方法是从交换机拉出一个端口到路由器，设置这个路由器的端口同时属于这两个子网。那么不同广播域的子网发消息，只需要经历 主机1-&gt;交换机广播域1-&gt;路由器-&gt;交换机广播域2-&gt;主机2 此时考虑这样一个问题：假如有两个交换机，每个交换机上分别有 N 个广播域。这 N 个广播域的端口，一半在交换机1上，另一半在交换机2上。我们希望让每个广播域分属两个交换机的端口互联，那么这需要至少 N 条链路。 另一种解决方案是利用 VLAN trunk。即两个交换机各挑出一个端口，设置成 trunk port，然后相连。这样交换机1的广播域1在广播的时候，帧会沿着 trunk 来到交换机2，再经过交换机2的分流进入交换机2的广播域1。 总览 假设主机 A 要向主机 B 发 IP 数据包 A 先得到 B 的 IP 地址（或者利用 DNS 解析域名） 判断 A, B 是否处于同一子网 是，那么把 IP 数据包封装成帧，根据 IP 地址查询目标 MAC 存在 IP-&gt;MAC 映射，发 不存在，用 ARP 查询到 IP 对应的 MAC 地址，发 否，那么把 IP 数据包封装成帧，根据默认路由的 IP 地址查询 MAC 同上 同上","tags":["Network"]},{"title":"Automata07 TS","path":"/2023/02/08/Automata07-TS/","content":"Def Transition System 本质上是一个五元组 \\(\\mathcal{A}=\\left&lt;S,S_0,T,\\alpha,\\beta\\right&gt;\\)，分别表示状态集、初始状态、转移集和端点函数。这里 \\((\\alpha(t),\\beta(t))\\) 表示转移 \\(t\\) 的起点和重点，这么做的一个好处是 \\(T\\) 可以任意多，并且两个状态间存在多个转移。 当然本质上还是有限有向图。 Path 一条路径是一个序列 \\(\\Set{t_i}\\)，满足 \\(\\forall i,\\beta(t_i)=\\alpha(t_{i+1})\\)，且 \\(\\alpha(t_0)=S_0\\)。即首尾相连的一系列边。 有了路径就能定义联通性，比较 fancy 的写法是 \\(\\twoheadrightarrow\\subseteq S\\times S\\) 是包含 \\(\\alpha(T)\\times\\beta(T)\\) 的传递闭包。那么一个状态 \\(s\\in S\\) 可达当且仅当 \\(S_0\\twoheadrightarrow s\\)。 规定 \\(T^+=\\Set{\\text{finite paths in $T$}}\\)，\\(T^\\omega=\\Set{\\text{infinite paths in $T$}}\\)，容易将 \\(\\alpha,\\beta\\) 拓展到 \\(T^+\\) 上 同时可以把连接操作拓展到路径上，即 \\(p_1\\circ p_2\\) 规定为两个序列的拼接。为了方便 \\(p_1\\) 不能为无穷路径。 Terminal \\(\\forall t\\in S\\) 都不存在 \\(s\\twoheadrightarrow t\\)。即没有后继状态 Deadlock 如果 \\(s\\) 是 terminal 且 reachable，那么 \\(s\\) 也叫死锁状态 Label 可以多带一个 \\(\\lambda:T\\mapsto A\\) 表示给转移加上名字，\\(A\\) 就是名字集。 Trace 给定一个路径 \\(p\\)，那么定义 \\(\\text{trace}(p)=\\text{map}(\\lambda,p)\\) Equivalence 给定两个 TS \\(T_1,T_2\\)，我们可以寻找它们之间的若干``等价''关系。下面从强到弱给出。 Homomorphism 定义为一对映射 \\((f,g)\\)，其中 \\(f\\colon S_1\\mapsto S_2\\)，\\(g\\colon T_1\\mapsto T_2\\) 满足： \\(f\\circ \\alpha_1=\\alpha_2\\circ g\\) \\(f\\circ\\beta_1=\\beta_2\\circ g\\) \\(f\\circ\\lambda_1=\\lambda_2\\circ g\\) 如果 \\((f,g)\\) 恰好都是满射，那么称 \\(T_2\\) 是 \\(T_1\\) 的一个商（quotient） Strong Isomorphism \\((f,g)\\) 均为双射 强同构说明两个 TS 没有本质上的不同 Weak Isomorphism \\((f,g)\\) 定义在 reachable state 上。即 \\(T_1,T_2\\) 删去不可达的点和边之后是 Strongly Isomorphic 的 很显然强同构 implies 弱同构 Bisimulation 存在关系 \\(\\simeq\\subseteq S_1\\times S_2\\) 满足 \\(S_0\\simeq S_1\\) \\(s_1\\sim s_1&#39;\\) 且 \\(s_1\\twoheadrightarrow s_2\\) implies \\(s_2\\simeq s_2&#39;\\) 且 \\(s_1&#39;\\twoheadrightarrow s_2&#39;\\) \\(s_1\\simeq s_1&#39;\\) 且 \\(s_1&#39;\\twoheadrightarrow s_2&#39;\\) implies \\(s_2\\simeq s_2&#39;\\) 且 \\(s_1\\twoheadrightarrow s_2\\) 给一个例子 bisim Product 定义两个 TS 的乘积为分别将状态、转移相乘。直接相乘叫做自由积 Free product 可以给自由积加上限制 \\(I\\subseteq \\prod A_i\\)，表示只有 \\(I\\) 中的转移才可以被触发。 不难发现自由积定义出来的新 TS 默认所有转移是同时进行的。 Temporal logic 本质上是一种带状态机结构的谓词逻辑，可以表达例如 \"eventually true\" 或者 \"never true\" 这样的句子。 一般描述一个 TS \\(\\mathcal{A}\\) 的时候会用元组 \\((\\mathcal{A}, AP, L)\\) 来表示，其中 \\(AP\\) 表示原子命题，\\(L:S\\to 2^{AP}\\) 给每个状态标记上若干个原子命题，表示这个状态的性质。 一个 trace 是 \\(\\mathcal A\\) 中状态的无穷序列 \\(w=\\Set{s_i}_{i=0}^\\infty\\)，定义 \\(w_k=s_k\\)，\\(w^k=\\Set{s_i}_{i=k}^{\\infty}\\) LTL Linear Temporal Logic 要求 \\(\\mathcal A\\) 中的自动机是线性的。 Syntax 主要是多了几个算子 \\(X\\)，neXt \\(U\\)，Until \\(G\\)，Globally 或者 Always \\(F\\)，Finally \\(R\\)，Release Semantics 对于给定的 trace \\(w\\) \\(w\\models a\\iff a\\in w_0\\) \\(w\\models a\\wedge b\\iff w\\models a\\text{ and } w\\models b\\) \\(w\\models Xa\\iff w^1\\models a\\) 即下一个状态满足。 \\(w\\models aUb\\iff \\exists k, w^k\\models b\\text{ and } \\forall i&lt;k,w^i\\models a\\) 即存在一个时刻 \\(b\\) 满足，并且在 \\(b\\) 满足之前 \\(a\\) 都得持续满足。 \\(w\\models Fa\\iff w\\models \\text{true}Ua\\) 即存在一个时刻使得 \\(a\\) 满足。 \\(w\\models Ga \\iff w\\models eg F eg a\\) 即并不存在一个时刻使得 \\(a\\) 满足 也就是说 \\(a\\) 会在任意时刻满足 \\(w\\models aRb\\iff w\\models eg( eg aU eg b)\\) 即并非（\\( eg a U eg b\\)） 即并非（存在一个时刻满足 \\( eg b\\)，且在 \\( eg b\\) 满足之前 \\( eg a\\) 一直满足） 即（不存在一个时刻满足 \\( eg b\\)）或者（存在一个时刻满足 \\( eg b\\) 但是 \\( eg b\\) 满足之前 \\( eg a\\) 并非一直满足） 即（任意时刻都满足 \\(b\\)）或者（存在一个时刻不满足 \\(b\\)，但是 \\(b\\) 不满足之前存在时刻满足 \\(a\\)） 即在 \\(a\\) 满足之前，\\(b\\) 必然一直满足。如果 \\(a\\) 一直不满足，那么 \\(b\\) 将一直满足。 （有一种 \\(U\\) 逆转然后放松的感觉） CTL Syntax Semantics CTL* Syntax Semantics","tags":["Automata"]},{"title":"cmm Compiler Design","path":"/2023/01/28/cmm-Design/","content":"Intro cmm 是用 C99 编写的编程语言 cmm 的编译器 本文的主要目的是记录 cmm 的细节和取舍，以记录分析和优化相关的内容、IR 设计的知识和从 LLVM/JDK 那里抄来的东西（ Parsing 主要是基于 Flex/Bison 这套工具做的，然后顺便设计了一下 AST 的结构 Flex/Bison 讲几个比较有用的小技巧（全都是从手册里抄来的） 可以给符号定义 %destructor，从而实现一些资源的管理。例如 Bison 的错误处理会不断从栈上弹出符号，这些符号相关资源的释放则会调用这个自定义的析构函数。 可以设置自定义报错 %define parse.error custom，然后在里面调用一些 Bison 提供的函数获取一些解析。例如做成 “expected symbol XXX, got XXX” AST 用了一个 X-macro+OO 的操作来写得比较爽一点，具体可以看 repo。 有了这俩之后就能做 visitor 了。具体而言有这么几个： ast_alloc，节点的构造函数 ast_free，节点的析构函数 ast_check，对节点做语义分析 本来还想做一些 AST 上的常量折叠/表达式旋转的，但是摸了。 当然这玩意的本质还是一个大的 switch-case，或者按照 tls 的说法是“二维dispatch” Polymorphism 实验手册给出的实现多态的方案是这样的： struct base &#123; type_t type; union &#123; struct derived1 field1; struct derived2 field2; /* other subclasses */ &#125;; &#125;; 这样的好处是比较简单，而且很有 C 的风味。 与手册中推荐的 union 方式不同，我这里选取了\"继承\"的路子。考虑如下代码： struct base &#123; type_t type; /* base fields */ &#125; ; struct derived1 &#123; struct base super; /* other fields */ &#125;; struct derived2 &#123; struct base super; /* other fields */ &#125;; 通过排布内存布局和指针类型的强转就可以实现一部分继承和子类型多态的特性。 这样的写法可以让我们忽略 AST 节点的具体类型，但仍然保留遍历 AST 的能力（回忆内核里的侵入式链表）； 同时不需要滥用 void *，方便内存管理；在 cast 之后还可以获得编辑器的补全，无需考虑 union 带来的误用。 Typechecking typechecking 可以用一个 visitor 实现。需要注意的细节会比较多。 symtab 符号表需要支持 嵌套作用域 插入符号（长度有限的字符串），得到一个 entry 或 NULL（表示重复插入），entry 可以写入信息 查询符号，得到一个 entry 或 NULL（表示不存在） 实际上类似的结构在 IR 上的分析也会用到，因此抽出来一个 hashtab 用于查询符号，插入则通过简单的写入 field 来实现。symtab 只是简单包装了一层 hashtab，这样底层的哈希函数和查找逻辑后面都可以改动（咕咕咕）。 type 类型相关的设计非常糟心。理想的类型应当是 ADT，但实际上做起来很麻烦。 数组类型（TYPE_ARRAY）可以做成 enum Type &#123; TypeInt, TypeFlt, TypeArr(u32, Box&lt;Type&gt;) &#125; 这样本质上是将数组视为类型构造子，有一层一层剥开的感觉，做 partial addressing 或者是 type equivalence checking 会很简单，但是生成 IR 的时候算下标会比较麻烦。 或者直接压平，做成 enum Type &#123; TypeInt, TypeFlt, TypeArr(u32, Vec&lt;Type&gt;) &#125; 当然最佳实践应当是做成多级的 IR，这样就可以二者兼顾了。但我比较懒所以直接压平了。 cmm 里可以定义结构体，考虑下面这么一段代码： struct A &#123; int a, b, c; &#125; d; 这一段看上去只是 VarDecl，但它实际上还构造了一个类型 struct A。这也解释了有没有 d 不影响语法正确性。 这里我引入了一个 TypeDecl 的节点用来处理这种情况，那么其余所有地方都实质性地变成了对 struct A 的引用。 最怪的地方：cmm 并不支持浮点数和整数的任意转换。当然到了后面才会发现这么做是因为 cmm 里的浮点数都是酱油。 err propagation 不妨假设类型检查出错了，例如考虑如下代码： int foo(int x) &#123; return x; &#125; float foo(int arr[114]) &#123; return foo(arr); // ? &#125; int main() &#123; float f = foo(1, 2); // ? float f = bar(1); // ? &#125; foo 重复定义，那么第二个 foo 里的错误要不要报？第三行的 foo 是哪个函数的引用？ foo 的参数不对。这里 foo 的返回值仍然能保证是 int 吗？ bar 不存在，这里返回值的类型应该是什么？应不应该报赋值两侧类型不匹配？ （思考：C语言又会是什么情况？） IR 咕咕咕 Analysis 目前只实现了一个数据流分析，并且还没有抽出一个比较通用的数据流分析框架，因此下一步就是实现框架，通过“注册-管理”的模式来跑各种分析。 Framework 这个框架可以说是改得最痛苦的部分了。实现了这么几个优化： Order 事实上这个是最大的大头。这里抄一段讲义原话： 格理论告诉我们，in和out集合的运算顺序不影响数据流方程解的收敛性，但会影响解的收敛速度。对于上述数据流方程而言，按照i从大到小的顺序来计算in和out往往要比按照i从小到大的顺序进行计算要快得多。 Merge 对于 AVL 实现的 set 的 merge，可以通过两次中序遍历+线性合并+中序遍历建树来实现 \\(O(n)\\) 合并。 打了一些 log 之后发现跑 14Lab3Impossible.0.cmm 的时候 set 的大小普遍在 2K 左右，结合 perf 可以发现这样做确实有不错的性能提升 Monotonicity 注意到经典的数据流分析实质上在解一堆数据流约束，并且每个图上的节点对应的抽象数据都存在单调性。也就是大概长这样： def process(BLOCK): for PRED in BLOCK.pred(): IN[BLOCK].merge(OUT[PRED]) # A &lt;= MERGE(A,B) &amp;&amp; B &lt;= MERGE(A, B) OUT[BLOCK] = BLOCK.transfer(IN[BLOCK]) 因此可以这么优化：如果某个节点在 merge 之后不变，那么无需触发后续的计算。 这要求 merge 必须是单调的、能发现单调性的变化。 当然 transfer 却并不一定是单调的。考虑如下情况： a = NAC a = 0 a = NAC 这个基本块内的 a 的抽象值显然不单调。这种情况可以通过 改写成 SSA 形式（思考：为什么） 以语句为单位而不是基本块为单位求解 方案1是比较好的解法，但是写起来繁琐（实验要求的 IR 并不支持 \\(\\phi\\) 操作）；方案2是比较简单的解法，但是存在效率上的问题。 由于这里的数据流分析框架并没有对 IR 的形式有任何要求，我也就没实现这个优化。 Live 问了tls之后得知可以从 fact 中删去 dead 的那些，这样就可以节省集合操作了。 看了一下效果确实很不错。 Live Variables 简单的反向 may analysis。即判断一个赋值的 lhs 在未来是否仍然有用。 Optimization 调优化的过程可以说是非常坑了，大概花去了一周的时间才勉强克服各种 bug 实现了两个半正确性有一定保障的优化。 Local Value Numbering 发现这玩意本质上就是 Available Expressions，但是不需要对两个 valtab 进行 merge 操作，所以就更简单。 定义： \\(\\text{cvar}\\colon \\text{Val}\\mapsto 2^\\text{Var}\\)，即 \\(\\text{cvar(v)}\\) 表示持有值 \\(\\text v\\) 的所有变量 \\(\\text{holding} = \\text{cvar}^{-1}\\)，即 \\(\\text{holding(v)}\\) 表示变量 \\(\\text v\\) 持有的值 \\(\\text{val}\\colon \\text{Expr}\\mapsto \\text{Val}\\)，即给表达式标号 对 \\(\\text{val(e)}\\) 递归定义如下： 若 \\(\\text{e}\\) 是 OPRD_LIT，那么 \\(\\text{val(e) = e}\\) 若 \\(\\text{e}\\) 是 OPRD_VAR 如果 \\(\\text{e}\\) 在当前块内、当前指令之前被定义过，那么 \\(\\text{val(e) = holding(e)}\\) 如果没有，那么 \\(\\text{e}\\) 本身是 value （safe-approximation），\\(\\text{val(e) = encode(e)}\\) 若 \\(\\text{e}\\) 形如 \\(\\text{v$_1$ OP v$_2$}\\)，那么 \\(\\text{val(e) = encode(val(v$_1$), OP, val(v$_2$))}\\) 那么假设现在遇到了表达式 tar = lhs OP rhs，那么先根据递归定义求出 val(lhs), val(rhs), val(lhs OP rhs)。 观察 \\(\\text{cvar(n)}\\)： 若非空则当前指令可以写成 tar = cvar(n)。 否则我们分别观察 lhs 和 rhs，用一些启发式的规则来重写（以 lhs 为例）： 如果 \\(\\text{cvar(val(lhs))}\\) 中存在非临时变量，那么把 lhs 重写成该变量 如果全是临时变量，那么挑一个最早的（编号最小的） 这么做有如下考虑： 减少重复计算 copy-propagation，即尽可能少的使用临时变量，尽可能让临时变量 dead，尽可能让晚出现的临时变量 dead Constant Propagation CP 说起来简单，但是实现也并非那么容易的。 本质上的原理是设计了一个全格用于表示抽象值，每条运算指令都会在一个抽象的 Configuration 下抽象执行，比较坑的地方有这么几个： 抽象值的运算法则。比如说除0怎么处理、运算溢出怎么处理（特别还是用 C 写）、怎么利用代数恒等式提精度 数据流框架怎么加速。因为传递的数据实际上是抽象 Configuration，这样一个比较重的 map 对象的拷贝和修改都是很耗时的，就要挤各种效率 Dead Code Elimination Unreachable 非常简单的分析，但是存在比较大的问题：我的 cfg 并不支持很优雅地删除节点及其附带的边，所以暂时没有打开这个优化。 Dead Assignment 利用 live 的分析结果，删去无用的指令。","tags":["Compiler"]},{"title":"CMU-DB Lab","path":"/2023/01/10/CMU-DB-Lab/","content":"Intro 学校里的数据库实在是讲得太烂了，是听了十分钟就会想激情退课的水准 cmu这个感觉就 andy 比较有激情，21年代课的老师也比较闷 听说这个 lab 很牛逼（jyy和qlg强力推荐），于是做一做 我做的是2022fall的版本，这个评分的网站 gradescope 似乎没法注册，所以能通过本地测试就是胜利！ Lab0-Trie 看说明 lab0 是用来劝退的，大概是要你写一个支持并发的字典树 实测并发是个酱油，一把大锁就能解决了，然后读写锁也都给了你... 比较关键的点就是 TrieNodeWithValue 继承了 TrieNode，然后有一个构造函数是 TrieNodeWithValue(TrieNode &amp;&amp;other, T Value) 用于 consume 一个普通节点，构造出一个带 value 的节点。插入的时候，节点类型的转换我是通过先删除后添加的方法做的... 以及一个之前没注意过的点是这样的：make_unique&lt;T&gt;(ARGS) 的参数 ARGS 将被用于构造 T 而不是作为 unique_ptr 的内容。这也意味着 make_unique&lt;T&gt;() 只能创造指向 T 的指针。 而如果想要实现多态的话，我们得这么做 unique_ptr(new T())，这样用一个裸指针来初始化。下面是一个我附会了的例子 class Base &#123; public: virtual ~Base() = default; /* other stuff*/ &#125;; class Derived : public Base &#123; /* other stuff */ &#125;; int main() &#123; auto p1 = std::unique_ptr&lt;Base&gt;(new Derived()); auto p2 = std::make_unique&lt;Base&gt;(Derived()); std::cout &lt;&lt; dynamic_cast&lt;Derived *&gt;(p1.get()) &lt;&lt; std::endl; std::cout &lt;&lt; dynamic_cast&lt;Derived *&gt;(p2.get()) &lt;&lt; std::endl; &#125; 这里第一个是正常的指针，第二个是 nullptr。这是因为 std::make_unique&lt;Base&gt;(Derived()) 等价于 std::unique_ptr&lt;Base&gt;(new Base(Derived())) 然后这里又有一个子类型的转换，于是 std::unique_ptr&lt;Base&gt;(new Base((Base)Derived())) c++还真是神奇啊","tags":["Database"]},{"title":"Network04 Network","path":"/2023/01/09/Network04-Network/","content":"Intro 网络层向上提供主机到主机的服务，不保证可靠，主要功能是路由和转发，基于IP协议 通常路由器有多个网卡，IP数据包在传送途中会经过多个路由器、途径多个网卡 控制平面：路由，数据包应当怎么走 数据平面：转发，数据包从哪个网卡出去 SDN 传统的路由算法是在每个路由器上分开跑的，它们之间通过通信来进行数据交换，去中心化地、分布式地运行路由算法。一个比较新的做法是抽出来一个权威的路由计算中心，由它来运行路由算法，路由表则通过分发的方式下载到每个路由器里。SDN（Software Defined Network）说的就是这样的网络。 SDN的好处有这么几个： 性能。集中的路由计算可以更快、更优 自定义。传统的路由算法可能是写死的固件（要等厂商升级），而SDN可以通过分发不同的路由表来实现不同的功能。例如可以分发拦截名单变成防火墙、分发转发规则变成NAT服务器等等 Router Components 路由器一般有这么几个组成部分： 输入/输出网卡，实现了完整的链路层和物理层功能，有队列用于缓冲， switch fabric，我也不知道怎么翻译，大概就是连接输入和输出的部位 路由处理器，执行一些网络层协议的算法（例如格式的转换和分片）和路由算法 路由器的转发部件基本上都是硬件实现的，这是为了效率考虑。而路由算法则通常是软件实现的。 Forwarding Table 路由器的转发是以子网为单位的。一个传输中的IP数据包通常包含目标IP地址，而一个连续的IP地址区间被称为一个子网。转发表中通常只记录以子网为最小单位的转发信息（例如位于区间[xxx,yyy]的IP地址将被转发到端口z），否则需要针对每个IP地址维护转发信息，这里有至少 \\(2^{32}\\) 条。 当然一般用 (IPAddr, prefix) 这样的二元组来表示一个区间。若 addr 的前 prefix 位与 IPAddr 相同则认为 addr 位于这个区间。转发的时候需要找到能匹配到最长前缀的那个转发表项。 Switch Fabric 科普了几种交换的技术： via memory，相当于用通用计算机来做这件事情。由于需要先写入再读取内存，因此吞吐量只能达到内存吞吐量的一半（B/2），并且任意时刻只能转发一个数据包，没法并行 via bus，相当于用总线把所有输入和输出端口都接起来。由于所有发送方都是广播，因此任意时刻只能转发一个数据包，没法并行。但是总线不需要分开读取和写入，因此吞吐量可以达到完整的总线速率（B） via interconnection network，其中一种实现长这样。如果所有的数据包互不干扰，那么就可以实现完全的并行转发（NB）。类似的还可以把数据包分片，然后通过这种 fabric 转发，以实现更小粒度的并发（像不像内存分页） Queue 为什么需要缓冲队列： 输入和输出网卡的速率可能不匹配，需要缓冲 fabric可能会阻塞转发，需要缓冲。某个数据包被队列中排在它前面的包阻塞，这样的情况叫HOL blocking（Head of Line 阻塞） 当队列大到一定程度时路由器会被迫丢掉若干数据包，这里就有一个 victim choice 的问题。 队列的长度限制也是一个需要调好的参数，太短则容易频繁丢包，太长则容易造成延迟（某些实时的数据包会因此而失去意义）。传统的推荐大小是 RTT*C，最近的研究则认为应该是 RTT*C/SQRT(N)，其中 C 是链路带宽，N 是 TCP 连接数，RTT 一般为 250ms Packet Scheduling 谈来谈去就是那么几个 FIFO 分优先级 RR WFQ/MLFQ IPv4 有几个比较重要的 field header 长度，因为有可选项 总长度 identifier, flags, fragmentation offset。这三个主要是为了实现 IPv4 包的分片，即 IPv4 允许包在传输途中被切分（只要最终是完整的就行）。 TTL header checksum 源IP和目标IP Addressing 每个主机和路由器的端口都需要有一个IP，网络层的数据包从一个IP地址发往另一个IP地址 理论上每个主机和路由器的端口都有全局唯一的IP地址（用于辨认身份），在不考虑 NAT 的情况下也确实如此。 255.255.255.255 这个全1的地址是广播地址，向这个地址发包将会向子网内所有拥有IP地址的设备发包 子网 CIDR=Classless Interdomain Routing 子网指的是一段前缀相同、连续的IP地址的集合。通常用 \\(\\text{(prefix,len)}\\) 这样的二元组来描述一个子网。例如 223.1.1.0/24 指的就是前24位与 223.1.1.0 完全相同的所有IP地址的集合，它们组成一个子网。 CA=Classful Addressing 说的是只能按照 /8 /16 /24 进行子网划分，分别叫A类、B类、C类网络 统计子网可以删去所有的节点（主机/路由器），剩下联通块的数量就是子网的数量。即两个节点之间如果不需要额外的主机和路由器即可通信，那么它们同属一个子网。 DHCP 手动分配IP是很痛苦的事情，DHCP（Dynamic Host Configuration Protocol）做的就是让主机来自适应地获取它被分配到的IP地址。 DHCP一般会有一个子网内的服务器，协议步骤如下： 造一个UDP包，端口67，从0.0.0.0发往255.255.255.255 UDP和67都是任意的，全0是因为当前主机还没有分配任何IP地址，广播是因为当前主机不知道DHCP服务器的IP地址 服务器收到这个广播，然后也发一个广播。从服务器的IP地址发往255.255.255.255，提供 transaction ID，用于标记这是哪一次DHCP交互 IP地址 子网 有效时间 这时广播仍然是因为主机还没有IP地址，有效时间使得DHCP服务器可以定时更换主机的IP地址，或者收回地址资源（考虑主机频繁出入的无限局域网）。 主机收到这个广播，向目标IP地址发一个确认消息 服务器收到确认消息，向分配的IP地址发一个确认消息 NAT IPv4地址显然是不够用的，因此引入NAT（Network Address Translator）来缓解全局IP地址不够用的问题。 NAT本质上是利用一个拥有公网IP地址的主机作为代理，实现了 \\(\\text{\\texttt{(private\\_ip,private\\_port)}}\\mapsto\\texttt{(NAT\\_ip,NAT\\_port)}\\) 的翻译。即私有网络中不同IP的主机，最终都被映射到了同一个NAT的公网IP的不同端口上。 引入NAT也会引入一些问题，例如在P2P应用中经典端口被占用，这使得私有网络中的主机没法做server。 IPv6 主要有几个变化 地址长度变成了128位 header 定长 没有 checksum 不允许划分IP数据包 Tunneling 即如何让新协议跑在旧屎山硬件上。 非常简单，把IPv6的整个包作为IPv4的数据中途传输即可。这个操作就叫隧穿。 Routing 主要科普几个路由算法，要会做题。主要分为： 中心化/分布式 动态/静态 负载敏感/不敏感，说的是会不会考虑瞬时的拥塞状况 Link-State 所有节点通过广播获得全局信息 每个节点单独地以自己为起点跑Dijkstra算出到其余点的最短路径 更新路由表 oscillations 指的是一类考虑瞬时负载时出现的问题，我们的路由算法可能会在两条路径之间来回震荡 选择路径1，此时路径1拥塞，路径2畅通 第二轮选择路径2，此时路径2拥塞，路径1畅通 反复 解决的方法课本给了两种： 避免参考瞬时的链路评估 避免所有的路由器同时运行LS路由算法 做题的时候一般默认所有路由器得到相同的全局观，且同时完成算法。 Distance-Vector 每个节点只有局部的信息，向外广播自己的视角，通过接收广播来更新自己的视角 每个节点根据当前视角、所有邻居的局部信息更新自己的视角 更新路由表 本质上是在跑一个并行的bellman-ford 做题的时候一般默认每一轮都是同时进行的，并且每一轮只能看到上一轮的邻居视角。 可能出现 count-to-infinity 的问题： 中途链路代价的变化可能导致出现环，从而一个包没法正确送到目的地 在逐渐适应链路变化的过程中，环将一直存在，并且这个过程可能比较长 解决这个问题的办法之一是给边的正反方向赋予不同的代价，以引导算法不走环。 Poisoned Reverse 指的是这样的处理： 如果 x 途径 y 到达 z，那么 x 就会向 y 汇报自己到 z 的距离是 \\(\\infty\\) 可以发现这样实际上避免了边 \\(xy\\) 被重复连续走两次。但这样避免不了更大的圈的问题。 对比 LS需要交换全局信息，而DV不需要 LS的复杂度就是Dijkstra的复杂度，而DV的收敛取决于链路状况 LS的每个节点独立计算路由表，相对隔离；而DV在计算途中会传播计算结果 OSPF 这是一种 intra-AS（Autonomous System） 路由算法，即在一组路由器内部进行路由的算法。 OSPF 属于 Link-State 协议，加入了一些特色 MD5校验，加强安全性 允许同时使用多条代价相同的路径 支持AS内部继续分组（sub-AS） BGP BGP 是一种 inter-AS 路由协议，即在一堆的路由器集合之间进行路由的算法。 所有的路由器被划分成两种：internal 和 gateway，分别运行 intra-AS 和 inter-AS 的协议。 运行 BGP 协议路由器的路由表以子网为单位，两个路由器通过 BGP connection 交换信息 处于同一 AS 的路由器间的连接叫 iBGP(internal) 处于不同 AS 的路由器间的连接叫 eBGP(external) 分发信息 本质上是一个三元组 (AS-PATH, NEXT_HOP, TARGET_SUBNET_MASK)，分别表示从当前 AS 出发到达新主机的一条路径，和这条路径上第一个离开当前AS的路由器的IP地址，以及目标AS的子网掩码。 某个主机会通过 iBGP 向 AS 内的路由器传达存在性，然后通过一个 external 路由器向外，借助 eBGP 给另一个 AS 传达新主机的存在性 在传达的途中，消息会被逐渐附加上消息途径的路径，这样接收方就能反推出到达新主机的路径了。同时消息会逐步更新 NEXT_HOP，这是因为这个信息只对当前 AS 有用，因此可以覆盖 选择路径 不妨假设有许多从当前 AS 到达目标子网的 BGP 路径信息，我们需要选出一条来 Hot potato routing 即挑出一条路径使得当前路由器到 NEXT_HOP 的代价最小（通过 intra-AS 协议获得的信息）。这么做的 idea 在于尽可能快地让这个数据包离开当前 AS。 Route-selection algorithm 按照顺序执行下述策略 可以人为设置一个偏好值，例如更便宜的链路 挑 AS-PATH 最短的 hot potato 通过 BGP-identifier 排序，虽然我感觉这里顺序是无所谓的 IP-cast multicast，某个IP对应了一组设备 unicast，某个IP对应一台设备 broadcast，某个IP对应广播范围内所有设备 anycast，某个IP对应一组设备中的任意一个设备 前面提到的 DNS 可以做到域名的各种cast，而这里则是在IP层面实现的。 通过 BGP 协议，我们可以布置许多服务器，然后给它们分配相同的 IP 地址，这样在进行 BGP 广播的时候，接收方 AS 将会认为存在多条路径通往同一个主机（而真实的情况是存在多条路径通往多个不同的主机，这些主机恰好提供相同的服务），然后根据路由选择算法挑一条路径出来（也就是挑一个主机出来）。 稍微想想就会发现这样可能出现数据包交错发的情况，因为 BGP 的路由选择算法本质上是启发式的。因此面向连接的协议就比较难办，例如 TCP。而 DNS 这种基于 UDP 的服务就很方便，短平快而且实现了 IP 层面的负载均衡。","tags":["Network"]},{"title":"大三上昏睡日志","path":"/2023/01/03/大三上昏睡日志/","content":"写在前面 这学期实在过得没什么感觉，断断续续的网课+延迟的期末周着实让人昏睡。选的几门课也让人提不起精神，打算看的书也没怎么动，豆瓣的评分倒是多了很多。 有人说昏睡是 long covid 的症状之一，原来我早就是小阳人了（摔） 暑假 暑假过着一直写代码的生活。感觉做工程就是一种比较麻烦的事情，你需要考虑一些可能根本用不上的东西（比如比较虚幻的“可扩展性”和“可维护性”，以及我一直摸不透的“面向接口编程”）。一些命名方面的问题也很棘手，要不要缩写、要不要写长、怎么处理冲突、怎么给临时变量起个有意义的名，这些都是比较恶心但是不得不做的部分。 当然我也意识到自己的代码写得还是很烂的，这点没得说。 顺便还和jxp整了一个一直想整的网站，虽然感觉大家也不是很积极在维护，但有还是总比没有好的。 九月 舍友都跑路去了鼓楼，返校之后发现连路由器都被顺走了，于是只得大出血置办了一点东西。顺便一个人住大豪斯是真的很爽，可以外放、可以随便昏睡、可以自由作息，很难说昏睡的症状不是被惯出来的。 526之后的411又进行了一次搬迁，最终的归宿还得是大活。在此之前我只有在看表演的晚上才会进入这个现充的建筑。当然新的工作室很狭小，勉强能容纳六个直立、四个打坐。在搬迁之前还经历了人生第一次大翻车，本来是很有纪念意义的三位数单，结果就出问题了。在这之前也不是没有见过别人的事故现场，但是自己经历还是感觉很不一样的，比如手真的会抖。 中途也经历了很多的扯皮，比如社团管理层的讨论、和客户的交流、对原因的追溯。社团里的小伙伴们也很给力，在关键的时候能互相支持，这点我感激涕零，所以就再没接过单了。 十月 小群开展了丰富的课余活动——爬山。很难想象一群阿宅要怎么征服栖霞山，但我们确实做到了。 这学期去sly的次数显著地多了起来，但也并没有吃出便宜实惠的特点来。某人说阿宅的现充尝试都是对现充的拙劣模仿，感觉也在理。 十月的时候突然了解到有同学在玩毕昇杯，所以我就也试着跟进了一下。找了一个 cornell 的编译课看了看就开始写了，虽然现在还是大坑的阶段。途中做了一些小小的优化，然后还写了一个比较小的 IR 模拟器，还是很有成就感的（但不多）。听说后端还会有坑，但我一直都没有推进进度，就继续摸着吧... 十一月 双十一前后买了个体重秤，希望在体测前激励一下自己保持体重。当然最后减重的效果不是非常明显，但要命的1km有了长足长进。以及我才知道1km是分段给分的，跑不过一个大段都会让分数下取整，呵呵 把下了很久的四叠半看掉了，要是早几年看我肯定也会惊为天人然后四处开吹的，可惜我已经是老油子了，这片就应该大一开始看。 跑完体测那天立刻就去了鼓楼的百团干活（虽然也不清楚自己是不是真的很被需要），或者说去玩。在路上遇到了点小插曲，最后在新校园体验了一下新生的气息和活力。中间遇到了一些以前的同学，顺便感受了一下新生爷爷的精装修图书馆和暖气，金波！！！ 大概是期中后，每周的自动机下课都会有人一起吃饭，就这还凑成了一对，还都是熟人。这算什么，就像龙卷风吗？ 卷王和jxp说想要做老赵的助教，感觉大家都在自己找的路上前进了。 十二月 十一月底就有各种风声和神秘的活动，然而我什么都不知道。后来风声渐渐大了，说要放开解封。再后来变成各个大专竞相开始劝学生回家，于是我也就蠢蠢欲动了。在戴 N95 连着吃了六天自提麦和翘了三天早课之后终于等来了系里的赶人通知，非常感人。然后填了几张表就飞回家了。 这学期就这么糊里糊涂地过完了。 课程 选课的自由度不是很多，有些课现在看来纯属是执念而不是别的，有些就只是“看上去比实际上要有趣的多”，当然也可能是因为讲的人的风格。 以及因为这学期一直在摸，所以我甚至没有完整地做完任何一门课的全部笔记...还有一个很大的原因就是不少课程都取消了期末考试，其余的也都延期了，没啥复习的动力...过年前应该会补完的吧，大概（？） 计网 实验和讲义都是照搬的 CS144 （但搬得不是太好），作业都是找得到答案的往年题，按照某些小道消息连试卷也是（ 在暑假的时候试着看了一下 CS144 的视频，发现实在是太短小乱了，遂放弃。又因为这门课占据了一周最不好的两个时间段，所以我也就没怎么去上课，转而去听科大的那个视频了。感觉那边讲得更是模模糊糊，可能也只能怪这本书本来就模模糊糊。 实验倒是还好，最后的 tcp_connection 实在是折磨了我很久，还是用 tshark 抓包才看出来哪里写挂了。这段经历是不会忘记的。 zy老师还是很nice的，可惜这门课还是喜欢不起来...就这样吧 软工 定性为烂课 本来是全部讲理论的背书课，后面强行加上了实验得以免去了期末考。往年的实验是写 android app，这都没啥。 但是今年改革了！新实验端上来就十分的sb，需要你： 肉眼看 \\(C_n^2\\) 对程序的等价性 敲 \\(C_n^2\\) 条 CSV 条目，每个文件名的长度超过 20 个字符。 程序是某种独特的中间形式的“C语言源程序”，由某个远古的工具 eqminer 生成 \\(n\\ge15\\)，觉得太多可以去讨价还价 真的不是把学生当廉价数据工？really？ 而这只是第一次实验。问助教的回复如下： 这是一门选修课，你觉得烂可以退课。如果是必修的话明年来不就好了？反正明年实验肯定不是这个。 就不说后续实验课助教念PPT跑路、群里提问装死、没有回复和反馈这些破事了。 理论课也值得一喷，比如什么“动态类型都不是类型安全的”，什么“鸭子类型和函数式最近很火”，什么“统一建模语言是非常有市场的形式化方法，很流行”，以及这门课还是会点名签到的。 自动机 blei挺会讲课的，这门课也比较有意思。光听课看PPT也能学到很多有意思的，但是再深入就没了。 以及这门课的目的并不是普及计算理论，而是普及用各种计算模型来建模，这也是为什么最后一节课塞了很多很多的私货。 实验感觉难度一般，就是写起来没什么意思... 并发 想选并发是因为上过os和看了一点点The art of multiprocessor programming，结果发现这个并发和想象中的又不太一样。先是普及了一堆 memory model 的东西，然后又简单讲了几种并发的数据结构。 这门课今年把期末给干掉了，作业又不是特别的难，所以最后体验就还是不错的，如果能起得来还是建议选。 语概 为了看文院妹子选的课。 语言学大概讲的东西就是那些，看起来比较眼熟的也就是句法相关的内容。由于本质上是一门自然科学，所以会有非常非常多的理论解释同一个现象，并且这些理论很多时候是没法互相解释的。而更坑跌的是这些理论甚至很难证伪，而更多的通过一种“信念”维系学者们的偏好，我觉得这是很民科的。 当然罗老师是非常有激情的老师，可以感受到他对语言学的那种情感。课上的妹子也很多，我觉得选这门课很值。 毛概 经历了近代史换老师的惨败，我决定还是随大流了。 lhb几乎没啥事情，上课就讲讲他的光辉岁月，一学期只点了两次名，最后一周画了一些重点。 《新教伦理与资本主义精神》 老师挺帅的，不过我也不记得老师叫啥 这本书当成论文读就很好懂了，韦伯在论证两个东西的相关性，然后就是找证据列数据，再分开讲两个东西分别是什么，最后猜一个（几个）导致了这个相关性的原因。","tags":["Eureka Moments"]},{"title":"Automata06 Complexity","path":"/2022/12/08/Automata06-Complexity/","content":"Intro TM 是比较基本的计算模型，后续对“计算”本身的讨论也都会基于 TM 来做。 对于给定的问题，我们会粗略地考虑“计算难度”上的分类： 这个问题可以判定吗？ 如果是可判定的，它花费的计算代价（Cost）是可接受的（Tractable Problems）吗？ 一般的套路就是各种各样的规约，再加上各种各样已知的问题，在密码学的时候就应该见得很多了（虽然我学得很烂..） Decidability 从最大的讲起 \\(\\Sigma^*\\) 注意到 TM 本身可以被编码（七元组中的元素都是有限的）为串，因此 \\(\\lvert{\\Set{M\\mid M\\text{ is Turing Machine}}}\\rvert=\\aleph_0\\) 而 \\(\\lvert\\Sigma^*\\rvert=\\aleph_0\\)，因此所有语言组成的集合 \\(\\lvert 2^{\\Sigma^*}\\rvert &gt;\\aleph_0\\) 本身是不可数的，这说明不存在所有 TM 到所有语言的双射。 即必然存在某个语言没法用 TM 描述。 RE/Partially Decidable PD 的定义和 RE 是等价的，即不能保证停机的 TM \\(M\\) 定义出的语言恰好就是半可判定的。经典的半可判定算法有一阶谓词逻辑中判定 skolem 范式是否永真的那个算法。 co-RE 前面说过 RE 在补操作下不封闭，那么某个 RE 的补会是什么呢？ 定义 \\(L\\) 是 co-RE 当且仅当 \\(\\bar L\\) 是 RE。容易发现 \\(\\text{RE}\\subsetneq \\text{co-RE}\\)，且 \\(\\text{Decidable}=\\text{co-RE}\\cap\\text{RE}\\) Decidable 称必然停机的 TM \\(M\\) 为算法（Algorithm），\\(L(M)\\) 为递归语言或可判定 Complexity 对于可判定的问题才有讨论计算代价的必要，因此下面提到的 TM 默认都是算法（会停机），默认讨论的是时间复杂度。 渐进意义的复杂度已经讲烂了，这里只需要额外规定 TM 上的代价可以用磁头操作的次数表示（不动也算一次） TM 的输入规模用输入串的长度表示 同样是从最大的讲起 EXP 指需要代价为 \\(O(2^{\\text{poly}(n)})\\) 的 DTM。目前并不清楚是否有 \\(\\text{NP $\\subsetneq$ EXP}\\)。 PSPACE 指需要空间代价为 \\(O(\\text{poly}(n))\\) 的 DTM。 NP-hard 指难度不弱于 NP 的问题。即如果问题 A 满足 \\(\\forall B\\in NP\\) 都有 \\(B\\le_P A\\)，那么 A 就是 NP-hard 的。 这个定义可以拓展到 X-hard。即 \\(A\\in \\text{X-hard}\\iff \\forall B\\in \\text{X},B\\le_P A\\) NP-complete 指 NP 中最难的那一批问题。定义为 \\(\\text{NP-complete}= \\text{NP} \\cap\\text{NP-hard}\\) 这个定义可以拓展到 X-complete。 例子 HAMILTON 3-SAT k-CLIQUE NP 指需要代价为 \\(O(\\text{poly}(n))\\) 的 NTM。关于对 NTM 的时间复杂度的定义可以看这里。 另一个等价的定义是这样的： 对于一个 NP 问题（语言） \\(L\\)，我们总可以构造一个二元关系 \\(R\\) 使得 \\(L\\) 的一个等价定义为 \\(L=\\Set{w\\mid \\text{$\\exists s$, s.t. $(w,s)\\in R$}}\\)，并且 \\(R\\) 作为语言是个 P 问题。 这里的 \\(\\exists s\\) 是比较灵活的，并不需要严格是“问题本身的答案”。 一个经典的套路是用 DTM 模拟 NTM，此时状态数是指数增长的 \\(\\sum_{i}k^i\\)，其中 \\(k\\) 为 NTM 每一步的可选后继状态的最大值。 P 需要代价为 \\(O(\\text{poly}(n))\\) 的 DTM。 规约 已知问题 A 的可判定性/复杂性，用来证明问题 B 的可判定性/复杂性 Decidability 给几个不可判定的例子 HALT \\(A_{TM}=\\Set{\\braket{M,w}\\mid \\text{$M$ accepts $w$}}\\) \\(E_{TM}=\\Set{\\braket{M}\\mid L(M)=\\varnothing}\\) HALT 的证明最经典，剩下两个都可以规约到 HALT。 实际上根据 Rice's Theorem 可知，TM 上的非平凡性质都是不可判定的。 Complexity 涉及到复杂度的规约需要强调规约额外引入的复杂度。 如果问题 A 能被多项式规约到 B，且 B 是 P，那么 A 也是 P。 用符号描述就是 \\(A\\le_P B\\) 且 \\(B\\in P\\)，那么 \\(A\\in P\\)。 例子 k-SHORT-PATH 是 P 的 k-LONG-PATH 是 NP-hard 的，可以把 HAMILTON 规约到它 Rice's Theorem TM 上的非平凡性质不可判定。称性质 \\(P\\) 是非平凡的，当且仅当 \\(\\Set{M\\mid P(M)}\\) 和 \\(\\Set{M\\mid eg P(M)}\\) 都非空。 由反证法，设某个非平凡性质 \\(P\\) 存在判定算法 \\(D\\)，我们以此来构造一个用来判定 HALT 的算法 \\(H\\)。 由于 \\(P\\) 非平凡，因此必然存在 \\(M\\) 满足 \\(P(M)\\)。不妨假设 \\(\\varnothing ot\\in P\\)，否则取 \\(\\bar P\\) 即可。 我们的 \\(H\\) 设计如下： 读入 \\(\\braket{TM,w}\\) 构造一个算法 \\(S\\) 如下： 读入 \\(x\\) 模拟把 \\(w\\) 喂给 \\(TM\\) 执行完2之后，模拟把 \\(x\\) 喂给 \\(M\\)，然后输出结果 然后观察 \\(D(S)\\) 如果 \\(D(S)\\) 接受，则说明 \\(S\\) 最终等价与一个 \\(M\\)，意味着 \\(TM\\) 停机了 如果 \\(D(S)\\) 拒绝，则说明 \\(TM\\) 没停机，\\(L(S)=\\varnothing\\) 写成代码大概是这样 func H(TM, w) &#123; func S(x) &#123; TM(w); return M(x); &#125; if (D(S)) &#123; return true; &#125; else &#123; return false; &#125; &#125; 即我们利用了一个阻塞的计算过程使得 \\(S\\) 是否拥有性质 \\(P\\) 取决于 \\(\\braket{TM,w}\\) 是否停机，这样就可以利用 \\(P\\) 的算法来判定停机问题，从而导出一个矛盾。","tags":["Automata"]},{"title":"Automata05 TM","path":"/2022/12/07/Automata05-TM/","content":"Intro 图灵机最早是作为一种计算模型出现的，目的是讨论在这样一种机器上能解决什么样的问题。 形式化的图灵机定义为七元组 \\((Q,\\Sigma,\\Gamma,\\delta,q_0,B,F)\\)，含义分别为：状态集、输入符号集、纸带符号集、转移函数、初始状态、空符号、接受状态集。 和 PDA 的最大区别在于 TM 的纸带两端无限长 TM 可以读/写纸带 TM 的读写头可以左右移动 别的没有本质区别。 对于 TM 的 ID \\((q,\\alpha,\\text{Head})\\)，寸晷要求可以这么写：\\(\\gamma q\\beta\\)，其中 \\(\\alpha=\\gamma\\beta\\)，且读头在 \\(\\beta\\) 的第一个字符上。 ID 间的转移写作 \\(ID\\vdash ID&#39;\\) Recursive Enumerable 给定 TM \\(M\\)，有两种定义其接收语言的方法 定义 \\(H(M)=\\Set{w\\mid M(w)\\text{ 停机}}\\) 定义 \\(L(M)=\\Set{w\\mid q_0w\\vdash^* ID(q,\\alpha,\\text{Head})\\wedge q\\in F}\\) 由 TM 定义的语言叫做递归可枚举语言（Recursive Enumerable Language） 定理： 任给 \\(L=L(M)\\)，存在 \\(M&#39;\\) 使得 \\(L(M)=H(M&#39;)\\)； 任给 \\(L=H(M)\\)，存在 \\(M&#39;&#39;\\) 使得 \\(H(M)=L(M&#39;&#39;)\\) 证明： 对 \\(M\\) 稍作修改即可得到 \\(M&#39;\\) 把所有接收状态的所有后继状态都删掉 对于非接收状态的未定义后继状态，设成一个死循环（永远往右走） 同样对 \\(M\\) 稍作修改即可得到 \\(M&#39;&#39;\\) 把所有未定义后继状态的转移都重定向到新造的接受状态 注意到1的证明可能会删去一些路径：先进入接收状态，然后走出去，再次进入后停机。在修改过后这样的trace就没法出现了。但是根据定义可知，\\(w\\in L(M)\\) 当且仅当存在一个 \\(ID\\) 使得 \\(q_0w\\vdash^* ID\\) 且 \\(ID\\) 是接收状态，因此我们仍然可以接收这个串。即只要有一次进入接收态就算接收。 Recursive 给定 TM \\(M\\)，且保证 \\(M\\) 停机，则称 \\(L(M)\\) 是递归语言（Recursive Language） 这样的 \\(M\\) 也叫算法（Algorithm） 这样的定义意味着纯粹的 TM 不一定停机，也可能从未进入接收状态。 Variation 这些变体都是等价的，即它们作为整体接收的语言都是同一类语言——递归可枚举语言。 Multiple Tracks 原本纸带上的每个位置只有一个符号，现在纸带上的每个位置可以有 \\(k\\) 个符号。例如内存是以字节（8位）为单位的。 通过简单的编码就可以证明计算能力（capability）与单个纸带的等价性。 Marks 利用 Multiple Track 可以给不同的位置打上标记 Registers 状态集可以带上寄存器，其中装着有限长度的串。 Semi-infinite Tape 即单边无限的纸带。注意到存在平凡双射 \\(\\mathbb N\\mapsto \\mathbb Z\\) ，因此正常 TM 的操作同样可以在 ST-TM 上完成。 Multiple Tapes 注意和 Multiple Tracks 的区别，这里是可以有 \\(k\\) 个读写头独立操作的。 这仍然和一般路过 TM 在计算能力上等价，即单个读写头可以模拟多个读写头并行操作的情况（单核CPU仍然可以跑多任务）。 Nondeterministic NTM \\(N\\) 的每一步有多种选择可以走。定义 \\(L(N)\\) 为所有能走到接收状态的串 \\(w\\)。 注意到 \\(N\\) 本身也是可以编码的，可以用一个 TM \\(M\\) 来 bfs 模拟 \\(N\\) 的执行，也就是做 model checking。 并且由于前面这些等价的拓展，我们可以假定被编码的 TM 尽可能简单（例如都是 ST-TM），而作为 host 的外层 TM 可以尽可能复杂（例如可以有多个读写头），这样会比较方便。 Key-Value Store 新开两条纸带分别存 Key 和 Value。插入和删除都在纸带上扫。 Closure Property 不加解释说明是递归语言和递归可枚举语言共同具有的特性。 \\(L(M_1)\\cup L(M_2)\\) 构造 \\(M\\)： 有两个纸带，分别跑 \\(M_1,M_2\\) 任意一个接收都接收。 \\(L(M_1)\\cap L(M_2)\\) 和上面是类似的 \\(\\overline{L(M)}\\) 对递归语言而言是封闭的。由终止性可知 \\(M\\) 必然会给出判定，只需要把结果取反即可。 对递归可枚举语言而言是不封闭的。证明： 若 \\(L\\) 和 \\(\\bar L\\) 都是递归可枚举的，那么可以取 \\(M,\\bar M\\) 分别为判定 \\(L,\\bar L\\) 的 TM。 对于输入 \\(w\\) 分别喂给 \\(M,\\bar M\\) 若 \\(M\\) 停机，则停机进入接收态 若 \\(\\bar M\\) 停机，则停机进入非接收态 这说明 \\(L\\) 将会是递归语言。 注意到我们的论证仅依赖于 \\(L\\) 及 \\(\\bar L\\) 是递归可枚举的，因此一切递归可枚举语言都是递归语言。 上面这条很容易举出反例（例如后面会说的停机问题），矛盾；故假设不成立。 \\(LR\\) 对于递归语言的联，只需要枚举分界线，然后用两个 TM 分别判定就可以了。为了避免麻烦的停机，这里的分界线枚举可以用 NTM。 对于递归可枚举语言需要保证两个 TM 是并行跑的（调度公平即可） \\(L^R\\) 只需要用 TM 把输入翻转一次就好了。 \\(f^{-1}(L)\\) 对于输入 \\(w\\) 得到 \\(f(w)\\)，然后放在 \\(L\\) 的 TM 上跑一下得到结果。","tags":["Automata"]},{"title":"Automata04 PDA","path":"/2022/11/09/Automata04-PDA/","content":"Intro 一般说的 PDA 本质上是带栈的 \\(\\epsilon\\)-NFA。PDA 这块可以看到非确定性引入的能力差别，这在正则语言、递归可枚举语言里是没有的。 Def 形式化的 PDA 定义为七元组 \\((Q,\\Sigma,\\Gamma,\\delta,q_0,Z_0,F)\\)，其中 \\(\\Gamma\\) 表示栈上的符号集，\\(Z_0\\in\\Gamma\\) 表示栈上的初始符号。 转移函数 \\(\\delta\\) 定义为 \\(\\delta(q,c,Z)=\\Set{(p,\\alpha)\\mid p\\in Q, \\alpha\\in\\Gamma^*}\\)，表示在状态 \\(q\\) 下看到符号 \\(c\\) 和栈顶为 \\(Z\\) 时，可以从集合中随便挑一个元组 \\((p,\\alpha)\\)，然后走到状态 \\(p\\)，弹出栈顶 \\(Z\\)，压入一串新的栈符号 \\(\\alpha\\)。 ID 要描述一个运行中的 PDA 状态只需要一个三元组 \\((q,w,\\alpha)\\)，其中 \\(q\\in Q,w\\in\\Sigma^*,\\alpha\\in \\Gamma^*\\)。 若 \\(w=cx\\)，\\(\\alpha=Z\\beta\\)，且 \\((p,\\gamma)\\in\\delta(q,c,Z)\\)，那么当前状态可以转移到 \\((p,x,\\gamma\\beta)\\)，记成 \\((q,w,\\alpha)\\vdash(p,x,\\gamma\\beta)\\)。由 \\(\\vdash\\) 归纳得到 \\(\\vdash^*\\) 是简单的。 有定理：若 \\((q,w,\\alpha)\\vdash^* (p,y,\\beta)\\)，那么对一切 \\(x\\in\\Sigma^*,\\gamma\\in \\Gamma^*\\) 都有 \\((q,wx,\\alpha\\gamma)\\vdash^*(p,yx,\\beta\\gamma)\\)，反过来不成立。 直观地讲，这是因为栈单边有限，前一个转移序列的成立说明用不到后续的符号，因此可以任意叠。反过来不成立也是因为去掉底部垫着的符号可能会对空栈做 pop 操作。 \\(L(P)\\) 对于给定的 PDA \\(P\\)，一种定义它接收的语言的方法为 \\(L(P)=\\Set{w\\in \\Sigma^*\\mid (q_0,w,Z_0)\\vdash^* (q,\\epsilon,\\alpha), q\\in F}\\) 即如果消耗完了整个串之后进入了接收状态，那么就认为这个串被接收，很符合命名。 \\(N(P)\\) 另一种定义为 \\(N(P)=\\Set{w\\in\\Sigma^*\\mid (q_0,w,Z_0)\\vdash^*(q,\\epsilon,\\epsilon)}\\) 即一旦栈空了，那么就认为这个串被接收。 \\(L(P),N(P)\\) Equivalence 任给一个 \\(P\\)，我们必然可以构造一个 \\(P&#39;\\) 使得 \\(L(P)=N(P&#39;)\\) 只需要在 \\(P\\) 的接收状态之后通过自环不断弹栈直到空即可。 但这样可能会引入问题：在 \\(L(P)\\) 中可能经过某些非接收状态，并且栈恰好是空的。如果我们什么都不做的话就会引入原本无法接收的串。 一个技巧是 \\(P&#39;\\) 引入一个新的栈符号 \\(Z&#39;\\) 放在栈底，然后再模拟 \\(P\\)，这样由于 \\(P\\) 没有针对 \\(Z&#39;\\) 的转移，因此无论如何都不会空栈；并且最后的清理状态一视同仁地弹栈，也能保证接收。 任给一个 \\(P\\)，我们必然可以构造一个 \\(P&#39;&#39;\\) 使得 \\(N(P)=L(P&#39;&#39;)\\) 同样引入 \\(Z&#39;\\)，如果遇到 \\(Z&#39;\\) 在栈顶就直接进入接收状态。即令 \\(\\forall q\\in Q,(F,Z&#39;)\\in\\delta(q,\\epsilon,Z&#39;)\\) DPDA 一般的 PDA 说的是 NPDA DPDA 即必须有 \\(\\forall c\\in\\Sigma,\\lvert\\delta(q,c,Z)\\rvert=1\\)，且 \\(\\forall c\\in\\Sigma,\\lvert\\delta(q,c,Z)\\rvert + \\lvert \\delta(q,\\epsilon,Z)\\rvert &lt;2\\) 成立 确定性的引入带来了能力上的损失，考虑语言 \\(\\Set{ww^R\\mid w\\in\\Sigma^*}\\)。由于确定性可知对于串 \\(0^n110^n\\)，DPDA 必然会在某个位置把串划分为前后两部分，不妨假设这个 DPDA 正确地划分出了 \\(0^n1|10^n\\)。考虑新的输入 \\(0^n11110^n\\)，这个新的输入和之前的输入有相同的前缀，因此必然有相同的分隔位置，但是是错的。 Containment \\[ \\text{RE $\\subseteq$ DPDA(L(P)) $\\subseteq$ NPDA} \\\\ \\text{RE $ ot\\subseteq$ DPDA(N(P)) $\\subseteq$ NPDA} \\] 第一行是比较显然的。 第二行的不满足可以考虑 \\(\\Set{0}^*\\) 这个语言，它是正则的，但不存在 \\(P\\) 使得 \\(\\text{DPDA(N(P))}=\\Set{0}^*\\) Ambiguity 若存在 DPDA \\(P\\) 使得 \\(L=L(P)\\)，那么 \\(L\\) 必然存在非歧义的语法，反过来则不一定成立（考虑 \\(\\Set{ww^R\\mid w\\in \\Sigma^*}\\)） 由 DPDA 可知任意串存在唯一确定的 ID 推导序列，这唯一对应了一个文法中的推导序列。 CFG Equivalence 任给一个 CFG \\(G\\)，都能构造出一个 PDA \\(P\\)，使得 \\(L(G)=L(P)\\)；反过来也成立。 对于给定的 \\(G\\)，构造只有一个状态的 \\(P\\)，初始栈上放着变量 \\(S\\)，每次从栈顶弹出符号 \\(V\\) 存在产生式 \\(V\\to w\\alpha\\)，其中 \\(w\\) 是终止符号的序列，那么就消耗掉 \\(w\\)，压入 \\(\\alpha\\) 存在产生式 \\(V\\to \\beta\\)，其中 \\(\\beta\\) 的开头是非终止符，那么就压入 \\(\\beta\\) 运用人类智慧可以发现这个东西本质上是在做非确定性的 DFS 对于给定的 PDA \\(P\\)，构造文法 \\(G\\)，其中变量都是 \\([pXq]\\) 的形式，我们希望有 \\(L([pXq])=\\Set{w\\mid (p,w,X)\\vdash^*(q,\\epsilon,\\epsilon)}\\) 成立 接下来对状态 \\(p\\) 处的转移分类讨论： \\((q,\\epsilon)\\in \\delta(p,c,X)\\)，说明这个转移只弹出 \\(X\\)。因此引入产生式 \\([pXq]\\to c\\) \\((r,Y)\\in\\delta(p,c,X)\\)，说明这个转移会先读一个 \\(c\\)，再走到 \\(r\\)，然后把栈顶的 \\(X\\) 换成 \\(Y\\)。因此引入产生式 \\([pXq]\\to c[rYq]\\) \\((r,YX)\\in\\delta(p,c,X)\\)，即压入一个 \\(Y\\)。我们枚举弹出当前 \\(X\\) 之后的状态 \\(s\\)，引入产生式 \\([pXq]\\to c[rYs][sZq]\\) 有这几个就足够了。最后还需要引入一个全新的初始变量 \\(S\\)，然后规定 \\(\\forall p, S\\to [q_0Z_0p]\\)","tags":["Automata"]},{"title":"Automata03 CFL","path":"/2022/11/09/Automata03-CFL/","content":"Intro 上下文无关语言（Context Free Language）是一类比正则语言更强的语言。描述上下文无关语言的文法叫上下文无关文法（CFG）。 CFG 上下文无关文法定义为四元组 \\(G=(T,V,S,P)\\)，其中 \\(T\\) 是符号集，也叫Terminals \\(V\\) 是变量集 \\(S\\) 是初始变量 \\(P\\) 是产生式的集合（Set of Productions） 定义 \\(T^*\\) 是串，\\((T\\cup V)^*\\) 是句子（sentential form） 上下文无关文法的大概想法是这样的：我们总是可以从初始变量开始，通过不断地把一个句子中的符号替换成另一个句子，最终得到一个串。这里“替换”指的就是关于产生式的应用，并且“上下文无关”的名字来源于“我们可以无脑替换”这一操作策略。 一个比较无聊的帮助理解的例子是这样的：可以把变量看成 STLC 里的变量，然后在最前面加上 quantifier，这样所谓的应用产生式就是 function application，串就是 value，产生式就是 function。 若 \\(S\\) 能经过若干步替换得到句子 \\(\\alpha\\)，我们就写作 \\(S\\Rightarrow^* \\alpha\\)，称 \\(S\\) 推导出（derives）\\(\\alpha\\)。通常不要求 \\(S\\) 一定是变量。 很显然 CFG \\(G=(T,V,S,P)\\) 定义的语言就是 \\(\\Set{s\\mid S\\Rightarrow^* s}\\) Derivation 这部分比较绕。 注意到在定义“推导”的时候我们并没有规定替换符号的顺序，因此给定语法 \\(G\\) 时，对于某个串 \\(s\\) 可能存在多个推导序列。一个平凡的例子是 \\(S\\rightarrow AA, A\\rightarrow 0\\) \\(S\\Rightarrow AA\\Rightarrow 0A\\Rightarrow 00\\) \\(S\\Rightarrow AA\\Rightarrow A0\\Rightarrow 00\\) 当然我们更关注的其实是推导作为树的结构，而不是线性序的形状，因此可以通过限定变量替换的顺序来得到推导树与推导序列的双射。常用的就是最左和最右推导。 关于双射的证明可以用简单的归纳得到... Ambiguity 消除了线性序上的歧义还不够，对于给定的 \\(G\\) 和串 \\(s\\)，如果存在 \\(s\\) 的两个不同推导树，那么就称 \\(G\\) 是有歧义的（ambiguous）。由推导树与最左推导的双射可知，\\(G\\) 有歧义 iff 存在 \\(s\\) 有两个不同的最左推导。 注意到这里的歧义是语法的性质，如果是编译课就会讲怎么设计文法消除歧义。但实际上并非所有的上下文无关语言都能消除歧义，一个例子是 \\(L=\\Set{0^i1^j2^k\\mid i=j\\vee j=k}\\)： 先推出 \\(i=j\\)，再推 \\(j=k\\) 先推出 \\(j=k\\)，再推 \\(i=j\\) Normal Form 讲的是对于给定的语法 \\(G\\)，怎么化简得到等价的 \\(G&#39;\\)。 这里需要特殊处理一下 \\(\\epsilon\\)。若 \\(\\epsilon\\in L(G)\\)，那么我们实际上可以构造 \\(G&#39;\\) 使得 \\(L(G&#39;)=L(G)-\\Set{\\epsilon}\\)，最后单独考虑 \\(\\epsilon\\) （通过单独加产生式）。之所以特殊处理是为了形式更简单更好看（\\(\\epsilon\\) 在连接操作下是幂等的，可以任意出现而不改变语言，神出鬼没）。 \\(\\epsilon\\)-Production 若 \\(V\\Rightarrow \\epsilon\\)，那么就称这个产生式是 \\(\\epsilon\\)-Production，这一步的目的是去掉长成这样的产生式。 Identification 归纳地定义变量上的谓词 \\(\\text{Nullable}\\) 如下： 若 \\(V\\to \\epsilon\\)，则 \\(\\text{Nullable}(V)\\) 若 \\(V\\to \\alpha\\)，且对 \\(\\alpha\\) 中的任意变量 \\(X\\) 都有 \\(\\text{Nullable}(X)\\)，则 \\(\\text{Nullable}(V)\\) 容易看出 \\(\\text{Nullable}(V)\\) 的含义是 \\(V\\Rightarrow^* \\epsilon\\)。 Removal 对于一条产生式 \\(V\\to \\alpha\\)，我们可以根据 \\(\\text{Nullable}()\\) 的结果将这条产生式变成若干产生式。 具体而言是这样的，假设 \\(A\\in\\text{Nullable}\\)，那么我们必然可以引入中间变量 \\(A&#39;\\) 写成如下形式： \\(A\\to \\epsilon\\) \\(A\\to A&#39;\\)，且 \\(A&#39; ot\\in \\text{Nullable}\\) 当 \\(A\\) 出现在产生式的右侧时，我们不能直接删去1，考虑这个例子： \\(S\\to AB\\)，\\(A\\to \\epsilon\\mid a\\)，\\(B\\to b\\)，这个语法规定的语言是 \\(\\Set{ab,b}\\)。假设我们删掉了 \\(A\\to\\epsilon\\)，那么语言就会变成 \\(\\Set{ab}\\)，这样引入了语义变化。 注意到表达“空”有两种办法： 变量推导出 \\(\\epsilon\\) 从产生式里删掉这个变量 因此可以先构造 \\(S\\to A&#39;B\\mid B\\) 来语法化表示 \\(\\epsilon\\)，此处的 \\(A&#39;\\) 定义为 \\(A&#39;\\to a\\)，这样就维持了语义等价。 推广到产生式右侧有 \\(k\\) 个 \\(\\text{Nullable}\\) 变量，那么就是新构造 \\(2^k\\) 个产生式，然后分别递归消消乐就好了。 Unit Production Unit Production 说的是形如 \\(A\\to B, B\\to C\\) 这样的产生式。 如果 \\(A\\Rightarrow ^* B\\)，且 \\(B\\to \\alpha\\)，\\(\\alpha\\) 中不止一个变量，那么就引入新产生式 \\(A\\to \\alpha\\)。 反复添加直至收敛，然后就可以删去那些 Unit Production 了。 这个有点 copy propagation 的感觉，或者可以认为是图的 contraction。 Divergent 如果一个符号 \\(A\\) 满足 \\(L(A)=\\varnothing\\)，那么以 \\(A\\) 为起点的推导将永不终止。这样的符号也可以直接删掉。 Identification 构造谓词 \\(\\text{Stoppable}\\) 如下： 若 \\(V\\to s\\)，那么 \\(\\text{Stoppable}(V)\\) 若 \\(V\\to\\alpha\\)，且 \\(\\alpha\\) 中的一切变量 \\(X\\) 都有 \\(\\text{Stoppable}(X)\\)，那么 \\(\\text{Stoppable}(V)\\) Removal 对于 \\(V ot\\in \\text{Stoppable}\\)，删去 \\(V\\) 所处的产生式（左侧和右侧都算出现） Unreachable 若 \\(S ot\\Rightarrow^* V\\)，那么 \\(V\\) 就是 unreachable symbol，它和它的产生式都可以直接删掉。 这个就是 deadcode 了。 CNF 大名鼎鼎的 Chomsky Normal Form 大概意思是在做完上面那些 clean up 之后，我们的语法中的产生式右侧长度至少为 \\(2\\)（没有 Unit Production 和 \\(\\epsilon\\)-Production） 那么可以人为地引入变量把语法树变成二叉的，并且叶子全是 \\(V\\to c\\)，\\(c\\in\\Sigma\\) 这样的形式。 之所以这么做是为了证明上的方便，比如 CNF 的形式可以给树高估一个界。","tags":["Automata"]},{"title":"Automata02 RE","path":"/2022/11/09/Automata02-RE/","content":"Intro 正则表达式（Regular Expression）是描述正则语言的另一种方法，有时候用起来会比 DFA 更方便。 Definition 给出归纳定义： 单个字符 \\(a\\in\\Sigma\\) 是正则表达式 \\(\\epsilon\\) 是正则表达式 \\(\\varnothing\\) 是正则表达式 如果 \\(R_1,R_2\\) 是正则表达式，那么 \\(R_1+R_2\\) 也是正则表达式 如果 \\(R_1,R_2\\) 是正则表达式，那么 \\(R_1R_2\\) 也是正则表达式 如果 \\(R\\) 是正则表达式，那么 \\(R^*\\) 也是正则表达式 正则表达式仅限于此 同时给出语义函数 \\(L(a)=\\set{a}\\) \\(L(\\epsilon)=\\set{\\epsilon}\\) \\(L(\\varnothing)=\\varnothing\\) \\(L(R_1+R_2)=L(R_1)\\cup L(R_2)\\) \\(L(R_1R_2)=L(R_1)L(R_2)\\) \\(L(R^*)=L(R)^*\\) 我们称一个语言 \\(L\\) 是正则语言（Regular Language），当且仅当存在一个正则表达式 \\(R\\) 使得 \\(L(R)=L\\)。 Equivalence 任给一个正则表达式 \\(R\\)，都能构造一个 \\(\\epsilon\\)-NFA \\(N\\) 使得 \\(L(R)=L(N)\\) 这个构造具体可以看之前写过的 RE-compiler 任给一个 DFA \\(D\\)，都能构造一个 \\(R\\) 使得 \\(L(D)=L(R)\\) 大概的想法是这样的：对于任意一个 DFA \\(D\\)，我们总可以挑出一个状态 \\(q\\in Q_D\\)，使得在进行一次状态削减操作后得到新的 DFA \\(D&#39;\\)，且 \\(Q_{D&#39;}=Q_D-\\set{q}\\)。这样在至多 \\(\\lvert{Q}\\rvert\\) 次操作后就能得到状态数为 \\(2\\)、仅含有一条边的 DFA。这时唯一的边上的标号即为要求的正则表达式。 PPT 上 k-PATH 的说法本质上是按照编号递增的顺序去缩减状态。 Properties Membership 即给定串 \\(w\\) 和正则语言 \\(L\\)，判定是否有 \\(w\\in L\\) 只需要找到 \\(L\\) 的 DFA \\(D\\)，然后跑一下 \\(w\\) 即可 Emptiness 给定正则语言 \\(L\\)，判定是否有 \\(L=\\varnothing\\) 只需要取 DFA \\(D\\)，然后从 \\(D\\) 的初始状态出发广搜，看看能不能找到接收状态即可 Infiniteness 给定正则语言 \\(L\\)，判定是否有 \\(\\lvert L\\rvert=\\infty\\) 只需要取最小 DFA \\(D\\)，去掉不可达状态，判断 \\(D\\) 中是否存在以初始状态为起点的圈即可。 Equivalence 给定两个正则语言 \\(L_1,L_2\\)，判定是否有 \\(L_1=L_2\\) 取两个 DFA \\(D_1,D_2\\)， 只需要取乘积自动机 \\(D_1\\times D_2\\)，然后令 \\(F&#39;=\\Set{(q_1,q_2)\\mid\\text{$q_1$ 和 $q_2$ 恰有一个是接受状态}}\\) 可以发现 \\(w\\) 被 \\(D_1\\times D_2\\) 接收当且仅当 \\(w\\) 恰好被 \\(D_1\\) 或 \\(D_2\\) 中的一个接收。因此 \\(D_1\\times D_2\\) 的 emptiness 就能说明 \\(D_1,D_2\\) 的等价性。 类似的还可以做包含关系等等集合操作。 Pumping Lemma 若 \\(L\\) 是正则语言，则存在泵常数 \\(n\\) 使得一切长度大于等于 \\(n\\) 的串 \\(s\\) 都能写成 \\(xyz\\) 的形式，满足 \\(\\lvert y\\rvert &gt;0\\) \\(\\lvert xy\\rvert \\le n\\) \\(\\forall i\\in\\mathbb N, xy^iz\\in L\\) 证明是比较简单的，只需要取 \\(n=\\lvert Q_\\text{DFA}\\rvert\\) 就有串 \\(s\\) 在 DFA 上跑的时候必然经过了重复的两个状态。这个圈可以不断重复（或删去），最终仍然能在 DFA 上走到终点。 Closure 对正则语言做一些操作之后仍然会得到正则语言 Union, Intersection, Difference, Complement 用乘积自动机。Complement 本质上是 \\(\\bar L=\\Sigma^*-L\\)，而 \\(\\Sigma^*\\) 是正则的 一个比较好用的性质是 \\(L\\cap R\\) 仍然是正则的，我们可以构造一个特殊的 \\(R\\) 来“规范”字符出现的顺序，并利用反证法证明 \\(L\\) 不是正则的。例如 \\(\\Set{s\\mid s\\in\\set{0,1}^*, \\lvert s\\rvert_0=\\lvert s\\rvert_1}\\) Concat, Kleene Closure 这两个都是正则表达式里的操作，只需要取出语言对应的 RE 然后操作一下就好了 Reversal 定义函数 \\(\\text{rev::RE$\\to$RE}\\) \\(\\text{rev($a$)=}a\\) \\(\\text{rev($e_1+e_2$)=}\\text{rev($e_1$)}+\\text{rev($e_2$)}\\) \\(\\text{rev($e_1e_2$)=}\\text{rev($e_2$)}\\text{rev($e_1$)}\\) \\(\\text{rev($e^*$)=}\\text{rev($e$)}^*\\) Homomorphism 同态说的是给定 \\(\\Sigma_1\\) 上的语言 \\(L_1\\) 和函数 \\(f\\colon\\Sigma_1\\to {\\Sigma_2}^*\\)，定义 \\(\\text{map}(f,s)=\\prod_{i=1}^{\\lvert s\\rvert} f(s_i)\\)，那么可以构造新语言 \\(L_2=\\Set{\\text{map}(f,s)\\mid s\\in L_1}\\)。称 \\(L_2\\) 是 \\(L_1\\) 关于 \\(f\\) 的同态，记为 \\(L_2=f(L_1)\\) 只需要取 \\(L_1\\) 的正则表达式，然后把所有符号映射一下就得到 \\(L_2\\) 的正则表达式了。 Inverse Homomorphism 说的是给定 \\(L_2\\) 和 \\(f\\)，求 \\(L_1=\\Set{s\\mid \\text{map}(f,s)\\in L_2}\\)，记作 \\(L_1=f^{-1}(L_2)\\) 这里并不要求 \\(f\\) 是可逆的 取 \\(L_2\\) 的 DFA \\(D_2\\)，并以此构造 \\(D_1\\)：\\(\\delta_1(x,c)=\\delta_2(x,f(c))\\)，其余一模一样","tags":["Automata"]},{"title":"Concurrency05 Linearizability","path":"/2022/11/07/Concurrency05-Linearizability/","content":""},{"title":"Concurrency04 Promising","path":"/2022/11/07/Concurrency04-Promising/","content":"Intro C11 model 的最大问题有这么几个 OOTA，这说明 C11 model 太弱了 UB，这让一些 type-safe 的语言没法用这个 model，因为 UB 本质上和 type safety 冲突。 所以几个大家都很熟悉的人提出了 promising semantics，claim 他们解决了 OOTA 的问题，同时还是 operational 的，这样就不会有 UB。 当然这一段我不是太感冒，就当是夹带私活了。 Idea promising 的叫法来自于 model 中的一个特殊操作，make promises。 这个操作的意义在于，一个线程可以在真正操作内存之前，向其它线程claim它将会完成一些内存的修改（写入）。 在这个 model 中，内存 \\(M\\) 被视为一个消息的集合（set of messages），每个消息是三元组 \\(\\left&lt;x=v \\text{ @t}\\right&gt;\\)，表示在 timestamp \\(t\\) 时刻，\\(x\\) 将被写入值 \\(v\\)。每个线程对内存的视角（view）可以不同，这里视角定义为函数 \\(\\text{loc}\\mapsto M\\vert_{\\text{loc}}\\)，论文要求每个线程的 view 关于 timestamp 必须是单调的，同时在读写内存时有一些 view 的修改的规定。 为了解决 OOTA，论文还作出了 well-behaved 这样的要求：即任意时刻，每个线程都需要保证它确实能够完成它的 promise。当然这里的描述很模糊，具体可以去看论文。当然我觉得这样有些耍赖的感觉，毕竟每一步都要保证所有的结果都是可达的，自然就不存在 OOTA 了。 感觉之所以会讲这个主要是因为 PLAX 自己也在 follow 做相关的东西。但是在不涉及真的形式化和真的验证的时候，好像很难说你真的懂了这个，以及这个东西真的有用处。看看论文就差不多了吧。","tags":["Concurrency"]},{"title":"Linguistics02 Phonetics","path":"/2022/10/17/Linguistics02-Phonetics/","content":"Intro 语音学（Phonetics）的基本假设是这样的 语音存在最小的单位——音素 所有的人类都能发出相同的音 由1、2，存在一种通用的注音符号体系（IPA） 音素可以由若干音韵特征唯一决定 IPA 全称是 International Phonetics Alphabet，大概长这样 语音学家希望有一种唯一的表征音素的手段，于是就像门捷列夫发现元素周期表一样发明了 IPA。 当然这个表很难背，符号也需要适应 Articulate Phonetics 语音分为两类 元音（Vowels） 辅音（Consonants） 二者最大的区别就在于发声时气流是否受阻。 为了区分这些音，我们可以从 发声部位 发声方式 两种角度区分它们。 辅音 发声部位 翻译是发声语音学。为了理解声音是如何产生的，我们需要先知道正常人有哪些发声部位。 发声部位说的其实是说话时气流在哪里受阻。从外到内一共可以分成这么些。 Bilabial 双唇音，例如 [m] [b] [p] 之类。labial 是唇的意思 Labiodental 唇齿音，例如 [f] [v] 之类，dental 是牙的意思 Interdental 舌头插入上下齿中间，例如 [\\(\\theta\\)] Dental 舌头放在上齿后面，有些人这么发 [\\(\\theta\\)] Alveolar 舌头在牙槽嵴的位置，例如 [t] [d] [n]。大概就是硬腭和门牙之间的一个小小突起 Palatal 舌头卷起到硬腭的位置，例如 [j] [ʃ] [ʒ] Velar 舌头到软腭的位置，例如 [k] [g] Uvular 小舌音，这个我真发不出来。 Glottal 声带发声，例如 [h] [?]。这个 [?] 是类似于 uh-oh 中的音，感觉也比较难单独发出来。 发声方式 说的是仅依靠发声部位很难精确定位一个音，所以需要另一个维度的特征来给声音细分类。 发声方式的每个特征通常是二选一的，例如一个音要么是 voiced 要么是 voiceless 的。 Voiced/Voiceless 也就是清音和浊音，区别在于发声时声带是否振动。例如 [p] [b]、[k] [g] 就是一些清浊音对。 Aspirated/Unaspirated 说的是送气和不送气。对于清音而言还可以继续分成送气清音、不送气清音。 一个书上给的例子是 pit - spit。写法是 [\\(\\text{p}^\\text{h}\\)] - [\\(\\text{p}\\)]。 Nasal/Oral 也就是鼻音和口腔音。在发 [m] 的时候鼻腔通气，而 [b] 则不通气。 Stop/Continuant 阻塞音和连续音。例如 [b] [p] 都是阻塞音，而 [s] [z] 都是连续音。区别在于气流是否完全被阻塞。 Fricative 例如 [s] [z] 这样的丝擦音。 Affricate 特指 church、judge 这样的音。 Approximant [w] [j] [l] [r] Liquid 指的是 [l] [r] 这样气流从舌头两侧流过的音。 Glide 滑音，指的是 [w] [j] 这样有半元音感觉的辅音。 还有一些更难发的音例如 trills 和 clicks 就不放了，估计是不会考的。 元音 元音要简单得多 舌位高/低。像 [i] [u] 就是高舌位，而 [o] [a] 则是舌位偏低的 舌位前/后。像 [i] 就是前元音，[u] 则偏后。 唇的圆/扁。像 [i] 就是扁唇，[u] 则是圆唇。 鼻化元音。例如 bean，或者普通话的“安”。通过在元音上加波浪号来表示，例如 [\\(\\tilde {\\text{i}}\\)] 松/紧。像 beat - bit，IPA 是 [i] - [I]","tags":["Linguistics"]},{"title":"Linguistics01 Intro","path":"/2022/10/17/Linguistics01-Intro/","content":"Intro 回忆老罗讲的导论，感觉还是很有激情的。 语言学研究的是广泛的语言，它的一些基本假设是这样的 人之所以为人，是因为人有其它生物所不具备的能力和特质。 研究我们何以为人就是在研究人类的本质。 能使用语言是人类特有的能力和现象。 宽泛地说，语言学家的任务是通过研究一些语言中的现象，得出某些关于语言的规律，并尝试用于预测更广泛层面上的现象（例如同语言中的其它现象、其它语言中的类似情况等等）。这么看来还是和物理学家很像的。 Branches 语言学的分支大概可以分成三类，分别关于声音、语法、含义 Phonetics 就是语音学，关心的是音素（phone） 的物理属性，例如响度、音调 人们如何听懂和理解 如何产生，例如发声方式、发声部位等等 当然还有可以到处乱跑的田野语音学家和坐办公室的一般语音学家，那些喜欢录音和学乱七八糟的语言的一般就是了。 Phonology 翻译是音系学，关心的是音位（phoneme） 语音只是表象，人脑中真正的抽象表示是音位而非音素 不同音素可以对应一个音位，这说明语言有潜在的音位-&gt;音素变化规则 使用不同语言的人理论上能发的音是一样的（都有正常的发声器官），早成了各语言无法理解的原因是不同语言中音素与音位的对应关系不同。 这部分就比较有意思，假设和结果都很漂亮 Morphology 翻译是形态学，即造词法。关心的是语素（morpheme） 不同语言偏好不同的扩充词汇的手段（例如加词缀、造合成词、变格），这些手段又反过来影响语法的具体形态 当然语素究竟是什么也没有特别好的判定准则，并且通常作为 native speaker 也会有拿不准的情况。 一个例子： “俄罗斯” 和 “俄联邦” 分别有几个语素？那 “白俄罗斯” 呢？ Syntax 翻译是语法，但也叫句法学。关心的是如何通过词汇得到表达含义的句子。 涉及到的一个经典问题就是语序，通常形态学的一些特征会影响到句子的结构（例如格位变化多的语言可能就不那么在意语序）。 Semantics 咕咕 Pragmatics 咕咕","tags":["Linguistics"]},{"title":"Network03 Transport","path":"/2022/10/16/Network03-Transport/","content":"Intro 传输层向上给应用层提供进程间通信的服务，向下利用网络层提供的主机到主机的服务。 可以认为传输层本质是对网络层的复用(MUX - DEMUX)，因为实质上所有进程共享向外的网络通路，传输层负责把混在一起的网络层数据拆分整理成发到各个进程的数据（通过端口来区分）。当然实际上的做法更加聪明一些，通过 socket 就可以把进程和数据传输的目的地解耦，这样 OS 只需要通过端口号识别出 socket，然后进程主动和特定的 socket 建立连接即可。 通常把传输层的数据叫做 segment。 Socket &amp;&amp; Port 端口号用于区分主机上的不同进程，是16bit的无符号整数。著名端口号一般在0~1023中取。 无论是 TCP 还是 UDP，segment 中都包含了 srcPort 和 dstPort。这是方便了接收方回传消息。 接收方的 UDP_Socket 和 (dstIP, dstPort) 一一对应，这意味着两个来自不同主机/进程的 UDP 数据在给到同一个接收方的同一个端口号时，会被分发给同一个 UDP_Socket。 而 TCP_Socket 和 (dstIP, dstPort, srcIP, srcPort) 一一对应（面向连接），这意味着两个来自不同主机/进程的 TCP 数据必然会被分发给不同的 socket。 注意这里讲的是已经建立连接之后的情况。考虑连接的建立则是 发送方发送特殊的 TCP 数据，用一个 bit 标志表示希望建立连接，同时数据内有目标端口。 接收方的 OS 发现这是一个连接建立请求，因此拿着这个端口去寻找监听该端口且正在等待连接建立的进程（可能有多个） 建立一个新 socket，将四元组与这个 socket 建立对应。 UDP 不可靠，UDP 只是对 IP 的简单封装（包上端口号和checksum） 短（header）小（封装）快（没有拥塞控制） 无连接，对主机的资源消耗少 进程到进程（只有目标地址+端口） 虽然 UDP 看起来很挫，但是 UDP 本身封装得很少，实现简单，没什么包袱，基于 UDP 可以在应用层实现一些可靠的协议（同时比单纯的 TCP 高效），例如 QUIC。 Header srcPort dstPort length checksum TCP 可靠 拥塞控制 面向连接 进程到进程（源地址+端口，目标地址+端口） Re-implement TCP 这一段我还蛮喜欢的。下面谈到的适用于网络中的任何层 trivial 首先假设下层服务是可靠的，那么收发双方只需要直接调用下层服务即可，这样就能搭出一个可靠的服务。 bit flip/lost 假设下层服务只会出现数据错误（但仍然保序），这时引入 retransmission based ARQ protocols(Automatic Repeat reQuest)，通常需要有 错误检查，即需要能发现 bit flip。这里假设这样的办法总是存在的 接收方反馈，即接收后检查确认无误需要告知发送方。一般叫做 ACK(成功) NAK(失败)。 重传策略，通常是超时后重传或得知 NAK 后重传。 编号，防止接收方无法区分此次消息是重传还是正常发。例如发 m1，收m1发ACK，ACK丢重传 m1，这时接收方就不知道自己的 ACK 到底届到没，也就无从得知这时上一次的重传还是新消息。 注意到一个简单的策略是遇到错误就丢包，因此丢包和错位没有太大区别。 同时为了描述协议，通常用 DFA 的形式来规定状态和操作。 这里的协议状态也很简单，只需要在发送后等待 ACK，在获得 NAK 后重传并重新等待即可。","tags":["Network"]},{"title":"Automata01 FSM","path":"/2022/10/04/Automata01-FSM/","content":"Intro FSM(Finite State Machine/Finite Automata) 是一种 formal system，当然用处有很多了。 Basics String 取定一个字符集 \\(\\Sigma\\)，其中元素称为字符。 定义串是字符的有限序列，也可以归纳定义如下： \\(\\epsilon\\) 是特殊的串，称为空串 若 \\(\\alpha\\) 为串，且 \\(x\\in \\Sigma\\)，则 \\(x\\alpha\\) 也为串。此处表示为字符与串的顺序拼接。 Language 语言即串的集合，\\(\\varnothing\\) 和 \\(\\Sigma^*\\) 是两个平凡的语言 注意区分 \\(\\Set{\\epsilon}\\) 和 \\(\\varnothing\\) DFA Def 定义为五元组 \\[ FSM\\overset{\\text{def}}= (S, \\Sigma, \\delta, s_0, F) \\] 分别表示状态集、字符集、转移函数、初始状态、接受状态集。 可以通过简单的归纳由 \\(\\delta:S\\to \\Sigma\\to S\\) 构造 \\(\\hat\\delta:S\\to \\Sigma^*\\to S\\) 定义自动机接受串 \\(\\alpha\\) 当且仅当 \\(\\hat\\delta(s_0,\\alpha)\\in F\\) 对于自动机 \\(A\\)，记它所能接受的串的集合为 \\(L(A)\\)，称为自动机 \\(A\\) 表示的语言。 若某个语言 \\(L\\) 可被某个 DFA \\(A\\) 接受，则称 \\(L\\) 为正则语言 \\(\\Set{0^n1^n\\mid n\\in\\mathbb N^+}\\) 下面证明这个不是正则语言（好像证过很多次了） 由反证法设这是正则语言，则存在 DFA \\(A\\) 使得 \\(L(A)=\\Set{0^n1^n\\mid n\\in\\mathbb N^+}\\) 不妨设 \\(A\\) 状态数为 \\(m\\)，则必然存在 \\(0^m1^m\\in L(A)\\)。考虑 \\(A\\) 正在识别 \\(0^m1^m\\) 的前缀 \\(0^m\\)，由鸽笼原理可知必然存在状态 \\(s\\) 经过了两次，即 \\(A\\) 中存在一个不为空的圈，且圈上只有 \\(0\\)。这说明我们可以走出一个 \\(0^{m+k}1^m\\) 的串后到达接受状态，这与 \\(L(A)=\\Set{0^n1^n\\mid n\\in\\mathbb N^+}\\) 矛盾。 故假设不成立。 NFA Def 和 DFA 的区别仅在于转移函数 \\[ \\delta:S\\to \\Sigma\\to 2^S \\] 其含义为：从状态 \\(s\\) 出发，经过一条字符边后可能到达哪些状态，NFA 的转移不一定是唯一的。 不难发现 \\(\\delta\\) 本身是一个 monad，此时的 \\(\\hat\\delta=\\text{return s &gt;&gt;= }\\delta\\) 定义 NFA 接受串 \\(\\alpha\\) 当且仅当 \\(\\hat\\delta(s_0,\\alpha)\\cap F eq\\varnothing\\) Subset Construction 给定 NFA \\(A=(S,\\Sigma,\\delta,s_0,F)\\)，我们可以构造 DFA \\(A&#39;=(2^S,\\Sigma,\\delta&#39;,\\set{s_0},\\Set{s\\in 2^S\\mid s\\cap F eq \\varnothing})\\) 关键的 \\(\\delta&#39;\\) 可以用 monad bind 定义... Equivalence 我们说 DFA 和 NFA 的表达能力是等价的，意思是： 对于任意的 NFA \\(A\\)，都存在 DFA \\(A&#39;\\) 使得 \\(L(A)=L(A&#39;)\\)，反之亦然。 这也说明所有的 NFA 能表达的语言类与所有的 DFA 能表达的语言类相等。 \"\\(\\Rightarrow\\)\"： 考虑 \\(A\\) 的子集构造 \\(A&#39;\\)，即证明对任意的串 \\(\\alpha\\)，有 \\(\\delta(s_0,\\alpha)=\\delta&#39;(\\set{s_0},\\alpha)\\)。简单地对 \\(|\\alpha|\\) 归纳即可。 \"\\(\\Leftarrow\\)\"： 很显然 DFA 是一种特殊的 NFA。 \\(\\epsilon\\)-NFA Def 符号集加上 \\(\\epsilon\\) 引入 closure 操作 \\(C:S\\to 2^S\\)，含义为从某个状态出发，只走 \\(\\epsilon\\)-边能到达的状态集合。 这一定义可以通过 monad bind 简单拓展为 \\(C:2^S\\to 2^S\\)，称为状态集的 \\(\\epsilon\\)-closure。这个操作显然是幂等的。 不妨记不含 \\(\\epsilon\\)-边的转移函数为 \\(\\delta\\)，则加入 \\(\\epsilon\\)-边后的新转移函数可以简单定义为 \\(\\delta&#39;=C\\circ\\delta\\circ C\\)，意思是每走一步前后都做一次闭包。 Equivalence NFA 显然是一种特殊的 \\(\\epsilon\\)-NFA。 注意到上面的 \\(\\delta-\\delta&#39;\\) 提供了一种消去 \\(\\epsilon\\)-边的方法，这样就能由 \\(\\epsilon\\)-NFA 得到等价的 NFA 了。 Minimization \\(\\simeq\\) 定义 DFA 上状态等价 \\(\\simeq\\)，当且仅当以它们为起点能接受的串的集合相等。由此可以立即得到接受状态和非接受状态不等价（考虑 \\(\\epsilon\\)）。很显然这是一个二元等价关系。 并且有引理：若状态 \\(A_1,B_1\\) 不等价，且 \\(\\exists c\\in\\Sigma\\) 使得 \\(\\delta(A_2,c)=A_1,\\delta(B_2,c)=B_1\\)，那么 \\(A_2,B_2\\) 不等价。 引理的证明十分简单，由 \\(A_1,B_1\\) 的不等价性可知存在串 \\(s\\) 使得 \\(A_1,B_1\\) 中恰好一个接受 \\(s\\)，此时考虑 \\(cs\\) 即为区分出 \\(A_2,B_2\\) 的串。 引理的逆否形式也很有用，即等价状态的对应后继状态也会是等价的。 由此得到一个构造算法：首先区分出所有的终止状态和非终止状态，然后进行如下操作： 枚举每对暂时无法区分的状态对，看看它们的后继状态是否可区分 可区分，则它们也可以区分 不可区分，则它们暂时不可区分 更新分类，重复1操作直至收敛。 此时得到一个状态集的划分，由此构造新的 DFA 即为最小化的 DFA。 Proof 记最小化算法得到的 DFA 为 \\(\\bar A\\)，由反证法设存在更小 DFA \\(A\\) 与之等价 必然有 \\(\\bar A\\) 的初始状态与 \\(A\\) 的初始状态等价（这里是跨越 DFA 的等价） 任意等价的状态对，它们的后继状态也等价 因此必然存在 \\(\\bar A\\) 中两个状态它们与同一个 \\(A\\) 中状态等价，由传递性可知它们彼此等价，这与 \\(\\bar A\\) 的定义矛盾。","tags":["Automata"]},{"title":"Concurrency03 Axiomatic","path":"/2022/09/25/Concurrency03-Axiomatic/","content":""},{"title":"Concurrency02 Operational","path":"/2022/09/25/Concurrency02-Operational/","content":"Intro Memory Model 可以通过一种 Operational 的方式来定义，在这种定义下，我们可以看到一个结果是如何一步一步产生的。 简单地说，一个并发的系统可以看成是 CPU + 内存 两个模块化的部分共同组成的，其中 CPU 有并发的执行流和线程局部的寄存器，内存则可以和 CPU 交互进行读写。这么做的好处在于： CPU 通常没有太大的变化，可以单独抽象出来定义好，后续直接复用 内存通过接口与 CPU 交互，这样我们就可以简单地替换不同的内存模型观察执行的结果 CPU 这部分没啥好说的，主要是记号的问题。那几个函数的用法倒是挺巧妙的，可以把一些编号的信息 encode 到参数里，这样就看起来很简洁。 Memory Interface/Label 我到今天都不知道到底是 l 还是 I 注意到这里的 Interface 并不是一种具体的行为，而是描述了一个结果，即这部分与细节无关，仍然是 declarative 的。具体以 U(x, v1, v2) 举例。 U(x, v1, v2) 例如有条规则长这样 \\[ \\frac{I\\text{=U(x,s($e_r$),s($e_w$))}} {\\text{r:=}\\bold{CAS}\\text{(x,$e_r$,$e_w$)},s\\overset{I}\\Rightarrow \\bold{skip},s[r\\mapsto1]} \\] 可以发现 CPU 在获得内存返回值之前是不可能更改寄存器的值的，同理内存在获得 CPU 提供的值之前也不可能返回比较的结果，因此者无论如何都不是一次通信就可以实现的。 而这个接口记号的真正意义在于，它只描述了最后二者达成的共识：内存读到了 \\(s(e_r)\\)，写入 \\(s(e_w)\\)，具体的实现则无关紧要，并且这个结果的达成本身是原子的。 这部分讲得比较绕，但大概意思是这样。 Rules 线程内部的 step rules 还是很简单的，重点在于 CAS 和 FAA CAS(x, e1, e2) 即 Compare-And-Swap。含义为 返回内存中的值与 e1 比较的结果 如果相等，就写入 e2 需要注意的地方有两点： 此处的接口用的形式为 U(x, v1, v2)，含义为“x读到v1，写入v2” 如果不符合条件只会产生 R(x, v1) 的接口 CAS 通常会作为一个同步操作，这点在后面可以看到 FAA(x, e) 即 Fetch-And-Add Fence 一类特殊的指令，可以看成是对内存的状态控制指令。 Whole 系统作为整体的 step 可以分为 CPU 内部进行计算 内存内部进行状态的维护 CPU 与内存通过接口进行交互，共同走一步 通常把1、2叫做 silent transition，因为对外界而言它们是不可感知的。 然后就没了，那个习题挺简单的。","tags":["Concurrency"]},{"title":"Concurrency01 HMM","path":"/2022/09/25/Concurrency01-HMM/","content":"Intro 弱内存模型的出现，本质上是因为单线程的程序优化(编译优化/执行优化)中应用了许多的技术，这些技术在进入多线程编程时代之后使得一些符合直觉的假设不再成立，即内存模型变\"弱\"了。 为了 cover 这些已经成为历史包袱的优化，同时给出这些\"不合常理\"行为的边界，Memory Model 就出现了。即我们希望 Memory Model 不能太强，这是因为有许多现存的实现、优化 break 了强内存模型的假设，此乃现状； Memory Model 不能太弱，否则这意味着完全的不可控。 HMM 规定事件为原子的，通常只关注以下几类事件： 读内存 写内存 同步原语的操作，例如 lck_acq(), lck_rel() 根据各个线程的代码可以得到一个事件上的偏序 program order Happens-before Order 我们可以构造一个全局事件上的偏序关系 happens-before order： 若两个事件有 program order，那它们有 happens-before order 若两个事件是关于同一个同步对象的操作，那么它们有 happens-before order。例如对锁的 release() 必然在 acquire() 之后发生。 如果两个事件之间不存在 happens-before order，那么说它们是并发的。 HMM 说的是，一个读事件可以读到 在它之前发生的最后一个同地址的写入 与它并发的同地址的写入 在使用 HMM 的时候，通常会假设一个 outcome，然后写出 happens-before order，最后判断是否满足 HMM 的要求 Out of Thin Air HMM 只是一个模型，OOTA 这样的例子就能说明这一点。 OOTA 的意思是 HMM 这样先给出一个 outcome 后判断是否可行的模型，会存在一些反直觉的 outcome 符合。例如 thread1 &#123; r1 = x; y = r1; &#125; thread2 &#123; r2 = y; x = r2; &#125; 它的一个 outcome r1=42, r2=42 是符合 HMM 的。但可以发现整个程序都没有出现过可能产生 42 的常量，并且这个 42 可以是任意的值，所以说这是凭空产生的(Out of Thin Air) 也有一个解释是这样的：CPU 在投机执行的时候会猜一个 r1 = x 的结果，如果它恰好猜了 r1 = 42 那么就会出现这个局面。","tags":["Concurrency"]},{"title":"Network02 Application","path":"/2022/09/12/Network02-Application/","content":"Intro 应用一般跑在端系统上，此时网络对应用而言是透明的，所有的网络 API 起到进程间通信的作用。 Application architecture C/S 服务器长时间开机提供服务，客户端与服务器建立连接来交换数据。服务器通常有固定 IP。当然服务器可以有很多（集群） P2P 用户之间直接通信（当然也可能要预先借助服务器建立连接）。比如大学生都爱玩的 Bittorrent。 Process Communication 网络本质上提供的是进程间通信的服务，只不过这里的进程可以跑在不同的主机上。基于字符串的网络通信可以屏蔽很多底层细节。 在一对通信中，通常把提出建立连接的一方称为客户端，另一端称为服务端。 socket socket 是操作系统提供的通信 API，一个 socket 指示了一对连接的一端。unix socket 可以看成特殊的文件，向其中写入或读取就能完成消息的发送和接收。 当然，连接是建立在进程-进程之间的，因此 socket 在建立连接时需要获得 主机 IP 端口号 可以认为端口号上守候的进程即为目标通信进程。 Transport layer services 传输层提供的服务有几个可以度量的方面 Reliable data transfer，即是否向应用层提供可靠的传输服务 Throughput，某些传输层协议可以提供带宽保障 Timing，某些协议提供延时保障 Security TCP 面向连接，在应用层穿消息之前，传输层会首先进行若干次通信以建立连接、设定状态（称为握手）。在结束通信前需要解除连接。 可靠数据传输，保证顺序、正确、完整。 拥塞控制，降低流量强度 没有延时和带宽的保证 UDP 无连接，直接发 不可靠 没有延时和带宽的保证 TCP UDP 都是不保证安全性的，新的 TLS(Transport Layer Security) 解决了这个问题。TLS 实质上是在应用层实现的，即可以看成是 TCP 的应用层代理，所有原本途径 TCP 的数据现在绕道经过 TLS 加密。 而延时和带宽则是无法加抽象层解决的问题，本质是因为下层提供的服务太弱了（或没有利用好下层的强服务）。 至于为什么只做增量式的修修补补，我觉得这就是维护屎山艺术了。 Application layer protocol 本质上就是 API，和 PA 里面写过的 frame_buffer 作为显存的格式规定是一模一样的。协议可以是公开的（例如 HTTP）也可以是私有的（例如你自己写的即时聊天软件） Web web 就是很经典的 C/S 架构，用户只会在需要的时候向服务器发出请求。 文件（包括页面）被称为对象，页面中可以出现对不同对象的引用（通过 URL）。URL 则由 主机名 + 路径 构成，指向了一个对象。 HTTP/1.0 在建立 TCP 连接后，客户端将发起请求（提供 URL 和若干指令），等待服务器返回请求的对象。 无状态，即通信双方无需维护历史信息（client-server 都是纯函数） HTTP/1.1 额外支持了 持久化/非持久化 连接，即可以控制本次连接是否是一次性的（通信后即销毁）。持久化的通信可以减少 TCP 连接建立的次数。 cookie 本质上是对 HTTP 协议无状态的一个补丁，其中 cookie 随着 HTTP 消息的传输一起传输（实质上是 cookie 的标识符/编号） cookie 在客户端和服务器各存一份 书上的原话是 cookie creates a user session layer on top of HTTP，事实如此 HTTP/2 TCP 的特性之一是保证所有连接共享带宽，这样的特性使得浏览器倾向于打开更多的 TCP 连接来抢占更多的带宽。而 HTTP/2 则是为了缓解这样的情况，尽可能重用 TCP 连接，降低服务器的负载。 另一个问题是关于等待的。仅凭 URL 很难得知对象的大小，而大对象的传输又会推迟小对象的传输，这就使得浏览器倾向于打开更多 TCP 连接来并行地传输对象。 frames 简单的说就是在应用层做了传输层的活，请求和传输将会首先被切割成块，然后以块为单位做真正的请求和传输。这样就实现了带宽在文件之间的共享（也就是细化了并发的粒度）。没啥好说的。 priority 请求块可以被赋予优先级。是不是很像实时操作系统的套路？ push 服务端可以预先推送未经请求的对象给客户端，也就是协议级别的 prefetch Web cache/Proxy server 本质上是一台联网的计算机。用户的请求会被先给到 proxy server，然后由 proxy server 请求真正的页面、缓存、返回，很好理解。 之所以这么做是因为网络带宽由瓶颈链路决定，而代理服务器通常到客户端有较高的带宽，向外的请求则可以合并成一个降低流量强度。这么做要求请求的命中率要比较好。 当然 cache 就带来了缓存一致性的问题，解决的办法就是加一个 conditional get 请求，用来获得某个对象的修改时间。如果发现修改时间变动了说明缓存过期，那么重新 get 一次或者踢出缓存就好了。顺便一提，这也是社团仙贝 304 Not Modified 这个昵称的由来（ Mail 主要是几个比较有历史厚重感的协议 发 发送端通过 SMTP 协议发给服务器，发送服务器通过 SMTP 协议发给接收服务器。 SMTP 协议要求报文是ASCII可显示的，这座屎山使得中文和附件这些东西得通过特殊的编码来发送。 注意SMTP的报文本身有一层格式要求（表明身份、发出指令等等），同时邮件的内容也有格式的要求（主题、内容等等） 收 接收端则主动向接收服务器查询收件箱（HTTP/POP3/IMAP） 这么做的一个好处是客户端可以不需要随时在线，同时一个服务器可以服务很多客户端。 POP3收取后会删除服务器上的数据。 实际上收发邮件就是在做文本传输的事情，因此用 HTTP 协议也是可以的。 DNS 互联网上的主机需要区分彼此，因此都有一个（可以视为唯一的）标识符 IP 地址。但是为了方便称呼，还会有一个字符串组成的主机名。 类比文件系统，DNS 就是主机名解析。例如浏览器里输入的网址最后会被解析成 IP 地址（文件系统中打开的文件最后会被解析成inode）。DNS 是跑在 UDP 上的。 当然 DNS 有两层含义 DNS 系统，是一个世界范围的分布式解析系统，提供解析服务 DNS 协议，通过协议来进行报文传递 除了域名解析，DNS 还能做 Host aliasing. 即可以给一个主机赋予不同的名字，其中一个叫 canonical name，其余的都是 alias。 Load distribution. 可以把一个主机名解析到不同的 IP 上，实现负载均衡。 DNS Server Hierarchy Root server，实际上有很多个 TLD server，TLD 对应于域名中最后一段，例如 .cn .jp .fr .org。每个 TLD 都会有自己的 DNS 服务器。 Authoritative server。每个 hostname 都将最终被一台 DNS server 给解析，这样的 server 叫做这个 hostname 的 authoritative server。 本质上是把域名按照 \".\" 划分成若干段，每段只由一类服务器负责。 网上则还有两层，每次解析一般会按照顺序从上到下查询 DNS server，最终定位到一个 authoritative server，然后从中获得 IP 地址。 当然 ISP 还可以提供 local DNS server 这样的东西，本质上是一个 DNS proxy。 recursive query，说的是客户端的解析请求由另一个主机代理完成查询，仅和代理主机通信。 iterative query，说的是客户端的解析请求全部由自己完成，其与其它主机直接通信。 DNS server 内部还可以做 caching，没啥好说的。 Resource Records 格式形如 (Name, Value, Type, TTL)，其中 (Name, Value) 是 Key-Value pair Type A: (hostname, IP) Type NS: (hostname, dns server hostname)，同时还会有 (dns server hostname, IP, A, TTL) Type CNAME: (hostname, canonical name) Type MX: (hostname, canonical name) P2P File Distribution 大学生都爱玩的 PT Tracker 指一类特殊的 host。所有 BT 客户端上线后都将向 Tracker 发出请求，随后 Tracker 将返回一份客户主机的名单，这份名单上的主机称为这个新上线客户端的邻居节点。 Download 客户端会向邻居发出请求，得到若干份邻居所持有资源的表 找到最稀有的一份资源，发出下载请求 Upload 客户端会给所有邻居打分，上传时优先满足分数最高的邻居。分数与之前的上传情况相关 隔一段时间会随机挑选邻居重新评估分数 有点 MFLQ 的味道在里面。 关于 P2P 有一个定量的分析，比较简单就不说了。 Video Streaming 杀手级应用 DASH (Dynamic Adapting Streaming over HTTP) 说的是在向服务器请求文件的时候，服务器会返回同一个视频在不同码率下的 URL，客户端就可以动态作出播放何种码率的视频的决策... 同时 HTTP GET 请求中可以附带块(chunks)座标，这样就可以按需加载以节省带宽。在我小学的时候老师很喜欢打开一个视频页面，等整个视频加载完再看，现在大概是做不到了。 CDN (Content Distribution Network) CDN 说的是把提供内容的服务器架设在各个地方，通过某种方式从中央服务器分发内容到各个 CDN 服务器上，用户对于中央服务器中内容的请求最终会被重定向到 CDN 服务器。 可以发现网络相关的技术都是这么玩的——怎么在客户端无法感知的前提下提供更优质的 underlying 服务 CDN 后请求的大概流程会变成这样： 用户通过官方 URL 请求一个文件，这个 URL 中的域名部分会被 local DNS 服务器解析 local DNS 的 DNS request 经过层层查询来到了权威 DNS 服务器 权威 DNS 服务器返回 CDN 权威服务器的域名 CDN 权威服务器挑出一个 CDN 服务器，返回这个 CDN 服务器的 IP 地址 客户端拿到了 CDN 服务器的 IP 地址，就能请求到真正的视频文件了。 可以发现这个“重定向”的操作是建立在 DNS 之上的。即官方的服务器并没有直接提供数据，而是把请求重定向到了一个 CDN 的服务器。因此也可以说 CDN 本质上只是加了一层域名解析。 Content Update Strategy 大概讲了两类 Pull caching. 说的是用户最终的请求定向到了一个 CDN 服务器，但是这个 CDN 服务器上没有这个资源，那么 CDN 服务器就会向中央服务器请求这个资源、缓存、回传给用户 Push. 说的是中央服务器会主动把内容分发到各个 CDN 服务器上 当然它们可以混合 CDN Selection Strategy 这就五花八门了，但总归还是评分排序","tags":["Network"]},{"title":"Memory Models","path":"/2022/09/07/Memory-Model/","content":"Intro 看了很多资料发现基本都是同一批人在写，这篇 是我感觉讲得比较清楚同时也好懂的。 同时我发现很少有人用形式化的方法描述 model，反而都是用各种 case 的执行结果来区分不同的 model，感觉有点捉急。 Memory Model 说的是内存具体的行为。通常并发程序的执行模型是多个执行流+唯一的共享内存，Memory Model 则决定了多个并发执行流的交互结果（因为它们共同读写这块内存） 之所以要讨论这个，是因为 真正的硬件提供的机制很复杂，不一定和直觉一致 这些硬件机制的描述通常不准确，需要正确的建模 在开始之前要讲一下： 方便起见，不妨假设每条指令都是原子的。 简单来说，并发的本质是存在多个并发的执行流，使得多个并发执行的代码片段 \\(\\Set{P_i}\\)（指令序列）最终以某种顺序被合并成了一个序列 \\(L_M\\)（操作发生的先后顺序），并且按照这个序列执行了。其中 \\(\\to_P,\\to_M\\) 分别是 \\(P,L_M\\) 上的全序关系（事件发生的顺序）。 Memory Model 更具体而言就是若干对 \\(L_M\\) 中与内存相关的指令的限制（纯计算指令是无状态/线程局部的） 方便起见，可以假设所有对于内存的操作都是立即见效的，那么 OoO（乱序执行）、batching（写缓冲）之类的操作就可以简单的理解为指令顺序的调换，同时也可以涵盖指令重排的行为。 Sequential Consistency 这个是大名鼎鼎的 Leslie Lamport 提出来的。比较像分布式里面说的 Linearizability。 SC 的性质保证了 \\(\\forall C_i,C_j\\in P,C_i\\to_P C_j\\Rightarrow C_i\\to_M C_j\\)。即最终的执行序列中，所有来自同一线程的指令保持了它们所在指令序列（代码片段）的相对顺序。这说明线程内部严格按照顺序执行指令，唯一的不确定性仅来自于线程间的交错。 规定 \\(read(x)=\\max \\Set{C\\in L_M\\mid C\\in Store\\wedge C\\to_M read(x)}\\)，即某次读取的结果只与最后一次写入有关。 没有指令的重排意味着所有对内存的修改都是立刻全局可见的。 例如 jyy 在 os 课上展示的 model-checker 就是基于 SC 的 memory model 的。 Total Store Order 在硬件上给每个核心一个写缓存队列（write buffer queue），所有的写入会 首先被放入队列中 在一段时间后核心会冲刷队列，把所有的修改提交到共享内存 所有的读都会 先查询本地的缓存，命中即返回 再查询内存 看起来是非常合理的优化，但是这样会导致一致性的问题，例如线程 A 对缓存的写入，线程 B 是看不到的。这就使得这样的情况可能发生： 初始时 x=0 线程 A 在缓存中写入了变量 x=1 线程 B 读取 x=0 此处虽然执行序列为 writeA(x, 1), readB(x)，但是 B 可能读到 x=0 或 x=1（取决于队列是否被冲刷了） 对应到我们的假设（对内存的操作是立见的），那么此处的效果延迟则可以看成是“store 指令可能会被推迟”，即 \\(P\\) 中的一对 store-load 指令 \\(C_s,C_l\\)，\\(C_s\\to_P C_l\\) 并不能保证 \\(C_s\\to_M C_l\\)。 规定 \\(read(x)=\\max \\Set{C\\in L_M\\mid C\\in Store\\wedge C\\to_M read(x) }\\cup\\Set{C\\in L_M\\mid C\\in Store\\wedge C\\to_P read(x)}\\)，即某次读取的结果只与最后一次全局写入或最近一次局部写入有关，取决于它们的相对顺序。 此外，TSO 也保证了 所有线程见到的内存写入历史都相同。这是因为线程对内存的读取是直接的。一个经典的例子是 IRIW Litmus Test: Independent Reads of Independent Writes (IRIW) Can this program see r1 = 1, r2 = 0, r3 = 1, r4 = 0? (Can Threads 3 and 4 see x and y change in different orders?) // Thread 1 // Thread 2 // Thread 3 // Thread 4 x = 1 y = 1 r1 = x r3 = y r2 = y r4 = x On sequentially consistent hardware: no. On x86 (or other TSO): no. 但是如果线程1、3 线程2、4 各自共享写入缓存队列的话，考虑执行顺序如下： write(x,1) r1=x=1 r2=y=0 write(y,1) r3=y=1 r4=x=0 // in memory flush(1,3) flush(2,4) 就会出现虽然存在一个唯一的全局写入顺序，但是两个线程各自观察到了不同的写入顺序。它们眼中的历史出现了偏差，这句话挺有意思的。 这件事情非常像 git 的协作模式（不妨假设只允许 rebase）。每个人都有自己本地的 git 历史，同时有一个共享的remote branch。每个人的修改都是在本地进行的，因此他人不能立刻看到我的代码提交，而只有我 push 之后我的工作才会反映给其他人。并且所有人都能见到唯一的共享历史记录。","tags":["Concurrency"]},{"title":"TAOMP02 Mutex","path":"/2022/09/02/TAOMP02-Mutex/","content":"形式化 定义 事件是瞬间的、原子的，通常用 \\(read, write\\) 这样的名字表示一个事件，记事件的集合为 \\(E\\) 定义 \\(\\to\\) 为 \\(E\\) 上的全序关系，其含义为两事件发生的顺序。因为事件是瞬时的，因此不存在两个事件同时发生。 过程或时间段是二元组 \\((s,t)\\in E\\times E\\) 满足 \\(s\\rightarrow t\\)，其含义为两个时刻中间的时间 定义 \\(\\rightarrow\\) 同时为 \\(E\\times E\\) 上的偏序关系，其含义为两个过程发生的顺序。很显然存在某些过程有重叠 若过程 \\(A,B\\) 重叠，则它们是并发的 某些事件存在副作用。其含义为，若 \\(P(A)\\) 成立，且 \\(A\\to B\\)，则 \\(P(B)\\) 仍然成立，其中 \\(P\\) 为关于事件的谓词。 副作用可以被覆盖，因此某个性质是否成立需要类似于“最后一次xx”的前提 锁 锁是一类对象，通常提供两种方法 lock() 用于获取锁 unlock() 用于释放锁 规定一把锁至多被一个线程获取。若一把锁被某个线程持有，则称锁是忙的（busy）。 Critical Sections Critical Section: A block of code that can be executed by only one thread at a time 这样的性质叫 mutual exclusion，通常用锁来实现。 形式化地说就是把进入和退出 critical section 视为事件 \\(i,o\\)，那么对于线程 \\(\\set{0,1}\\) 而言，要么 \\((i_0,o_0)\\to(i_1,o_1)\\)，要么 \\((i_1,o_1)\\to(i_0,o_0)\\) 可以把 critical section 与一把特定的锁关联起来，并要求所有执行这段代码的线程都必须持有这把锁，这样就保证了 mutual exclusion 的性质。同样也可以把某个共享数据与一把锁关联，这样就起到了串行化对共享数据的访问和修改。 Property of interests 在锁的语境下，前面提到的各种性质就可以叙述为 mutual exclusion：任意时刻只有至多一个线程持有锁 deadlock-free：若某个线程尝试获取/释放锁，则最终将有至少一个线程得到/释放了锁。如果某个线程在 lock() 中卡住，那么其余线程必然能无数次进入 critical section。意思是可以有局部的卡住，但是相应的必然有某些线程能执行。 starvation-free：所有获取/释放锁的尝试最终都会成功。意思是对于每个线程而言都不会卡住。 其中 deadlock-free 指的是全局的 progress，而 starvation-free 则表明了每个线程的 progress。因此 starvation-free 必然是 deadlock-free 的。 当然还有更好的保证等待时间的性质，这就比 starvation-free 要更强了。 Fairness fairness 讲的是锁算法能够保证先到先得。为了形式化说明“先到”，每个锁算法应当被划分成两部分： doorway section，使得存在 \\(n\\in\\mathbb N\\) 保证了这部分必然在 \\(n\\) 步之内完成。 waiting section，这就是忙等的部分 我们说某个锁算法满足公平性，当且仅当对于任意两个线程 \\(A,B\\) 都有 \\[ D_A\\to D_B\\Rightarrow CS_A\\to CS_B \\] 直观含义就是，率先完成 doorway section 的线程会更早进入 critical section。注意此处都是时间段的比较。 这里引入的一个性质就叫 first-come-first-serve，如果一个锁既是 deadlock-free 又是 fcfs 的，那么它一定是 starvation-free 的。 锁1 伪代码是这样的 typedef struct &#123; bool flag[2]; &#125; lock_t; void lock(lock_t *lck) &#123; lck-&gt;flag[current_thread_id()] = true; while (lck-&gt;flag[another_thread_id()]) ; &#125; 可以发现这样是保证了 mutual exclusion 的（简单反证一下），但是没有 liveness 的保证。例如两边同时设置了 flag 之后就会卡住。 锁2 伪代码是这样的 typedef struct &#123; unsigned victim; &#125; lock_t; void lock(lock_t *lck) &#123; lck-&gt;victim = another_thread_id(); while (lck-&gt;victim != current_thread_id()) ; &#125; 可以发现这样是保证了 mutual exclusion 的（因为 victim 只能取其一），但是也没有 liveness 保证。当只有一个线程活跃的时候它将永远等待下去。 比较巧妙的点在于往 victim 中写入对方的 thread id，这是为了实现“先到先得”的效果。 Peterson's algorithm 神秘的地方在于，上述两种锁 都满足 mutual exclusion 各自在 存在争抢 和 不存在争抢 两种情况下表现良好 所以把它们结合起来就得到了 Peterson's Algorithm，伪代码是这样的 typedef struct &#123; unsigned victim; bool flag[2]; &#125; lock_t; void lock(lock_t *lck) &#123; lck-&gt;flag[current_thread_id()] = true; lck-&gt;victim = another_thread_id(); while ( lck-&gt;flag[another_thread_id()] == true &amp;&amp; lck-&gt;victim != current_thread_id()) ; &#125; 证明可以直接讨论，此时算法退化成其中一种，就证完了。 同时 Peterson's Algorithm 是满足 starvation-free 的，证明只需要注意到 victim 的设置在各自线程是不可逆的就行了。 Filter lock 也就是推广后的 Peterson's Algorithm。大概的想法是 设定长度为 \\(n-1\\) 的队列 每个想要获取锁的线程都从队尾开始排队 如果多个线程争抢队尾，则规定最后一个进入队尾的线程滞留在队尾，其余线程越过队尾争抢倒数第二个位置 如果只有一个线程想要进入队尾，则可以立刻进入 所有被滞留的线程将等待直至下一个位置为空，然后争抢下一个位置。 伪代码长这样 typedef struct &#123; unsigned victim[N]; unsigned level[N]; &#125; lock_t; void lock(lock_t *lck) &#123; int curr = current_thread_id(); for (int i = 1; i &lt; N; ++ i) &#123; lck-&gt;level[curr] = i; lck-&gt;victim[i] = curr; while (∃thread≠curr, lck-&gt;level[thread]≥i &amp;&amp; lck-&gt;victim[i]==curr) ; &#125; &#125; 可以发现这里的 level[t] 表示了线程 t 在队列中希望争抢的位置（1 是队尾），而 victim[i] 起到了区分争抢位置 i 的最后一个线程的作用 mutual exclusion 的证明可以通过归纳 level 简单做到，starvation-free 的证明则需要反向归纳一下（所有进入 \\(n-i\\) 层的线程最终都将返回，对 \\(i\\) 归纳） Bakery 每个线程都会被分配一个唯一的编号，能进入 CS 当且仅当编号在它前面的线程都离开了 CS，其中分配编号的部分就是 doorway section。 有一些比较有意思的性质： 编号是全局严格递增的 对单个线程而言，编号也是严格递增的 满足 FCFS Deadlock 狭义的 deadlock 虽然 Peterson's Algorithm 本身是 deadlock-free 的，但是涉及到多个锁的时候，整个系统仍然会卡住。例如经典的 lock-ordering 问题。有的时候 deadlock 指的是这样的情形。 livelock 书上讲得很模糊，大概意思是要满足 多个线程整体是卡住的 它们各自在阻碍其它线程前进 livelock 通常是可以通过特殊的调度来避免的，例如 OSTEP 里面提过的经典的 try-release-retry 这样的模式带来的 livelock。","tags":["TAOMP"]},{"title":"TAOMP01 Intro","path":"/2022/09/01/TAOMP01-Intro/","content":"八月份已经过去了，放假前立下的 flag 还有好多没有实现... 但是这本是上学期上 OS 就想看的书，更早是在吃土耳其烤肉的时候就被安利过 开干吧... Intro why multiprocessor 摩尔定律失效，现代的 CPU 已经引入了多核架构，现代的程序设计也在拥抱并行来提升性能 what's different 多个并发的执行流操作若干共享的对象是一种常见的并发计算模型，在这之上会遇到 正确性的问题，计算模型的变化使得原本正确的程序变得不正确 性能的问题，对共享对象的操作需要某种“序列化” 能力的问题，并非所有单线程程序都可以“改造”成多线程 评估的问题，我们应当如何评价一个多线程程序的优劣？ what's in interest liveness property，即并发的系统不会卡住。例如 deadlock-freedom safety property，即并发的系统不会进入某些错误状态。例如 mutual exclusion Shared states 又是那个经典的多线程共享 Counter 的栗子 一句话解释：单线程计算模型下的编程语言没有提供语句的原子性保证，这一历史包袱导致了共享状态在多线程下需要做同步操作。 解决办法是人为地打包捆绑某些指令，规定这些“指令包”是原子的。这样的性质（某段代码/某个对象 在同一时刻只能有一个线程执行/访问/修改）被称为 mutual exclusion。当然也可以叫做某种“协议”，指的是多个并发的执行流通过某种约定好的方式访问这些共享对象。 栗子 描述 Alice 和 Bob 各自养了宠物，他们共享一个花园。要求宠物不能同时出现在花园中，且两人的宠物都有进入花园的机会，要设计一个通信协议。他们各自可以举手，也可以看到对方有没有举手，除此之外没有别的交流手段。 协议 对 Alice 而言 举手 等待 Bob 放手 放宠物进花园 等宠物回来 放手 对 Bob 而言 举手 看 Alice，如果 Alice 举手就放手，等待 Alice 放手后再举手 放宠物进花园 等宠物回来 放手 mutual exclusion 假设存在某个时刻两人的宠物都进了花园，分别考虑两人的最后一次通信流程： 由 Alice 放了宠物可知，此前Alice 必然举起了手、看到了 Bob 放手。因此 Bob 最后一次举手必然发生在 Alice 最后一次看之后，这说明 Bob 必然能看到 Alice 举起了手。 由 Bob 放了宠物可知，此前Bob 必然举起了手、看到了 Alice 放手。这就产生了矛盾，故假设不成立。 deadlock-free 证明是类似的 starvation-free 是不满足的，例如当 Alice 和 Bob 总是同时举手时，Alice 的宠物总是优先进花园。 fault-tolerance 某一方可能出错，这时候另一方需要等待出错方恢复正常。 一些分布式的协议则会有专门的询问/验证环节来容忍节点的错误甚至是离线 Pattern 可以发现类似的证明通常只关心若干时间之间的拓扑关系，通过协议来限制事件发生的顺序成为我们想要的形状。 这样也就能理解为什么后面要抽象出 DAG 的证明模型了。 Back to CS 这种基于 举手/看 的通信协议本质上是延时的，即双方不需要约定时间后建立连接 举手/看 分别代表原子的 写/读 操作，并且各自只占据一个比特 Producer-Consumer 这个就不写了。 producer-consumer 这个性质说的是两个/两类线程满足 一类读取共享缓冲区 当且仅当缓冲区已满 一类写入共享缓冲区 当且仅当缓冲区为空 Pattern 涉及到状态存储的协议，就可以用探索状态的方式来证明正确性了。即“某协议保证XX不会出现”当且仅当“在任意合法的状态上XX都不成立”。而这又可以用 初始状态满足性质P + 状态转移保持性质P 这个套路来证明。 Reader-Writer 这个也不写了 Amdahl’s law 一个简单的公式，含义为：我们如果能使得 \\(p\\) 的任务加速 \\(n\\) 倍（通过 \\(n\\) 个核心），那么得到的效率增长是 \\(S\\) 的 \\[ S=\\frac{1}{(1-p)+\\dfrac{p}{n}} \\] 可以发现当 \\(n\\to \\infty\\) 时，\\(S\\to \\dfrac{1}{1-p}\\) 实际上并行处理是存在通信和同步开销的，这会反映在时间花费上。","tags":["TAOMP"]},{"title":"TAPL09 Recursive Types","path":"/2022/08/29/TAPL09-Recursive-Types/","content":"Intro 有种前菜只是刚刚上完的感觉。 递归数据结构是非常常见的结构，给递归数据结构赋予的类型通常会是一个递归的类型。 经典的例如 data IntList = Nil | Cons Int IntList 当然这里还没有讲到参数化类型，因此这里 List 作为“容器”的概念，其内容类型的区分是通过名字完成的。 为了避免这样自引用的情况（类似于 ULC 中定义递归的方式），引入算子 \\(\\mu\\)。那么上面的 haskell 类型定义就可以形式化地写成下面的样子 \\[ \\text{IntList=$\\mu X.\\left&lt;\\text{Nil}, \\Set{\\text{hd=Int, tl=}X}\\right&gt;$} \\] 一个分析中的例子可以是这样的：给定函数 \\(f\\)，定义算子 \\(\\gamma\\) 使得 \\(\\gamma f\\) 为方程 \\(f(x)=x\\) 的解集。此处 \\(\\mu\\) 的作用是类似的。 类比 \\(\\text{fix}\\) combinator，这个 \\(\\mu\\) 同样可以像剥洋葱一样一层层剥开类型并且不收敛。 Equivalence 书上讲有两种构造递归类型的方法，它们的区别在于对如下问题的回答不同： 递归类型本身和它剥开一层后的类型，二者关系如何？ 这个“剥开”函数一般叫 \\(\\text{fold}\\colon T\\mapsto T\\) equi-recursive approach 认为它们是同一个类型，在语境下可以等价替换，type checker 负责决定把具体的递归类型剥开到什么程度。 iso-recursive approach 认为它们不同，但是是同构的。即 \\(\\text{unfold}\\circ\\text{fold}=id_T\\)。同时代码需要显式地在需要剥开的地方写出 \\(\\text{fold}\\) 和 \\(\\text{unfold}\\) Induction, Coinduction 定义 若 \\(F:2^U\\mapsto 2^U\\) 满足 \\(\\forall X\\subseteq Y, F(X)\\subseteq F(Y)\\)， 则称 \\(F\\) 是 \\(U\\) 上的单调函数 通常把这个（朴素意义的）集合 \\(U\\) 叫做 Universe，即我们所有可能讨论的所有元素组成的集合。 若 \\(F(X)\\subseteq X\\)，则称 \\(X\\) 是 \\(F\\)-closed 的 若 \\(X\\subseteq F(X)\\) ，则称 \\(X\\) 是 \\(F\\)-consistent 的 若 \\(F(X)=X\\)，则称 \\(X\\) 是 \\(F\\) 的不动点（fixed point） 可以发现 \\(F\\) 本质上是定义在格上的函数，并且这个格是由 \\(\\left&lt;U,\\subseteq\\right&gt;\\) 导出的。 通常把 \\(F\\) 的最小不动点写成 \\(\\mu F\\)，最大不动点写成 \\( u F\\)。 定理 Knaster-Tarski 定理的叙述如下： 所有 \\(F\\)-closed 集之交是 \\(F\\) 的最小不动点 所有 \\(F\\)-consistent 集之并是 \\(F\\) 的最大不动点 证一下①： 考虑 \\(C=\\Set{S\\mid F(S)\\subseteq S}\\)，不妨记 \\(M=\\bigcap C\\)，下面即证明 \\(F(M)=M\\) 且 \\(\\forall X\\subseteq U, F(X)=X\\Rightarrow M\\subseteq X\\) 注意到 \\(\\forall S\\in C\\) 都有 \\(M\\subseteq S\\)，因此 \\(\\forall S\\in C\\) 都有 \\(F(M)\\subseteq F(S)\\)，这说明 \\(F(M)\\subseteq \\bigcap_{S\\in C} F(S)\\) 又由 \\(C\\) 的定义可知 \\(F(M)\\subseteq\\bigcap_{S\\in C} F(S)\\subseteq \\bigcap C=M\\)，因此 \\(F(M)\\subseteq M\\)； 另一侧由反证法，不妨假设存在 \\(x\\in M\\) 使得 \\(x ot\\in F(M)\\)，此时记 \\(M&#39;=F(M)\\)，那么有 \\(M&#39;\\subseteq M\\)，根据单调性有 \\(F(M&#39;)\\subseteq F(M)=M&#39;\\)，因此 \\(M&#39;\\in C\\)，这说明 \\(M=\\bigcap C\\subseteq M&#39;=F(M)\\)，矛盾；故假设不成立，即 \\(M\\subseteq F(M)\\) 综上就有 \\(M=F(M)\\)，这说明 \\(M=\\bigcap C\\) 是 \\(F\\) 的一个不动点，下面证明这是最小的不动点。 由反证法设存在更小不动点 \\(M&#39;&#39;\\subsetneq M\\)，则根据不动点定义有 \\(M&#39;&#39;\\in C\\)，这说明 \\(M=\\bigcap C\\subseteq M&#39;&#39;\\subsetneq M\\)，矛盾；故假设不成立，不存在更小的不动点了。 ②的证明是对偶的。定理的另一种叙述如下： Principle of induction 若 \\(X\\) 是 \\(F\\)-closed 集，那么 \\(\\mu F\\subseteq X\\) Principle of coinduction 若 \\(X\\) 是 \\(F\\)-consistent 集，那么 \\(X\\subseteq u F\\) Subtyping tnnd，卖了一个很大的关子就跑了，今天就到这吧。","tags":["TAPL"]},{"title":"TAPL08 Subtyping","path":"/2022/08/28/TAPL08-Subtyping/","content":"Intro Subtyping（或者说 Subtype Polymorphism）给出了一种类型间的转换关系（一种 preorder），使得程序猿可以写出很通用的代码。典型的子类型出现在各种 OO 语言里，例如 Java 的子类。 同时需要指出的是，子类型和继承并非必须捆绑在一起 子类型说的是具有一种类型的项可以无痛替换掉另一种类型的项 继承强调的是代码复用 Java 把这俩放在一起了，即创造子类型的同时天然地复用了父类型的代码。 Subtyping 称 \\(A\\) 是 \\(B\\) 的子类型，记为 \\(A&lt;: B\\)。一个简单的基于朴素集合论的解释就是：类型为 \\(A\\) 的项构成的集合是类型为 \\(B\\) 的项构成的集合的子集。 这说明，若 \\(\\Gamma\\vdash a:A\\)，那么 \\(\\Gamma\\vdash a:B\\)。即 \\[ \\frac{\\Gamma \\vdash a:A,\\qquad A&lt;:B}{\\Gamma\\vdash a:B} \\] Subtype relation 子类型关系是类型上的一个二元关系，由若干推导规则给出。 Baiscs 通常子类型关系是任意的（对于不同的语言而言可以不同），因此存在可以设计的地方。但通常都包括下面两条（即子类型关系至少应该是一个 preorder） \\[ \\begin{aligned} \\frac{}{A&lt;:A} \\end{aligned} \\] \\[ \\begin{aligned} \\frac{A&lt;:B\\qquad B&lt;:C}{A&lt;:C} \\end{aligned} \\] Records 书上花了很长的篇幅讲两个 record 什么情况下是子类型，其实就是看 field 的包含情况就可以了。 Functions 这个就比较好玩 \\[ \\frac{S_1:&gt; S_2\\qquad T_1&lt;:T_2}{S_1\\rightarrow T_1&lt;:S_2\\rightarrow T_2} \\] 函数的子类型与函数参数的子类型关系是反的，这个叫逆变（contravariant） 函数的子类型与函数返回值的子类型关系是相同的，这个叫协变（covariant） Special types 定义 \\(\\text{Bot, Top}\\) 为两个特殊类型，其中 \\(\\forall S. S&lt;:\\text{Top}\\)，\\(\\text{Bot}\\) 类似。 Variant 对于 Sum Type 而言，子类型多态的引入可以省去一个 ascription，因为类型可以自动提升。 List \\[ \\frac{A&lt;:B}{\\text{List $A&lt;:$ List $B$}} \\] References \\[ \\frac{A&lt;:B\\qquad B&lt;:A}{\\text{ref $A&lt;:$ ref $B$}} \\] 之所以有这个要求，是因为一个引用既可以存入、也可以取出。 我们只能往 \\(\\text{ref $B$}\\) 中存入 \\(B\\) 的子类型，也只能取出 \\(B\\) 的子类型，因此 \\(\\text{ref $A$}\\) 能替代 \\(\\text{ref }B\\) 的场合要求 \\(A,B\\) 互为子类型。 也可以把读和写分离，做成类似管道的东西，那么这时候就可以在两侧分别赋予不同的类型了（逆变/协变）。 Array 数组本质上也是一种引用（引用是长度为 1 的数组），因此很容易写出 \\[ \\frac{A&lt;:B\\qquad B&lt;:A}{\\text{Array $A&lt;:$ Array $B$}} \\] 书上也提到了 Java 的设计缺陷，看一乐 Casting 一般分成 upCast 和 downCast upCast 就是之前提到的 ascription，即通过 subsumption 提升为父类型，这个没什么问题 downCast 就是 Java 里一般会遇到的 cast，即把某个父类型的项强转成某个子类型。最经典的就是 Java 在还没有泛型的时候需要这样实现 List： ListOfObjects list = new ListOfObjects; Dog dog = new Dog(); list.add(dog); Dog objDog = (Dog) list.get(0); 虽然即使有了本质上也还是这么实现的... 或者 C 里面的 void * 当然，不正确的 downCast 会丧失 safety，因此还需要做动态的检查。也有些小技巧可以通过静态的指针分析来得到一些 sound 解，使得我们可以省去某些动态的检查。 Coercion Semantics 这里花了一点时间才看明白在讲什么 上面讲的其实是 high-level 的 subtyping 语义，即我们真正做的实际上是把父子类型之并视为父类型。在操作这些值的过程中它们的内部表示仍然是各自具体类型的，这就可能涉及到装箱和拆箱的代价。这就是所谓的 subset semantics 而 Coercion Semantics 给了另一种方法，即我们可以把 high-level 的语言翻译到 low-level 且没有 subtyping 的语言。在这个过程中，我们通过添加若干 low-level 的 cast 函数来将子类型改写成父类型，这样在类型提升后，我们项的内部表示也得到了改变。这就是 coercion semantics 举个例子： struct A &#123;x: Nat&#125; struct B &#123;y: Bool -&gt; Bool, x: Nat&#125; 很显然 \\(B&lt;:A\\)。不妨记 f = fun x: Nat (x + 1)，立即有 \\(f:\\text{Nat$\\rightarrow$Nat}\\) subset semantics (fun arg: &#123;x: Nat&#125; (f arg.x)) &#123;y = id, x = 0&#125; -&gt; f (&#123;y = id, x = 0&#125;.x) -&gt; f 0 -&gt; 1 coercion semantics (fun arg: &#123;x: Nat&#125; (f arg.x)) ((fun from: &#123;y: Bool -&gt; Bool, x: Nat&#125; &#123;x = from.x&#125;)&#123;y = id, x = 0&#125;) -&gt; (fun arg: &#123;x: Nat&#125; (f arg.x)) &#123;x = &#123;y = id, x = 0&#125;.x&#125; -&gt; (fun arg: &#123;x: Nat&#125; (f arg.x)) &#123;x = 0&#125; -&gt; f (&#123;x = 0&#125;.x) -&gt; f 0 -&gt; 1 可以发现 subset semantics 要求我们能屏蔽一些底层细节，用统一的方法对不同类型（但属于同一个父类型）的项进行统一的操作。 而 coercion semantics 则不要求上面这一点，不同实际类型可以用不同的方法来操作，子类型的提升是通过显式的语义函数完成的。 通常这个生成 cast function 的过程是自动化的，并且是基于 typing derivation tree 来的。你可以把它叫做 compilation，没什么问题。 当然具体的规则自己去看书，只需要知道得分别对 类型、子类型提升、项 三个东西分别翻译就行了。 Coherence Coercion Semantics 的代码生成是基于 typing derivation tree 的，因此如果某种类型的 derivation 不唯一，那么代码生成就可能有歧义。经典的例如 \\(\\text{Bool&lt;:Int&lt;:Float}\\)，那么在 \\(\\text{toSring}(True)\\) 时就会出现 \"1\" 和 \"1.000\" 的区别 称一个翻译 coherent 当且仅当对于同一个 typing judgement \\(J\\)，其任意 derivation tree 产生的翻译程序语义等价（原文用的 behaviourally equivalent）。","tags":["TAPL"]},{"title":"TAPL07 Exceptions","path":"/2022/08/28/TAPL07-Exceptions/","content":"Intro 异常处理一直是老大难问题，大概有这么几个流派 用返回值区分，Rust 的 Result、Haskell 的 Either a b、C 的 errno 都是这类（其实我还想说 go 的）。 用异常控制流（本质上是non-local jumps），例如 Java 的 try ... catch ... finally、Python 类似、C++ 也能做。 遇到异常直接终止程序，例如防御性的 assert()、panic() 等等 ①和②往往针对可恢复的异常。书上大概讲的是②和③这种方法。 Exceptions Naive raise 加入一个新的项 \\(\\text{error}\\)，规定 \\[ \\begin{aligned} \\text{error}\\; t\\rightarrow \\text{error}\\\\ v\\;\\text{error}\\rightarrow \\text{error} \\end{aligned} \\] 这表明 \\(\\text{error}\\) 具有“传染性”，当一处异常出现时，后续的求值将使得这个异常扩散到整个程序（最终 abort）。可以发现这两条规则保持了执行的顺序，即一个异常不会过早被扔出来，也不会过晚才出现（以免执行了后续有副作用的部分） 同时 \\(\\text{error}\\) 本身并不是值，这表明 \\((\\lambda x:\\text{Top}.\\;x)\\;\\text{error}\\) 将不会执行 apply，而是直接变成 \\(\\text{error}\\)。否则将引入二义性的问题。 Naive raise Typing 有几个技巧 \\(\\text{error}\\) 可以具有类型 \\(\\text{Bot}\\)，这样在有子类型的类型系统中就可以被提升为任意其它类型 \\(\\text{error}\\) 可以具有类型 \\(\\forall X.X\\)，这样就可以被实例化为任意其它类型 或者留着不动，在做 typecheck 的时候赋予 \\(\\text{error}\\) 我们需要的类型。但这样就违背了 unique type 的性质 同时 preservation TH 也需要变换表述，即最终总会得到一个值或 \\(\\text{error}\\) Catch 就是设计这样的项 \\(\\text{try }t_1\\text{ with }t_2\\)，规定 \\[ \\begin{aligned} \\text{try error with } t_2&amp;\\rightarrow t_2 \\\\ \\text{try $v$ with $t_2$} &amp;\\rightarrow v \\end{aligned} \\] 即如果异常了就执行后面的代码擦屁股，否则就正常返回。你也可以当成是套了一个 meta-level 的 wrapper 这玩意的 typing 要求 \\(t_1, t_2\\) 有相同的类型（回想 \\(\\text{if-then-else}\\)）。 Raise with value 最经典的就是 C++ 里的 e.what()，即我们可以给异常带上一个值用于区分这是什么异常，同时在 handler 中利用这个携带值做一些事情。 引入 \\[ \\begin{aligned} &amp;\\text{raise $t$}\\\\ &amp;\\text{try $t_1$ with $t_2$} \\end{aligned} \\] 其中 \\(t_2\\) 是一个类型为 \\(T&#39;\\mapsto T\\) 函数，\\(T,T&#39;\\) 分别为 \\(t_1\\) 和 \\(\\text{raise}\\) 中信息的类型。 书上规定 \\(\\text{raise $t$}\\) 仍然具有任意的类型。这里一个小问题就是我们没法通过 structural 的方法静态地知道一个异常内信息的类型了（否则等价于用 Either a Monad 做返回值）。 当然书上给出的 argue 说的也是针对不可恢复的错误，我们是不需要关注其可能抛出的异常的（毕竟最终会支配整个程序）。但是对于可恢复的程序还是有必要的，例如 Java 就偏向于明确写出 public foo() throws BarException 这样的东西。 此外，用一些别的小技巧还是可以静态分析出一段代码可能会扔什么异常的。 另一个可以设计的地方在于内部信息的选择 用数字，这就是 C 的做法 用 String，那就是最简单的 e.what() 用一个 Sum Type（或者像 Java 那样用类），这样就可以 match 不同的情况做不同的处理","tags":["TAPL"]},{"title":"TAPL06 References","path":"/2022/08/27/TAPL06-References/","content":"Intro 之前提到的 x=term 都是 name binding。因为所有的项都可以求值，并且都是纯的计算模型（只和 substitution 有关），所以本质上是 meta level 的语法替换。可以简单地理解为宏展开。 这里引入的是可变引用（指针、C++的引用、rust的可变引用）这样的东西，即我们给某个变量绑定的不再是一个项，而是一个储存空间，其中储存着项。同时有两种方法“取出”和“存入”项。这就使得计算模型发生了变化（回想 SICP 引入 set! 之后的那一大坨话） 同样是引用，不同的语言在语法设计上也存在差异，例如 C 语言的变量默认是可变的，会在使用过程中隐式地解引用；也可以设计成 val 和 var 的区别以显式地区分可变与不可变变量。 当然书上的写法还是很像 C 的。 印象里去年上课还扩充了 magic wand 之类的高端内容，当时是完全没听懂了。 Side-effect, alias, shared state 副作用和状态很好理解，考虑如下项 gen start = let counter = ref start in &#123; inc = fun _: Unit (counter := !counter + 1), get = fun _: Unit (!counter) &#125; 我们调用 gen 0 即得到一个 variant，每次调用 variant.inc unit 就会造成 counter 所存内容的增加，调用 variant.get unit 就可以查询到这个值。 alias 也很好理解，考虑 x = ref 0; y = x; x := !x + 1; !y 结果就会是 1。 alias 使得分析变得困难，这也是为什么别名分析需要花那么大力气去对 heap 和 obj 建模。 共享状态就更好理解了，写过并发程序的话应该都懂的。 location \\(L\\) store 是对 memory 的抽象，即我们认为内存是一个 \\(L\\mapsto V\\) 的映射。此时对分配操作而言，其结果就是产生一个 \\(l\\in L\\)，把值存进 \\(\\mu\\)。而后续通过这个 location \\(l\\) 我们就可以得到具体的值。 引出的一个问题就是 gc。在形式化讨论的时候完全可以不管 gc，如果需要对 gc 建模则要 形式化定义 garbage 形式化定义 gc 的过程（即 \\(\\overset{\\text{gc}}\\rightarrow\\) 的操作语义） Typing 对于新加入的 location \\(l\\) 也需要赋予一个类型。 naive 的做法是通过访存 \\(\\mu\\) 来做 structural 的类型推导。考虑如下 store \\[ \\Set{l_1: \\lambda x\\colon\\text{Nat}. (!l_2).x,l_2: \\lambda x\\colon\\text{Nat}. (!l_1).x} \\] 这种 store 上的循环依赖将导致 naive 的类型推导不终止 一个更加简单的方法是给每个 location 标注类型，即所有被动态分配出来的内存都应当维护这段内存中的比特串应当被解释为什么。","tags":["TAPL"]},{"title":"Network01 Intro","path":"/2022/08/14/Network01-Intro/","content":"网络 形式化的定义网络为图 计算机网络 层级 应用层（application） 网络层（network） 传输层（transport） 链路层（link） 物理层（physics） 每层都向上提供接口，利用下层的服务实现更强的功能 逻辑上数据传输是在同层之间的，实际上是通过向下调用接口实现真正的传输，最终是物理信号在真实世界里的传输。 节点 特别的，计算机网络中的节点有两种类型 主机（host）。通常用 \\(\\square\\) 表示 网络交换设备（packet switch），包括交换机、路由器等等。通常用 \\(\\bigcirc\\) 表示 边 链路指连接两个节点的边，同样有两种类型 接入网链路，边的一端为主机。 主干链路，非接入网链路。 可以发现逻辑上我们只关心所有 \\(\\square\\) 组成的网络，所有的 \\(\\bigcirc\\) 只是为了中继。这是因为只有主机会产生和消费数据，而所有的交换设备只负责转发（当然也有交换设备会消耗数据，例如带拦截广告功能的软路由...） 互联网 以 TCP 和 IP 协议为主的最大的那个网络。感觉这个定义非常有意思。 粗略地讲，可以把网络划分为三个部分 核心，即非主机的、提供网络连接功能的部分 接入，即主机到核心的连接 边缘，即所有主机，提供了网络的应用服务 边缘 主要是应用进程。这些应用通过 OS 提供的网络 api 来进行远程进程通信 通信模式 C/S 模式，即主从关系 P2P 模式，即对等关系 通信方式 面向连接，即在正式通信前需要建立连接，进行准备。 无连接 网络不过是另一种进程间通信，具体的细节对于用户进程而言是不可感知的。 核心 电路交换(circuit switch) 从电话网来 通信双方建立专有连接（耗时） 中间节点需要维护通信状态 每段线路划分为固定的片，每个用户每次只能用其中一片 划分的方式包括： 时分复用 频分复用 分组交换(packet switch) 无需建立专有连接 中间节点无需感知通信状态 通信数据被划分成段传输。任意时刻，数据独占一段通信线路，可以实现同时间内不同线路的共享 转发节点内有等待队列 本质上是一种离散的时分复用，也叫 statistical multiplexing 还可以细分为 datagram switch 和 virtual circuit switch，后者本质上是希望 packet switch 能有 circuit switch 的某些特性提出来的 hack。 接入 这部分解决了我对一些名词的陈年疑惑...非常奇妙 调制解调器(modem) 最早是利用已有的电话线路接入网络的。modem 负责把数字信号调制成音频信号，由电话线路传输后入网。 上网和打电话不能同时进行 带宽很低 成本低，复用线路 是专线 有些实现把电话线频分复用，专门留出一部分给网络。其中又分出上下行专用的带宽，这样就可以同时打电话和上网了。这样的方式叫 DSL(Digital Subscriber Line) 有线电视(cable) 同样是类似的思路，利用已有的有线电视线路。 这下终于知道为什么以前爹妈拔掉机顶盒的线就没法上网了。 电网 可以调制解调到电力(电流)上... 物理媒体 导引性媒体，有型介质、存在导体、可以定向传播，例如光纤、电缆 非导引性媒体，例如电磁波 四层 应用层 注意到网络提供的服务本质上就是 RPC，因此协议做的事情和 shell 命令解析是同样的。 有 HTTP FTP SMTP POP3 一大堆协议。这是因为应用很多，不同应用的协议通常不共享。 应用层的许多协议因为历史原因，都是明文传输数据的（即协议本身不负责内容的加密）。解决的办法是在此之上加一层加密层（例如 HTTP + TLS = HTTPS），这样明文传输的就是加密后的内容了。 进程通常在 OS 中通过 socket api 来管理和使用它与其它进程的连接。连接（即进程-进程对）与 socket 是一一对应的关系，而和端口号没有数量关系（一个进程可以通过一个端口和多个进程建立连接，此时多个连接共享一个端口，各自通过不同的 socket 把数据传递给对应的进程） 传输层 提供的是进程到进程的字节流通讯，进程通过端口号区分。 TCP(Transmission Control Protocol) 面向连接，通信前需要握手建立连接 可靠服务，保证顺序、正确性等等 流量控制，可以协调发送方和接收方的传输速度 拥塞控制，可以适应网络的拥堵 UDP(User Datagram Protocol) 无连接 不可靠 无流量控制 无拥塞控制 网络层 IP 协议提供了主机到主机的不可靠通信。这里主要包括 路由，即修改和查询路由表这个数据结构 转发，查询路由表，把 packet 发给下一跳设备 缓冲，提供队列缓冲已处理但还未轮到发送的 packet TTL，丢弃可能走入环路的包 校验 链路层 也有可靠和不可靠的协议，通常介质越可靠则协议越不可靠；介质越不可靠则协议越可靠。 还有被动介质的说法，例如 WiFi 的工作原理决定了所有的数据都会被发送给终端设备，那些不属于目标设备的数据帧将会被终端设备自行丢弃。这使得同一 WiFi 下的终端设备可以彼此看到对方的数据帧，从而还原出数据包，这就带来了潜在的窃听风险。","tags":["Network"]},{"title":"TAPL05 Extensions","path":"/2022/08/09/TAPL05-Extensions/","content":"Intro 这一章主要是给 STLC 加上各种常用的 construct，也就是在做语法糖化的过程。比较轻松 Derived Form 考虑语言 \\(\\lambda\\) 及其拓展 \\(\\lambda^E\\)。若存在一个语法上的翻译函数 \\(t\\colon \\lambda^E\\mapsto \\lambda\\) 使得 \\[ \\forall m,m&#39;\\in \\lambda^E \\\\ \\begin{aligned} &amp; m\\rightarrow m&#39;\\iff t(m)\\rightarrow^E t(m&#39;) \\\\ &amp; \\Gamma\\vdash m\\colon \\tau \\iff \\Gamma \\vdash^E t(m)\\colon\\tau \\end{aligned} \\] 即如果某个语法和语义上的扩展，可以用纯语法替换的方式等价得到，那么就称这个扩展是“语法糖”(syntatic sugar)。此处等价特指 reduction relation 和 typing relation 保持。 当然这里语法糖的要求是单步推导等价，这个条件还是比较强的。 这里的一个好处是可以在不修改语言 core 的情况下添加各种 feature。一个类似的例子是微内核+形式化验证的套路，把用户态组件看成是语法糖就行了。 Base Types 一般的语言会提供若干基础类型用于表示一些基本的值，在此之上可以用一些类型构造子构建更复杂的类型以表示复杂的数据。 经典的比如 int, float, char 之类。 通常不关心基础类型内部的操作，因为此处主要讲的是类型系统的构建，因此可以把这些基础类型视为“原子的”，即它们只能向上构建新类型。 Unit Type 也是常说的 trivial type。这个类型所含值的集合为单元集(singleton set)。 扩展是通过加入一个特殊的值 unit，并规定其类型为 Unit 来实现的。 Bot Type 为了方便，通常规定 Bot 是所有类型的子类型，用于表示某些异常项（例如 exception），同时保证带有 error handling 的项可以通过类型检查。 这个类型所含的值的集合为空集： 由反证法，设存在 \\(\\Gamma\\vdash v\\colon \\text{Bot}\\)，那么根据子类型同时有 \\(\\Gamma\\vdash v\\colon \\text{Top$\\rightarrow$Top}\\) 和 \\(\\Gamma\\vdash v\\colon \\{\\}\\)。而 \\(v\\) 不可能既是函数又是 variant，因此矛盾。 Subtyping 称一个类型 \\(A\\) 是 \\(B\\) 的子类型，当且仅当类型为 \\(A\\) 的项可以无痛替换掉类型为 \\(B\\) 的项，程序仍然可以正常执行。记作 \\(A&lt;:B\\) 一个例子是 \\(A=\\{x:\\text{Nat},y:\\text{Bool}\\}\\)，它是 \\(B=\\{x:\\text{Nat}\\}\\) 的子类型，因为所有用到类型 \\(B\\) 的项至多访问 field \\(x\\)。可以发现子类型的定义是完全根据喜好来的，通常可以根据需求规定不同的子类型关系。 例如我们的 variant type，就可以通过规定所有 field 可以交换、加上若干 field 是子类型、field 的类型是子类型仍是子类型 这三条来得到一些比较方便的性质。 可以发现 Unit 天然的就是 Top，并且我们很难通过构造的方法给出一个 Bot 类型，这里的 Bot 有点 limit order \\(\\omega\\) 的意味在里面了。 Sequential 即 s1; s2 这样的写法，用于保证求值顺序。书上提到的 desugar 成 (\\(\\lambda x\\colon \\text{Unit}.\\; s_2)\\; s_1\\) 能够等价的前提是推导是 Call-by-value 的 Wildcard 即 \\(\\lambda \\_.\\; m\\) 这样的写法，表示我们并不会真的用到这个 binding。一个具体的例子就是 s1; s2 的 derived form desugar 之后就是选取 \\(FV(m)\\) 中的 fresh 做 binding。 Ascription 也就是说的程序员自己写的类型注解，理解成 term-type assertion 也没啥问题 ascription 本身也可以作为 derived form 添加进去，即 \\((\\lambda x\\colon \\tau.\\; x)\\; t\\equiv t\\text{ as } \\tau\\) 具体定义去看书 let Binding 即可以写出这样的代码(以 haskell 为例) dist:: Float -&gt; Float -&gt; Float dist x y = let sqrX = x * x sqrY = y * y in sqrt $ sqrX + sqrY 这同样是一个 derived form，但是区别在于在 derive 的时候需要有一个 type checker 来获得 binding 的类型。 因为涉及到 typing relation 的修改，所以这个语法糖和前面的比起来要更不 trivial 一点。但是执行过程是没有区别的。 具体定义去看书 Record 也叫 Product Type，一个例子就是 C 里面的 struct field 是数字时就退化为 tuple，长度为 2 则退化成 pair。 书上还讨论了顺序的问题，这里目前定义出来的都是要求顺序一致的。后续解决顺序可以用 subtyping 来做，咕咕咕咕。 Pattern matching 这里的 pattern matching 仅能用作 getter 具体而言是通过定义语法上的 pattern，然后把 pattern matching 的行为定义为若干单变量替换的复合 具体去看书。 Variant 也叫 Sum Type，一个例子就是 C 里面的 union 所有构造子都只吃掉一个 \\(\\text{Unit}\\) 的时候退化成 Enum。只有两种的时候退化成 Either，其中一个是 \\(\\text{Unit}\\) 的时候退化成 Optional 因为类型中的值是有限的，所以就可以 case value of 来进行操作的分派了。 一个比较有意思的地方在于 Either 的出现会破坏类型的唯一性，即我们很难静态地确定 \\(\\text{Left 42}\\) 到底是什么类型。最简单的解决方法就是要求程序猿显式加上注解。 Dynamic 即给项加上类型的标签，使得类型在动态运行时仍然附着在项上。同时提供一些动态对于类型的 case construct 来支持操作的分派。 Recursion 由于 STLC 中没法给 Y-Combinator 定类型，因此需要把这个东西提升到元语言的层级，作为一个 primitive。 好玩的地方在于 fix 并没有规定只能求一个函数的不动点，因此可以把很多函数用 Record 打包，实现类似常系数线性齐幂次递推的互相调用的递归函数。树上给的例子是关于 isEven 和 isOdd 的。 List 重要的地方在于给出了一个不同于 \\(\\rightarrow\\) 的类型构造子 \\(\\text{List}\\)，剩下的就没啥了。 Exception","tags":["TAPL"]},{"title":"TAPL04 Typed Lambda","path":"/2022/08/04/TAPL04-Typed-Lambda/","content":"Simply Typed Lambda Calculus 通常记为 \\(\\lambda_{\\rightarrow}\\) Intro 在 ULC + Bool + Nat 中会出现某个项 \\(T\\) 卡住的情况，意思是 \\(T\\) 不是值，且 不存在 \\(T&#39; eq T\\) 使得 \\(T\\rightarrow T&#39;\\) 例如经典的 suc True 就没法推导 解决方案是静态地给项加上类型，在运行前做类型检查，然后证明所有通过类型检查的项都终会推导成值 一个比较有意思的点是，我们可以将每个项都视为一种类型，这时候做类型检查（类型推导）就完全等价于对项本身进行推导。并且由于 LC 是图灵完备的（例如经典的 \\(\\Omega\\)），这样的类型推导可能不终止。 这说明所有有用的类型推导（停机的类型推导）必将做出过近似。意思是存在一个不会卡住（well-behaved）的项 \\(T_w\\)，\\(T_w\\) 没法通过类型检查。 还可以回想静态分析做的事情（也是在静态时过近似，用各种 derivation rules 做约束求解），所以说这玩意是同属于 PL 下的细分分支。甚至对于指针分析，只需要把对象集看成类型，那么求解过程本质上就是在定 type 了。 Definition Types 定义类型集合 \\(\\scr T\\) 为满足如下约束的最小集 \\(\\text{Bool}\\in {\\scr T}\\) \\(\\forall A,B\\in {\\scr T}, A\\rightarrow B\\in{\\scr T}\\) ②实际上是引入了产生复杂类型的方法，称 \\(\\rightarrow\\) 为类型构造子。例如我们可以有 \\( ewcommand{tBool}{\\text{Bool}} (\\tBool\\rightarrow\\tBool)\\rightarrow\\tBool\\) 这样的类型 Typing relation 我们把 \\(M: \\tau\\) 称为 typing judgement。又因为这实际上是 \\(T, {\\scr T}\\) 上的二元关系，因此也叫 typing relation。 对于这样的二元关系也存在推导（derivation rules），通常写成 \\(\\dfrac{\\text{premise}}{M:\\tau}\\) 这样的形式。 很显然并非所有的项都可以推导出一个类型（例如经典的 \\(\\Omega\\)），因此做关于类型的表述时通常说 “若项 \\(M\\) 存在类型 \\(\\tau\\)” 这样的句子。 \\(\\lambda\\)-abstractions 给出了类型集合的定义后，需要考虑的就是如何给项标记类型（怎么做 structural 的类型推导）。关键的地方在于形如 \\(\\lambda x.\\; M\\) 的项要如何确定类型。 这里插几句。由于前面提到的 \\(\\tBool\\)-calculus 是没有变量的，所有的符号都是常元，因此所有的类型都可以通过 structural 的过程得出。 在这里因为引入了 \\(\\lambda\\)-abstraction，所以会出现变量（回忆函数内访问参数）。如何推导函数的类型，本质上就是如何推导函数参数和返回值的类型。 explicitly typed: 可以手动给参数加上类型注解，然后推导返回值的类型 implicitly typed: 可以约束求解，此时得到的参数类型是一个可能类型的集合 ①很好理解，②可以类比 haskell 里面无类型注解的函数会用 typeclass 来限制参数的类型，这是一种从 body 到参数的类型推导 为了简单，书选择了把项写成 \\(\\lambda x:\\tau. M\\) 的形式 Typing context 同时因为 \\(\\lambda\\)-abstraction 的出现，项 \\(T\\) 中可能存在自由变元。这时候的 \\(T\\) 的类型在这些自由变元取不同类型时可能有不同的类型，因此引入typing context \\(\\Gamma\\)。 \\(\\Gamma\\) 为 typing judgement 的集合，类似命题逻辑有 \\(\\Gamma\\vdash M:\\tau\\) 这样的记号，含义为“在假设 \\(\\Gamma\\) 成立时，\\(M\\) 类型为 \\(\\tau\\)” Derivation rules 这个形式和命题逻辑的推理系统 G' 具有形式上的一致性，非常漂亮的一个结论。当然也可以认为做类型推导本身就是在构造一个逻辑系统，反过来命题逻辑恰好足够表达 STLC 的类型推导。 \\[ \\frac{}{\\Gamma,x:\\tau\\vdash x:\\tau} \\] \\[ \\dfrac{\\Gamma,x:\\tau\\vdash M:\\sigma}{\\Gamma\\vdash\\lambda x:\\tau.\\; M:\\tau\\rightarrow\\sigma} \\] \\[ \\frac{\\Gamma\\vdash M:\\tau\\rightarrow \\sigma\\quad \\Gamma\\vdash N:\\tau}{\\Gamma\\vdash M\\; N:\\sigma} \\] Meta Theorems 证明都是简单的，熟练掌握归纳法就可以了 Uniqueness of types 若 \\(M:\\tau\\)，则 \\(\\forall \\sigma\\in{\\scr T}\\)，\\(M:\\sigma\\Rightarrow \\tau=\\sigma\\) 只需要证明 derivation tree 到项存在双射即可。 Progress 若 \\(\\vdash M:\\tau\\)，则 要么 \\(M\\) 为值 要么 \\(\\exists M&#39; eq M\\) 使得 \\(M\\rightarrow M&#39;\\) 说的是我们的类型系统可以保证所有通过类型检查的项不会卡住 Preservation 几个比较显然的引理： 若 \\(\\Gamma\\vdash M:\\tau\\)，则 \\(\\Gamma,x:\\sigma\\vdash M:\\tau\\) 若 \\(\\Gamma\\vdash M:\\tau\\)，则 \\(\\Delta\\vdash M:\\tau\\)，其中 \\(\\Delta\\) 为 \\(\\Gamma\\) 的任意重排 若 \\(\\Gamma,x:\\tau\\vdash M:\\sigma\\) 且 \\(\\Gamma\\vdash N:\\tau\\)，则 \\(\\Gamma\\vdash M[N/x]:\\sigma\\)。证明只需要对 \\(M\\) 结构归纳即可。 若 \\(\\Gamma\\vdash M:\\tau\\)，且 \\(M\\rightarrow M&#39;\\)，则 \\(\\Gamma\\vdash M&#39;:\\tau\\) 同样只需要对 \\(M\\)结构归纳即可。 Erasure Property 定义类型擦除函数 \\(\\text{erase}(\\cdot)\\)。Erasure Property 说的是 \\(\\text{erase}\\) 函数建立了 \\(\\lambda_\\rightarrow\\) 到 \\(\\lambda\\) 的同态（很显然不是双射）。或者你也可以说成是 \\(\\text{erase}\\) 函数与 \\(\\rightarrow\\) 是可交换的（虽然这里的两个箭头是各自项集上的二元关系）。 通常把这个 erasure 也叫抽象的 compilation，这时候就说的是无类型的 low-level（汇编层面）语义和原语言等价。","tags":["TAPL"]},{"title":"操作系统05 调度","path":"/2022/08/03/OS05-调度/","content":"事实上后面还有很多关于实时性和 case study 的高端内容，打算咕咕咕了回头再看。 引入 操作系统是硬件资源的统一管理者，因此需要由操作系统来“调度”这些资源以供程序使用。 经常谈到的调度包括但不限于 CPU 调度 IO 调度 页面调度 可以想到操作系统是面向使用场景的，因此评估调度优劣的准则就有很多，针对不同的使用场景和用户会有完全不同的调度策略，也会带来具有很大差别的表现。 实际上很多用户态的程序也需要调度，例如 golang 的协程调度、数据库的请求调度、甚至是 libc 库函数 printf 也会调度 syscall 来优化性能，这只是一个听起来厉害但是没啥大不了的概念。 下面默认讨论任务调度 评价指标 数据包括但不限于 吞吐量，即单位时间完成任务的数量 周转时间，即任务被发起到结束的时间 响应时间，即任务被发起到产生IO的时间 能耗，即调度器本身占用的资源 评价维度包括但不限于 公平性，即不同任务被调度的概率是否一致（或是否符合优先级关系） 实时性，即对周转时间有要求 而且因为现在程序比较复杂，调度器比较难做。jyy上课提到有用户程序可以自行提供调度特征来获得更好的表现的工作，可以类比操作系统提供绕开抽象层直接暴露底层接口给应用程序。 并且多核心也给调度带来了很多问题，例如如何优化 cache 预热、更好地支持任务在核心间协作等等。这些都让调度非常的难。 机制 任务的调度机制是基于任务状态切换的，回忆讲烂了的经典的五态模型。与之配套的有 CPU 调度（基于上下文切换） 页调度（基于虚拟内存） 实际上我们说的调度器就是一个无情的操纵任务状态的操纵手 交大的枫叶书上提到了三种不同的调度 长期调度 中期调度 短期调度 大意是把一个复杂的调度任务分解成 根据资源选择一个任务的子集 在子集里睡眠一些频繁切换/缺页的任务（经典的“挂起”） 剩下的任务按照策略调度 策略 单核 FCFS 先到先做 经典的问题就是小任务略晚于大任务，就会让小任务等待，平均等待时间很长 SJF 贪心，最短的先做 解决了大小任务同时到达的问题，但FCFS提到的问题还在 STCF 贪心，最先完成的先做 FCFS 和 SJF 都是非抢占式的，而 STCF 则会中途打断用户程序去完成最早完成的任务 RR 划分固定时间片(time slice)，规定每跑满一个时间片就调度一次。 在忽略调度开销的时候，时间片越短越好；但实际上时间片太短则会导致花在调度上的开销太大 MLQ 单纯的 RR 会带来“一视同仁”的问题，因此可以引入优先级的概念 MLQ 就是把任务集合对优先级等价关系作划分，每次从优先级最高的集合中选取一个调度，如果没有可以调度的就找更低优先级的任务集合 缺点是显见的：在高优先级任务多的时候，低优先级任务没法调度 优先级的引入搭配互斥锁还会带来优先级反转的问题 MLFQ 核心的idea就是自动给程序评定优先级。初始都是最高，时间片用完了就优先级下沉 这个策略是基于这样的观察：IO密集的程序往往CPU占用时间很少，因此可以保持较高优先级；而批处理任务则一直跑满，会沉底。 最暴力美学的地方在于，如果所有任务都沉底了怎么办？那就定期重新来一次 jyy针对这个MLFQ讲了一些比较好玩的东西，例如恶意程序可以“伪装”成IO密集的程序来骗取较高优先级。 Fair share 有的时候会需要多个用户共享一台计算机，此时我们希望各个用户按照一定的比例来分配资源（例如 CPU 时间） 此时先前的优先级就没法用了，因为①优先级只可比不可数，②同时基于任务的资源分配也比较麻烦 ②的解决方案很简单，只需要引入任务组，给任务组分配比例即可。每个任务组下又可以各自分配比例。 ①的解决方法用到了期望，即产生一个随机数，观察其落在哪个区间来决定当前时间片分配给哪个任务，这个办法叫做彩票算法 实际上基于“份额”的概念还能玩出很多花样，也有很多相关的 trick 用于优化实现 Stride 核心的idea在于给每个任务一个积累量，每次被调度则增加这个积累量 优先级则是通过积累量的增长率来体现的，优先级越高的任务积累量的增长率越低，这意味着到达同样的积累量准线需要更多次调度高优先级任务，合理。 这时候调度就变成了一个 findMin-&gt;updateValue-&gt;insertValue 的任务了，这也就是 Linux 里 CFS 的关键数据结构为什么要用红黑树 jyy也提到了很多这方面的好玩的取舍，例如 新生进程的积累量如何设定？ 长时间睡眠进程的积累量如何设定？ 父子进程的积累量是什么关系？ 内核源码里的 magic number 是怎么来的？ 多核 通常多核指的是 SMP 的情况，即对称多核心。更新的例如大小核异构架构的调度更是老大难问题。 Load sharing 最简单的就是抽象出一层虚拟单核 CPU 视为一个任务队列，每次调度任务就分配给一个空闲核执行，任务切换意味着把任务放回全局队列，并取出新的就绪任务执行。 优点是非常简单，甚至不需要特殊设计。缺点则很多，例如任务分配和同步带来的开销、cache 预热带来的性能损失（相当于抹去了各个核心的区别） 层级调度 就是给每个核心一个局部调度器，并且保证每个核心上的任务只会在局部等待队列被挂起，原本的全局调度器只负责任务的向下分派，而原本的 Load sharing 做法则是双向分派任务的。 像不像做 kmt 时用到的 thread_local 链表？ 协同调度 意思是把一个任务进行依赖关系的拆分（画 DAG），那么只要两个子任务不存在依赖关系就可以并行调度了。 当然这个最难的地方在于“知道”这是一个子任务的拆分","tags":["Operating System"]},{"title":"Haskell Parser Combinator","path":"/2022/07/26/Haskell-Parser-Combinator/","content":"欢迎指正本文的错误！ Intro What is Parser Combinator? 传统的 Parsing 包括 Lexing 和 Parsing 两部分，指的是用算法来解析字符串以得到某些特定形态的数据结构(通常是AST这样的树形结构) 实际上 Parsing 有非常 general 的结构，因此平时真正的用法都是把特定的语法 \\(G\\) 喂给一个程序 \\(P\\)，然后得到一个可以解析 \\(G\\) 的程序 \\(P_G\\)。通常会用一个 DSL(Domain Specific Language) 来描述 \\(G\\)，这样的程序 \\(P\\) 称为 Parser Generator。 好处：比较高效 坏处：生成代码可读性、换一个语法需要重写 DSL Parser Combinator 则是另一条路子：一个大的 parser 可以看成是若干小 parser 组合得来的，这样就可以把 structural 的语法信息包含在 parser 里，同时追求 parser 作为组件的复用。 好处：清爽、调试方便、可以复用 坏处：性能问题 Why Haskell? 实际上 Haskell 里的名字和符号都比较奇怪，我其实是看了 \\(\\text{F}\\sharp\\) 的一个 talk 才恍然大悟的。用哪种语言都一样，Haskell 看起来更 Geek 一点。 Single Char 首先要清楚 parser 是什么。我们的 parser 应当有至少一个函数 runParser，它会 吃进一个字符串 解析字符串 如果解析成功，那么返回一个解析过的数据，以及剩下的输入 如果解析失败，那么返回一个报错信息 在 OO 的语言中(例如 Java)很容易设计一个 class Parser，然后实现一个 runParser() 方法。在 Haskell 中则是通过设计一个带 field 的数据，然后令这个类型的数据为一个函数来实现的。 写成代码就是 data Parser c a = Success &#123; getTerm:: a, inputStream:: [c] &#125; | Failure String | Parser &#123; runParser:: [c] -&gt; (Parser c a) &#125; 这里比较巧妙的地方有两处： 利用 Type Variable 可以得到更 general 的 parser，即我们创造的是解析类型 c 得到类型 a 的 parser。 Haskell data 的 field 本身可以是函数，这样就得到了类似 class + method 的结构。类似 OO 的 interface 多态也可以用这个办法实现。 如果考虑最简单的 parser，那么就是 吃进一个字符串 解析一个字符 如果解析成功，那么返回这个字符，以及剩下的输入 如果解析失败，那么返回一个报错信息 这就是一个最简单的 parser，也就是一个单字符的 lexer。 satisfy:: (Eq c, Show c) =&gt; (c -&gt; Bool) -&gt; Parser c [c] satisfy predicate = Parser go where go [] = Failure &quot;EOF&quot; go (x:xs) | predicate x = Success [x] xs | otherwise = Failure $ &quot;Only to find &quot; ++ (show x) pChar:: Char -&gt; Parser Char [Char] pChar c = satisfy (== c) 同样是两个比较巧妙的地方 可以用一个简短的 helper 函数搭配 where 做到看起来更清晰 type class 可以让 satisfy 很 general 这样 pc = pChar 'c' 就得到了一个可以解析单个字符 'c' 的 parser pc 了。 Combination 可以回忆 parsing 过程中都会出现哪些结构： 在解析 C 语言函数定义的时候，我们希望先解析一个类型，再解析一个标识符。即 &lt;type&gt; &lt;identifier&gt; '(' &lt;optional_args&gt; ')' ';' 这样的结构 在解析 C 语言的基础类型时，可以是 int 或者 float 或者 short 等等。即 &lt;type1&gt; | &lt;type2&gt; | &lt;type3&gt; ... 这样的结构 在解析 C 语言字面量的时候，我们希望把一个 &lt;int_literal&gt; 从字符串变成值储存在符号表中 上述三种 pattern 就能抽象出三种组合 parser 的基本方式 map 关键就是把 Parser c 这个具有 *-&gt;* 的 Kind 看成 Functor，含义则是“包含了结果(类型为 a)的一个上下文” 那么结构3就是：给定一个包含了结果 a1 的 parser 和一个函数 a1-&gt;a2，我们希望得到一个能给出结果 a2 的新 parser，其中 a2 是通过函数作用于 a1 得来的。 注意到此处某个类型为 Parser c a 的数据的 field 实际上是一个函数 [c] -&gt; (Parser c a)，这与逻辑上 Parser c 是解析结果的容器并不冲突。 -- map instance Functor (Parser c) where fmap f (Parser p) = Parser $ \\input -&gt; case p input of Failure err -&gt; Failure err Success result rest -&gt; Success (f result) rest andThen Haskell 是纯函数式的，这意味着 Haskell 中是没有副作用的。 这句话也意味着在 Haskell 中，我们不应当对函数参数的求值顺序有任何假设。但这显然是不能满足我们要求的，例如我们现在就想要实现先执行一个 parser，再执行另一个 parser。 这时就需要用到 Monad 了。 Monad Monad 的直觉就在于，我们虽然不能保证一个函数的各个参数的求值顺序，但是可以保证一个函数的参数必须在其结果之前求值。 这样就可以把若干个过程按顺序串起来，最后得到一个顺序复合(Sequential Composition)。包一层 Monad 只是规定了这个处于上下文内的值的使用方法，仅此而已。 -- andThen instance Applicative (Parser c) where pure x = Parser $ \\input -&gt; Success x input Parser f &lt;*&gt; Parser p = Parser $ \\input -&gt; case f input of Failure err1 -&gt; Failure err1 Success result1 rest1 -&gt; case p rest1 of Failure err2 -&gt; Failure err2 Success result2 rest2 -&gt; Success (result1 result2) rest2 instance Monad (Parser c) where return = pure Parser p &gt;&gt;= f = Parser $ \\input -&gt; case p input of Failure err -&gt; Failure err Success result rest -&gt; p&#39; rest where Parser p&#39; = f result 实际上我们只会用到一个 dummy Monad，即 (Parser p1) &gt;&gt;= \\_ -&gt; (Parser p2) 表示两个 parser 的顺序组合，前一个 parser 结果的获取是捕获得来的(或者说在 do-notation 中不需要考虑这个)。 有了 andThen 还可以得到很多有意思的组合子，例如 pString:: [Char] -&gt; (Parser Char [[Char]]) pString = traverse pChar 可以用来解析一个字符串，非常优雅 orElse 需要一个新的 typeclass Alternative 然后就没啥好讲的了。 -- orElse class (Applicative f) =&gt; Alternative f where empty:: f a (&lt;|&gt;):: f a -&gt; f a -&gt; f a instance Alternative (Parser c) where empty = Parser $ \\input -&gt; Failure &quot;empty&quot; Parser p1 &lt;|&gt; Parser p2 = Parser $ \\input -&gt; case p1 input of Success result1 rest1 -&gt; Success result1 rest1 Failure err1 -&gt; case p2 input of Success result2 rest2 -&gt; Success result2 rest2 Failure err2 -&gt; Failure $ err1 一个比较好玩的组合子是 anyOf anyOf:: [Parser c a] -&gt; (Parser c a) anyOf (x:[]) = x anyOf (x:xs) = x &lt;|&gt; (anyOf xs) 即给定一串 parser，我们会得到一个新的 parser，新 parser 会挨个尝试这些链表中的 parser。 于是在 Haskell 里就可以非常优雅地写出字母的 lexer 了： pAlphabet = anyOf $ pChar &lt;$&gt; [&#39;a&#39;..&#39;z&#39;] Untyped \\(\\lambda\\)-Calculus 周末一直在写这个，事实上一直在写 Parsing 的部分 给出语法： T ::= x | (λx.T) | (T T) 很显然加上了括号之后这是没有左递归的，并且也没有二义性 代码里 Parsing 的部分也非常简单： -- parsing λ-terms pID = anyOf $ pChar &lt;$&gt; [&#39;a&#39;..&#39;z&#39;] pTerm = pVar &lt;|&gt; pAbs &lt;|&gt; pApp pVar = do v &lt;- pID return $ Var&#39; v pAbs = do pChar &#39;(&#39; pChar &#39;λ&#39; v &lt;- pVar pChar &#39;.&#39; t &lt;- pTerm pChar &#39;)&#39; return $ Abs&#39; v t pApp = do pChar &#39;(&#39; m &lt;- pTerm pChar &#39; &#39; n &lt;- pTerm pChar &#39;)&#39; return $ App&#39; m n Summarize 这个 parser 总共也才写了 107 行，除去看起来困难实际上繁琐的 AFM 也才 66 行，属于非常轻量的代码。如果用像 MegaParse 这样的库还可以写得更爽一些，也能有更好的性能。 实际上还有很多可以挖掘 能不能改善报错？AFM的过程中可以看到很多时候都在写错误处理，这里是可以加入更多信息来让错误更可读的 Parser c a 设计成结果和解析函数放在一起是否合适？用 Either Error Result 这样的类型是否更好？ 还有哪些组合 parser 的方法？这些就是全部了吗？ 但考虑到 tapl 后面的章节已经咕了很久，所以打算 move on 了。","tags":["haskell"]},{"title":"TAPL03 Untyped Lambda","path":"/2022/07/23/TAPL03-Untyped-Lambda/","content":"之前写形式语义已经来过一次了，这里就跳过一点写过的 Untyped Lambda Calculus 如果说理解C语言的最好办法就是看看编译后得到的指令序列的执行过程，那么理解FP的最好办法就是看看最底层的 \\(\\lambda\\)-calculus 的执行过程。并且此处是 Untyped \\(\\lambda\\)-calculus，与汇编有奇妙的对应关系。 \\(\\lambda\\)-calculus 本身为理解语言(desugar)提供了工具，并且可以很容易地添加各种 construct 来得到更高级的语言，这与利用汇编来理解C语言和为 runtime 增加特性以支持上层语言也是类似的。 Syntax \\[ T::= x\\mid \\lambda x.\\; T \\mid T_1\\;T_2 \\] 分别叫 Variable Abstraction Application Value 定义 ULC 中的值为形如 \\(\\lambda x.\\;T\\) 的项，其中 \\(T\\) 是任意项。可以发现这样意味着函数和值是等价的。 \\(\\alpha\\) 即 renaming。若项 \\(T\\) 经过 binder 换名后得到 \\(T&#39;\\)，那么就称 \\(T,T&#39; \\;\\alpha\\)-equivalent，记作 \\(T=_\\alpha T&#39;\\)，这是 \\(T\\) 上的一个等价关系(同余关系) \\(\\beta\\) 定义形如 \\((\\lambda x.\\; T)\\;T&#39;\\) 的项为 redex(Reducible Expression)，含义为可以进行一步执行的语句。这样一次 apply 操作也叫做 \\(\\beta\\)-reduction。 基于 \\(\\beta\\)-reduction 可以定义二元关系 \\(\\rightarrow_\\beta\\)、\\(\\twoheadrightarrow_\\beta\\)、\\(=_\\beta\\)，分别表示一步规约、任意多步规约、在规约意义下的等价关系 Semantics 这里默认大家都会项的替换了(回去看数理逻辑/形式语义课件)，关键在于给 redex 定序 full \\(\\beta\\)-reduction \\[ \\begin{aligned} &amp; \\frac{T\\rightarrow T&#39;}{\\lambda x.\\; T\\rightarrow \\lambda x.\\; T&#39;} \\\\\\;\\\\ &amp; \\frac{T_1\\rightarrow T_1&#39;}{T_1\\;T_2\\rightarrow T_1&#39;\\; T_2} \\\\\\;\\\\ &amp; \\frac{T_2\\rightarrow T_2&#39;}{T_1\\;T_2\\rightarrow T_1\\; T_2&#39;} \\\\\\;\\\\ &amp; \\overline{(\\lambda x.\\; T)\\;T&#39;\\rightarrow T[T&#39;/x]} \\end{aligned} \\] 这四条规则是平级的，意思是每次随便挑一个 redex 做 \\(\\beta\\)-reduction normal order strategy \\[ \\begin{aligned} &amp; \\overline{(\\lambda x.\\; T)\\;T&#39;\\rightarrow T[T&#39;/x]} \\\\\\;\\\\ &amp; \\frac{T\\rightarrow T&#39;}{\\lambda x.\\; T\\rightarrow \\lambda x.\\; T&#39;} \\\\\\;\\\\ &amp; \\frac{T_1\\rightarrow T_1&#39;}{T_1\\;T_2\\rightarrow T_1&#39;\\; T_2} \\end{aligned} \\] 这三条规则是从上到下的顺序依次执行的。normal order strategy 说的就是每次找到最左、最外侧的 redex 执行一步。 call by name strategy \\[ \\begin{aligned} &amp; \\overline{(\\lambda x.\\; T)\\;T&#39;\\rightarrow T[T&#39;/x]} \\\\\\;\\\\ &amp; \\frac{T_1\\rightarrow T_1&#39;}{T_1\\;T_2\\rightarrow T_1&#39;\\; T_2} \\end{aligned} \\] 意思是不能对函数体预先执行，并且在计算函数的时候将实参表达式作为整体替换掉函数体中的形参(回忆 \\(\\lambda\\)-calculus 中项的替换需要一些小操作满足替换后语义不变)，这就是 call-by-name 的含义。 haskell 使用的是 call-by-name 的一个变体 call-by-need，含义就是把相同的多个表达式捏成一个(指向同一个表达式实体)，参数仍然整体代换，并只在需要时求值。这样得到的其实是一个 Abstract Syntax Graph。 call by value strategy \\[ \\begin{aligned} &amp; \\overline{(\\lambda x.\\; T)\\;(\\lambda y.\\; T&#39;)\\rightarrow T[\\lambda y.\\; T&#39;/x]} \\\\\\;\\\\ &amp; \\frac{T&#39;\\rightarrow T&#39;&#39;}{(\\lambda x.\\; T)\\;T&#39;\\rightarrow (\\lambda x.\\; T)\\;T&#39;&#39;} \\\\\\;\\\\ &amp; \\frac{T_1\\rightarrow T_1&#39;}{T_1\\;T_2\\rightarrow T_1&#39;\\; T_2} \\end{aligned} \\] 含义是所有的函数体不能预先执行、实参必须化简为值后才能做形参代换。 一个比较有意思的例子是这样的，考虑经典的 \\[ \\begin{aligned} \\Omega&amp;=(\\lambda x.\\; x\\; x)\\;(\\lambda x.\\; x\\; x) \\\\ \\text{id}&amp;=\\lambda x.\\; x \\\\ \\text{false}&amp;=\\lambda t.\\;\\lambda f.\\; f \\end{aligned} \\] 对于如下程序 \\[ \\text{false}\\;\\Omega\\; id \\] CBV 将不会终止，CBN 和 normal order 将会得到 \\(\\text{id}\\)，full \\(\\beta\\)-reduction 的结果则是不确定的(很显然它至少包含了其余三种的结果) ADT 之前也记了很多用 \\(\\lambda\\)-calculus 来编码数据的操作。当我们在讨论 encode sth. in ULC 的时候，我们实际上是在建立某个数学模型到代码的同构。 常数据、数据构造子、数据上的操作函数共同构成了抽象数据类型的“公理”，即定义了这个类型中元素的集合。 以 list[A] 为例。我们不妨假设 \\(A\\) 可以被 ULC encode，此时记 \\(L\\) 为集合 \\(A\\) 上所有 list 构成的集合，那么有 \\[ \\begin{aligned} &amp; \\overline{\\text{NIL}\\in L} \\\\\\;\\\\ &amp; \\frac{l\\in L,a\\in A}{\\text{cons}(a,l)\\in L}\\\\\\;\\\\ &amp; \\frac{l\\in L,a\\in A}{\\text{head}(\\text{cons}(a,l))=a}\\\\\\;\\\\ &amp; \\frac{l\\in L,a\\in A}{\\text{tail}(\\text{cons}(a,l))=l} \\end{aligned} \\] 此处 \\(\\text{NIL}\\) 为常元，\\(\\text{cons}\\colon A\\times L\\mapsto L\\) 是数据构造子，\\(\\text{head}\\colon L\\mapsto A\\) 和 \\(\\text{tail}\\colon L\\mapsto L\\) 是数据上的操作函数。这就是一个简单的代数(公理)系统。 encoding 的过程就是构造所有 ULC 的项 \\(T\\) 到所有链表 \\(L\\) 的双射 \\(\\pi\\colon L\\mapsto T\\)，以及对应的数据构造子、操作函数，使得这些映射对于这些操作而言是同构的。 从工程的角度看，这就是给出了一个 formal specification，然后我们用 formal 的方法证明了这个encoding(实现)是完全符合 spec 的。就这么简单。 Recursion 大名鼎鼎的 Y-combinator，给出 call-by-name 时的一种定义 \\[ Y = \\lambda F.\\; (\\lambda x.\\; F\\; (x\\; x))\\; (\\lambda x.\\; F\\; (x\\; x)) \\] 只需要注意到 \\[ YF = F\\; ((\\lambda x.\\; F\\; (x\\; x))\\; (\\lambda x.\\; F\\; (x\\; x)))=F(YF) \\] 在 call-by-value 时，求值顺序会强迫 \\(Y\\) 不断执行，意味着没法终止。这时需要一个小技巧把 \\(x\\; x\\) 这样的项在不改变语义的情况下变成值 \\[ Y&#39;=\\lambda F.\\; (\\lambda x.\\; F\\; (\\lambda y.\\; x\\;x\\;y))\\; (\\lambda x.\\; F\\; (\\lambda y.\\; x\\;x\\;y)) \\] 非常简单，用C语言类比也就是把一条可以执行化简的表达式 a+b+c; 变成了一个函数 int func() &#123; return a+b+c; &#125; 使得执行不能深入函数体内(hso)。 De Bruijn Form Q: 这个名字怎么念？ \\(=_\\alpha\\) 是等价关系，因此可以做 \\(T/=_\\alpha\\) 得到所有项在换名等价意义下的代表元。即我们希望研究一种等价意义下最简单的形式。 实际上 De Bruijn 敏锐意识到自然数就可以作为变量名，并且自然数天生可以表征一些 AST 上的结构信息，使得我们的解释器可以写起来更方便。 intuition 考虑 ULC 这个树形结构，一个约束变量的 binder 必然是其出现节点的祖先。De Bruijn Form 考虑的就是用自然数来表征每个约束变量 binder 的位置，即从当前节点要往上爬多少层 Abstraction 才能走到 binder(因为只有 Abstraction 会引入新的 binder)。不妨记这个为 binder depth。 naming context 若 \\(FV(T)=\\varnothing\\)，则称 \\(T\\) 为封闭项(closed term)或组合子(combinator)。 还可以定义 \\(\\Lambda^k=\\Set{t\\in T\\mid \\lvert{FV(t)}\\rvert=k}\\)，那么 combinator 就是 \\(\\Lambda^0\\) \\(FV(T)\\) 中的变量是找不到 binder 的，因此引入 naming context 的概念来给自由变量规定 binder，定义 context 为 \\(\\Gamma\\colon V\\mapsto \\mathbb N\\) 用以表示规约的环境。 shifting 注意到替换 \\(T[U/x]\\) 本质上是两棵 AST 的嫁接，\\(U\\) 中的自由变量的 binder 应当整体增加 \\(x\\) 在 \\(T\\) 中的 binder depth。 结构归纳地定义函数 \\(\\uparrow_c^d(t)\\) 如下 \\[ \\begin{aligned} \\uparrow_c^d(k)&amp;=\\left\\{\\begin{aligned}k+d&amp;,k\\ge c\\\\k&amp;,k&lt;c\\end{aligned}\\right. \\\\ \\uparrow_c^d(\\lambda x.\\; t)&amp;=\\lambda x. \\uparrow_{c+1}^d(t) \\\\ \\uparrow_c^d(m\\; n)&amp;=\\uparrow_c^d(m)\\;\\uparrow_c^d(n) \\end{aligned} \\] 这个操作就叫 shifting。 substitution \\[ \\begin{aligned} (\\lambda .\\; t)[t&#39;/x]&amp;=\\lambda .\\; (t[\\uparrow^1(t&#39;)/(x+1)]) \\end{aligned} \\] E-APP \\[ (\\lambda .\\; m)\\;n\\rightarrow \\uparrow^{-1}(m[\\uparrow^1(n)/0]) \\]","tags":["TAPL"]},{"title":"TAPL02 Basics","path":"/2022/07/17/TAPL02-Basics/","content":"Notations 简单的就不说了 Mathematics Partial Function 称 \\(f\\subseteq A\\times B\\) 这个二元关系为 partial function 当且仅当 \\(\\forall a\\in A, \\forall b_1,b_2\\in B, afb_1\\wedge afb_2\\Rightarrow b_1=b_2\\). 之所以说是 partial 的是因为 \\(f\\) 不一定对 \\(A\\) 中所有元素都有定义。一个写法是 \\(f(a)=\\bot\\) 表示 \\(a ot\\in D(f)\\)，经典的含义是 \\(f\\) 对输入 \\(a\\) 不会终止。 Relation Closure 二元关系 \\(R\\) 的 reflexive closure 记作 \\(R^+\\)，transitive closure 记作 \\(R^*\\) Preservation by Relation 考虑 \\(R\\subseteq A\\times B\\)，\\(P\\) 是谓词，若 \\(\\forall a,b, P(a)\\wedge aRb\\Rightarrow P(b)\\)，那么称 \\(P\\) 在 \\(R\\) 下保持。 Syntax 可以用如下几种等价方式定义语法 BNF 递归构造句子的集合 定义合法句子集合的性质，声明合法句子集合是满足这些性质的最小集 Semantics 主要是小步操作语义(Small Step Operational Semantics)，本质上是定义了一个抽象的状态机(不一定要是确定状态机)以及这个抽象状态机上的状态转换 方便起见，下面记语言的集合为 \\(T\\) reduction relation 二元关系 \\(\\rightarrow\\subseteq T\\times T\\) 被称为 reduction relation。通常用若干规则(Inference Rules)来定义这样的规约关系。例如 \\(\\dfrac{a\\rightarrow b\\quad b\\rightarrow c}{a\\rightarrow c}\\) 就展示了一种有传递性的规约关系。 If-Then-Else 给出如下例子 \\[ \\dfrac{}{\\text{if True then $s_1$ else $s_2$}\\rightarrow s_1} \\\\ \\\\ \\dfrac{e\\rightarrow e&#39;}{\\text{if $e$ then $s_1$ else $s_2$}\\rightarrow \\text{if $e&#39;$ then $s_1$ else $s_2$}} \\\\ \\] 第一条规则称为公理(Axiom)，这样直接进行计算的规则也叫Computation Rule。 第二条的前提(Premise)是 \\(e\\rightarrow e&#39;\\)，这样只是指出要对哪个 subterm 做规约的规则叫Congruence Rule(回忆Evaluation Context) Normal Form/Value 如果一个项没法继续规约了，就称这个项为Normal Form Term。 Value是认为规定的一个项的集合，通常希望Value是Normal Form的。 一些非预期的Normal Form往往意味着违反了语言的语义(或是未定义行为)，此时可以给这样的错误情况定一个 \\(\\overline{t\\rightarrow \\bot}\\) 的规则，这样就是前面提到的运行时错误检查了(不存在UB)。 Property/Metatheory 一些这一章提到的比较基础的语言性质 确定性(Determinism)，即每次只有唯一的规则可以推导 终止性(Termination)，即推导有限次可以变成Normal Form Diamond Property，即使没有单步推导的确定性，也有总体上的确定性。 Diamond Property是比较重要的，但是咕咕咕","tags":["TAPL"]},{"title":"TAPL01 Intro","path":"/2022/07/16/TAPL01-Intro/","content":"Type System Definition A type system is a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute. 原文用的plausible来描述这段定义。实际上 Type System 说的就是种静态检查代码的一种方法，其计算的结果是对一段程序运行状态的近似(是不是想到了静态分析)。具体而言，这种计算是通过给元素赋予类型来进行的，赋予类型的过程通常是根据其语法结构开展的(syntax-directed/compositional)。 static &amp; dynamic 我们说一种语言是静态类型的(statically typed)，意味着这门语言的类型检查(type checking)是在编译期完成的。对应的是动态类型(dynamically typed)，说明这门语言的类型检查是在运行时进行的。 很显然type checker需要做出保守的估计，即某些实质上正确的程序没法通过type checking。这是因为type checker往往只能证明某些情况下的某些错误不存在。对于没法证明/太难证明但实质上正确的程序就会拒绝。这里就引入了表达能力(实质上正确的程序集合)和保守性(type checker认为正确的程序集合)的权衡，一般而言会偏向限制表达能力来让type checker更好做，这样也更容易写出正确的程序(参见抖S的Rust编译器)。 通常 Type System 也只能证明通过检查的程序不会有某些错误，即只能检测所有错误的一个子集。那些Type System可以预防的错误称为运行时类型错误(run-time type errors)。然而前面说过type checker没法检测出所有错误，因此这些错误可以在运行时被检测处理。 safe &amp; unsafe 另一对概念是safe-unsafe，原文给的定义是 A safe language is one that protects its own abstractions. 也有说法是 A safe language is completely defined by its programmer's manual. 具体而言就是语言的行为完全由specification定义，不存在UB和编译器自由实现的部分。 language safety和type safety是两回事。type checking是达到language safety的一种手段，另一种手段则是运行时检测处理，例如Java的ArrayOutOfBound异常等等。 例如lisp就是动态类型且安全的语言，Java则是静态类型且安全的语言(回想Java中多如牛毛的Exception)。 Benefits 检测错误，把问题控制在编译期 更好地抽象，类型本质上是给数据集合起名字 可读性，类型可以附带语义信息 语言安全 效率更高，类型标注后可以省去不必要的运行时检查 还可以带来一些安全性的提升，例如引入priority作为类型以保证高机密变量的值不会流入低机密的变量(回想SPA提到的Taint Analysis) Design 通常类型系统的设计和语言息息相关。首先类型注解本身就是语法的一部分。动态类型的语言往往更灵活、(看起来)更简单。如果想要给动态类型语言加上静态的类型检查，往往要丧失一些表达能力(前面提到的type checker的保守性)。 类型的设计也是可以取巧的，例如可以引入类型推导(type inference)使得语法更简洁，例如Java中就可以这么写 var sum = 1 + 2; 此处sum的类型是可以在编译器获得的。","tags":["TAPL"]},{"title":"操作系统04 进程与线程","path":"/2022/07/15/OS04-进程与线程/","content":"进程 进程是运行中的程序实体。如果把代码视为状态机，那么进程即处于特定状态的状态机。 进程的状态 经典的五态模型：新生-&gt;就绪-&gt;运行-&gt;阻塞-&gt;终止 进程的状态由内核调度器负责维护 此外还有一些其它状态（页表基地址、栈指针、进程组等等）储存在进程控制块(Process Control Block)内。本质上是一个内核里的数据结构。 进程切换 回忆程序执行的上下文、上下文切换、栈切换等等ICS内容 实际上现代操作系统的最小调度单位是线程，即单个进程内可以有多个并发的执行流。 进程相关API 主要是创建进程、销毁进程 fork() 语义是复制当前进程cp得到一个子进程sp。若cp存在多个执行流，那么sp中只有执行fork()的执行流被复制了一份。考虑到fork()来自上个世纪，这一点是很自然的。 fork()通过返回值区分父子进程，除此之外二者没有任何区别(不考虑多线程)。与fork()相关的重要技术是Copy-On-Write。 Linux上的第一个进程是/usr/sbin/init，在Ubuntu22.04下可以看到是systemd的符号链接。想想还是很神奇的，所有的进程同宗同源，都是分裂来的。 时至今日fork()也有了落后时代的地方，具体可以看微软那篇paper，一些quirk包括但不限于： printf在buffered时fork会与预期不同 多线程fork 文件描述符继承带来的安全隐患 性能问题 execve() 语义是将当前进程重置为指定程序的初始状态。常见做法是搭配 fork()+execve() 实现新进程运行新的程序，同时在二者之间可以做一些初始化的操作。 kill() 实际上kill()只负责发送信号，具体结束进程的操作是由sighandler实现的，可以看后续的进程间通信部分。 进程管理 wait() 进程同步，即可以等待子进程结束，具体参数看手册。 wait()带来的一个设计就是僵尸进程。如果父进程先于子进程挂掉了，那么这些孤儿进程将无法被wait()回收，这也是为什么叫僵尸 Linux和xv6的设计是由init进程定期收养(手册原文是adopt，哈哈)僵尸进程，然后由init来wait() 进程组(process group) 定义为进程的集合，每个进程属于唯一的进程组，父子进程默认在同一个进程组。 操作系统可以以进程组为接收单位发送信号(signal)， 会话(session) 定义为进程组的集合，分为前台进程组和后台进程组。那么一个会话就可以通过前台进程与用户交互，进而控制后台进程组中的进程。 线程 进程创建和切换的开销比较大(涉及到地址空间切换，要预热cache冲刷TLB)，因此引入了更轻量级的调度单位线程。 线程是一个进程内的执行流，共享同一个地址空间。为了实现多个执行流的并发执行，需要多个线程栈。 内核态线程 内核是最早的多线程程序，内核需要并行地为多个用户进程提供服务是原因之一。 内核态线程是内核创建的线程(废话)，也是操作系统可以直接调度的实体。这意味着如果有n核n内核线程，那么这n个线程就是可以同时执行的。 这也意味着内核线程的数量限制了内核并发提供系统调用的能力 用户态线程 用户态线程对内核透明。注意到实现多个逻辑上并发的执行流并不需要陷入内核yield()，因此可以利用例如makecontext()或setjmp()等API来构造用户态上下文，然后在用户态管理多个并发的执行流。 一个例子是协程 线程本地存储(Thread Local Storage) 通过一个关键字即可实现Seemingly全局变量，Factually线程局部的全局变量。实现可以通过特殊的寄存器(X86用fs,risc-v用tp)加上偏移量 POSIX线程库 man -k pthread_* 纤程 多进程调度引入抢占式调度是因为不同进程相互隔离、互不信任，因此每个进程不能长时间霸占CPU，这是前提； 而多线程同样利用抢占式的调度就不合理了，因为同一个进程内的执行流应当相互信任、相互配合以达到效果。因此引入纤程(用户态线程)的概念，使得用户程序能够利用更多的信息实现调度的最优化(例如主动让出调度) 一些编程语言(C++ Go Lua)也支持用户态线程的创建和管理，称为协程。","tags":["Operating System"]},{"title":"Algebra03 同态与同构","path":"/2022/07/09/Algebra03-同态与同构/","content":"群的同构意味着二者本质上没有区别, 同态则可以导出某些同构. 下面默认 \\(G_1, G_2\\) 是两个群. 同态 若存在映射 \\(f\\colon G_1\\mapsto G_2\\), 且 \\(\\forall a,b\\in G_1, f(ab)=f(a)f(b)\\), 那么称 \\(f\\) 是 \\(G_1\\) 到 \\(G_2\\) 的同态. 若 \\(f\\) 为单射, 则 \\(f\\) 为 \\(G_1\\) 到 \\(G_2\\) 的单同态. 若 \\(f\\) 为满射, 则 \\(f\\) 为 \\(G_1\\) 到 \\(G_2\\) 的满同态. 下面默认 \\(f\\) 是 \\(G_1\\mapsto G_2\\) 的同态. 定理1 若有同态 \\(f\\colon G_1\\mapsto G_2, g\\colon G_2\\mapsto G_3\\), 那么有同态 \\(g\\circ f\\colon G_1\\mapsto G_3\\) 证明是简单的. 定理2 \\(f(e_1)=e_2\\), 其中 \\(e_1, e_2\\) 分别为 \\(G_1, G_2\\) 的幺元. 注意到 \\(f(e_1)f(e_1)=f(e_1e_1)e_2=f(e_1)e_2\\), 由消去律 \\(f(e_1)=e_2\\). 定理3 \\(\\text{Im}f\\leqslant G_2\\). 结合律是显然的 \\(f(e_1)\\) 是幺元 \\({f(a)}^{-1}f(b)=f({a}^{-1}b)\\in\\text{Im}f\\) 因此 \\(\\text{Im}f\\) 成群. 又因为 \\(\\text{Im}f\\subseteq G_2\\), 所以 \\(\\text{Im}f\\leqslant G_2\\). 这说明 \\(f\\) 即使不是满同态, 也可以选取 \\(G_2&#39;=\\text{Im}f\\) 来使得 \\(f\\) 为 \\(G_1\\mapsto G_2&#39;\\) 的满同态. 核 定义 \\(\\ker f=\\Set{a\\in G_1\\mid f(a)=e_2}\\), 称为 \\(f\\) 的核, 也叫零空间. 定理4 \\(\\ker f\\lhd G_1\\). 首先 \\(e_1\\in \\ker f\\), 且 \\(\\forall a,b\\in G_1\\), 若 \\(f(a),f(b)\\in \\ker f\\), 则 \\(f({a}^{-1}b)=f({a}^{-1})f(b)=e_2\\), 因此 \\(\\ker f\\leqslant G_1\\). 接着 \\(\\forall g\\in G_1, h\\in \\ker f\\), 有 \\(f({g}^{-1}hg)=f({g}^{-1})f(h)f(g)=f({g}^{-1}g)=e_2\\). 定理5 \\(f\\) 是单同态, 当且仅当 \\(\\ker f=\\set{e_1}\\). 若 \\(\\ker f=\\set{e_1}\\), 由反证法, 设 \\(\\exists a,b\\in G_1 \\text{ s.t. } f(a)=f(b)=c\\in G_2\\) 那么有 \\(f({a}^{-1}b)={f(a)}^{-1}f(b)={c}^{-1}c=e_2\\), 而 \\({a}^{-1}b eq e_1\\), 这与 \\(\\ker f=\\set{e_1}\\) 矛盾. 故假设不成立. 另一侧是显然的. 同构 若 \\(f\\colon G_1\\mapsto G_2\\) 既是满同态, 又是单同态, 那么 \\(f\\) 就是一个同构. 群同态第一定理 定理的顺序有很多说法 若 \\(f\\colon G_1\\mapsto G_2\\) 是满同态, \\(\\ker f\\lhd G_1\\), 那么 \\(\\sigma\\colon G_1/\\ker f\\mapsto G_2\\) 是同构, 其中 \\(\\sigma(a\\ker f)=f(a)\\). 有些地方会表述成 \\(\\sigma\\colon G/\\ker f\\mapsto \\text{Im }f\\), 看起来更漂亮一些. 首先需要证明 \\(\\sigma\\) 是良定义的映射, 即 \\(\\forall a,\\forall b\\in [a], \\sigma(a\\ker f)=\\sigma(b\\ker f)\\). 注意到 \\({a}^{-1}b\\in\\ker f\\), 因此 \\(f({a}^{-1}b)={f(a)}^{-1}f(b)=e_2\\), 立即有 \\(f(a)=f(b)\\). \\(\\sigma\\) 显然是满同态; \\(\\forall a,b\\in G\\), \\(f(a)=f(b)\\Rightarrow {a}^{-1}b\\in \\ker f\\Rightarrow a\\ker f=b\\ker f\\). 说明 \\(\\sigma\\) 是单射. 故 \\(\\sigma\\) 是同构. 可以回忆一下高代里讲过 \\(\\dim V = \\dim \\ker V + \\dim \\text{Im }V\\). 又因为 \\(\\dim (V/W)=\\dim V - \\dim W\\), 结合有限维线性空间同构当且仅当维数相同, 其实就是 \\(V/\\ker V\\mapsto \\text{Im }V\\) 同构. 对应定理 设 \\(f\\) 是群 \\(G\\) 到 \\(\\text{Im}f\\) 的满同态，定义 \\(\\sigma\\colon K\\mapsto \\Set{f(k)\\mid k\\in K}\\)，则 1. \\(\\sigma\\) 为 \\(\\Set{H\\leqslant G\\mid \\ker f\\subseteq H}\\to \\Set{H\\leqslant\\text{Im}f}\\) 的双射 2. \\(\\sigma\\) 将正规子群映射为正规子群 3. 若 \\(H\\lhd G\\) 且 \\(\\ker f\\subseteq H\\)，那么 \\(G/H\\cong \\text{Im} f/\\sigma(H)\\) 这个定理讲的是通过群之间的满同态 \\(f\\)，我们可以诱导出使得子群间一一对应的双射 \\(\\sigma_f\\) 为了追求双射的性质，这里对 \\(G\\) 的子群也就是 \\(\\sigma_f\\) 的定义域作出了限制：只考虑包含 \\(\\ker f\\) 的那些子群 对应定理的一个特殊情形就是当 \\(f\\) 为 \\(G\\) 到其商群 \\(G/N\\) 的自然同态时，\\(\\ker f=N\\)，\\(\\text{Im} f=G/N\\)，\\(\\sigma(H)=H/N\\)，写出来就是 \\(G/H\\cong (G/N)(H/N)\\)，当 \\(H=N\\) 时即为群同态第一定理","tags":["Algebra"]},{"title":"Algebra02 子群和商群","path":"/2022/07/06/Algebra02-子群和商群/","content":"主要关注怎么由已有群得到新的群. 如果上过正经高代应该会对这一套流程比较熟悉 子群 $ G$ 为子群, 非空子集 \\(H\\subseteq G\\) 是 \\(G\\) 的子群当且仅当 \\(H\\) 在 \\(G\\) 中同一运算下为群. 必然有 \\(e\\in H\\), 且 \\(e_H=e_G\\). 由反证法设 \\(e_H eq e_G\\), 则 \\(e_H e_G = e_H = e_G\\), 此处的运算均在 \\(G\\) 中进行. 子群的等价定义 定义1 \\(H\\subseteq G\\) 为子群, 当且仅当 \\(\\forall a, b\\in H, ab\\in G\\) \\(\\forall a\\in H, a^{-1}\\in G\\) 容易验证此定义与 \\(H\\) 为群等价. 定义2 \\(H\\subseteq G\\) 为子群, 当且仅当 \\(\\forall a, b\\in H, ab^{-1}\\in H\\) 注意到 \\(\\forall a\\in H, aa^{-1}=e\\in H\\), 且 \\(\\forall a\\in H, ea^{-1}=a^{-1}\\in H\\). 易得定义2与定义1等价. 子群之交 子群之交仍为子群. 若 \\(H_1,H_2\\leqslant G\\) 且 \\(H_1 eq H_2\\), 那么 \\(H_1\\cap H_2\\leqslant G\\) 很显然 \\(H_1\\cap H_2\\) 非空 (至少有 \\(e\\in H_1, e\\in H_2\\)) \\(\\forall a,b\\in H_1\\cap H_2\\), 那么有 \\(a,b\\in H_1\\) 且 \\(a,b\\in H_2\\). 这说明 \\(ab^{-1}\\in H_1\\) 且 \\(ab^{-1}\\in H_2\\), 即 \\(ab^{-1}\\in H_1\\cap H_2\\). 由定义2可知 \\(H_1\\cap H_2\\) 为群. 子群之并 子群之并不一定仍为子群. 取 \\(H_1,H_2\\leqslant G\\), 且 \\(H_1 eq H_2\\) 没有包含关系. 由反证法, 设 \\(H_1\\cup H_2\\) 仍为子群, 则取 \\(a\\in H_1\\backslash H_2, b\\in H_2\\backslash H_1\\), 观察 \\(c=ab\\) \\(c\\in H_1\\), 此时 \\(a, c\\in H_1\\), 由定义2 \\(ca^{-1}=b\\in H_1\\) 矛盾; \\(c\\in H_2\\backslash H_1\\), 此时 \\(b, c\\in H_2\\), 由定义2 \\(cb^{-1}=a\\in H_2\\) 矛盾; 故假设不成立, \\(H_1\\cup H_2\\) 不是子群. 当然在 \\(H_1\\leqslant H_2\\leqslant G\\) 的情况下, \\(H_1\\cup H_2=H_2\\leqslant G\\) 是一个子群. 有限子群 有限群 \\(G\\) 的子集 \\(H\\) 若对运算封闭, 那么 \\(H\\leqslant G\\) 考虑任取 \\(a\\in H\\), \\(\\Set{a^1， a^2\\ldots a^k}\\), 由 \\(G\\) 的有限性必然存在 \\(k&#39;\\in \\mathbb{N}\\) 使得 \\(a^{k&#39;}=e\\). 由这个序列容易得到 \\({a}^{-1}=a^{k&#39;-1}\\). 商群 对于群 \\(G\\) 和子群 \\(H\\leqslant G\\), 可以定义 \\(G\\) 上的二元关系 \\(R_H\\) 为 \\[ \\forall a,b\\in G, aR_Hb\\iff a^{-1}b\\in H \\] 易验证 \\(R_H\\) 是 \\(G\\) 上的等价关系, 由此可导出一个 \\(G\\) 的划分 \\(G/R_H\\), 也可记作 \\(G/H\\). 这个等价关系看起来也许有些奇怪, 可以考虑一个线性空间中的直观例子: 在三维欧式空间 \\(V\\) 中, 取 \\(z\\) 轴这个子空间 \\(Z\\). \\(\\forall \\alpha,\\beta\\in V\\), 定义二元关系 \\(\\alpha R\\beta\\iff \\alpha-\\beta\\in Z\\). 可以发现, 这样定义的二元关系\"连接\"了 \\(z\\) 轴垂直穿过的所有水平平面上的点, 此时作陪集划分 \\(V/X\\) 就是把三维空间\"压成\"了二维平面, 新平面中的每个点表示原本的一条垂直线. \\(G\\) 上关于 \\(H\\) 的陪集划分 给定 \\(H\\leqslant G\\), 关于元素 \\(a\\in G\\) 的左陪集定义为 \\(aH=\\Set{ah\\mid h\\in H}\\) 有 \\(aH=[a]_R\\), 即元素 \\(a\\) 的左陪集恰好等于其在关系 \\(R_H\\) 下的等价类. 即 \\(\\forall b\\in G, aR_H b\\iff b\\in aH\\). 证明是简单的. 通常把 \\(G/H\\) 称为 \\(G\\) 关于 \\(H\\) 的左陪集划分, 也叫 \\(G\\) 关于 \\(H\\) 的左商集空间. 指数和 Lagrange 定理 \\(G\\) 为有限群. 定义 \\([G:H] = \\left\\lvert G/H\\right\\rvert\\) 为 \\(H\\) 在 \\(G\\) 中的指数. 由左消去律可知 \\(\\forall a\\in G, \\left\\lvert aH\\right\\rvert=\\left\\lvert H\\right\\rvert\\), 且 \\(G/H=\\Set{aH\\mid a\\in G}\\) 为一个划分, 因此 \\(\\left\\lvert H\\right\\rvert [G:H] = \\left\\lvert G\\right\\rvert\\) 若有 \\(H\\leqslant K\\leqslant G\\) 为群, 那么有 \\[ [G:K]=[G:H] [H:K] \\] 连续两次 Lagrange 定理即得. 同余关系 \\(G\\) 中运算导出到 \\(G/H\\) 中, 要求 \\(R\\) 是同余关系, 即 \\[ \\left. \\begin{aligned} a^{-1}b\\in H \\\\ c^{-1}d\\in H \\end{aligned} \\right\\} \\Rightarrow {c}^{-1}{a}^{-1}bd\\in H \\] 为了形式上的对称, 可以写成 \\[ {c}^{-1}{a}^{-1}b(c {c}^{-1})d={c}^{-1}({a}^{-1}b)c ({c}^{-1}d)\\in H \\] 注意到 \\({c}^{-1}d\\in H\\), 因此上式成立只需要 \\({c}^{-1}({a}^{-1}b)c\\in H\\). 又由 \\(a,b\\) 的任意性, 只需要 \\({c}^{-1}hc\\in H\\) 对任意的 \\(h\\in H\\) 成立. 这是正规子群的来源. 正规子群 \\(H\\leqslant G\\), 称 \\(H\\) 为 \\(G\\) 的正规子群, 当且仅当 \\(\\forall g\\in G, \\forall h\\in H, {g}^{-1}hg\\in H\\), 记为 \\(H\\lhd G\\) 左右陪集定义 若 \\(H\\leqslant G\\) 且 \\(\\forall a\\in G\\) 都有\\(aH = Ha\\) 则 \\(H\\lhd G\\) 注意到 \\(aH=Ha\\) 可得 \\(\\forall h\\in H, ah {a}^{-1}\\in H\\), 且 $HG $ 可得 \\(\\forall h\\in H, \\exists h&#39;\\in H \\text{ s.t. } ah {a}^{-1} = h&#39;\\), 此时 \\(ah\\mapsto h&#39;a\\) 即为 \\(aH\\mapsto Ha\\) 的双射. 陪集运算定义 若 \\(H\\leqslant G\\) 且 \\(\\forall a,b\\in G, aH bH=\\Set{ah_1bh_2\\mid \\forall h_1,h_2\\in H}=abH\\), 那么有 \\(H\\lhd G\\). 若 \\(H\\lhd G\\), 由正规子群的左右陪集定义, \\(ah_1bh_2=a(h_1b)h_2\\), 而 \\(h_1b\\in Hb=bH\\), 因此存在 \\(h_1&#39;\\) 使得 \\(ah_1bh_2=abh_1&#39;h_2\\in abH\\) 若 \\(aHbH=abH\\), 则取 \\(b={a}^{-1}\\) 就有 \\(aH {a}^{-1} H=H\\). 这说明 \\(\\forall h_1,h_2\\in H\\), \\(ah_1 {a}^{-1} h_2\\in H\\), 即 \\(ah_1 {a}^{-1}\\in H\\). 非常奇妙, 我们怀着凑出一个同余关系的目的, 找到了一类特殊的子群, 并且发现在这类子群下的左右陪集划分竟然相等, 对称性在这里统一了. 同余关系和正规子群 给定子群 \\(H\\leqslant G\\), 我们有 \\[ \\text{$R_H$ 是 $G$ 上的同余关系}\\iff H\\lhd G. \\] \"\\(\\Rightarrow\\)\": 由同余关系的定义, \\(\\forall a,b,c,d\\in G, a R_H b, c R_H d\\Rightarrow ac R_H bd\\) 即给定 \\({a}^{-1}b\\in H, {c}^{-1}d\\in H\\) 必然有 \\({\\left({ac}\\right)}^{-1}bd\\in H\\), 求证 \\(H\\lhd G\\). 任取 \\(g\\in G, h\\in H\\), 显然有 \\(ghR_H g\\) 且 \\({g}^{-1}R_H e\\). 由同余关系, \\(gh {g}^{-1} R_H g\\), 即 \\(gh {g}^{-1}\\in H\\). \"\\(\\Leftarrow\\)\": 由正规子群的左右陪集定义, \\(aH=Ha\\) 注意到 \\({a}^{-1}b\\in H\\), 因此 \\({\\left({ac}\\right)}^{-1}bd={c}^{-1}{a}^{-1}bd\\in {c}^{-1}Hd={c}^{-1}dH=H\\).","tags":["Algebra"]},{"title":"Algebra01 群的定义","path":"/2022/07/06/Algebra01-群的定义/","content":"觉得要好好学一学这个东西了 代数系统 定义 称 \\(\\left&lt;G,\\circ\\right&gt;\\) 为代数系统, 当且仅当 \\(G\\) 为非空集合, \\(\\circ\\colon G\\times G\\mapsto G\\) 为 \\(G\\) 上封闭二元运算. 等价关系 划分 给出 \\(G\\) 上的等价关系 \\(R\\subseteq G\\times G\\), 可得 \\(G\\) 的划分 \\(P=\\Set{G_i}\\). 反之给出划分亦可得由划分定义的等价关系. 记 \\(P=G/R\\) 为 \\(G\\) 关于 \\(R\\) 的商集, 映射 \\(\\pi\\colon g\\mapsto [g]\\) 称为自然映射. 其中 \\([g]=\\Set{x\\mid xRg}\\) 可构造新代数系统 \\(\\left&lt;G/R, \\circ&#39;\\right&gt;\\). 其中 \\(\\circ&#39;\\) 定义为 \\([a]\\circ&#39; [b]=[a\\circ b]\\). 为了保证 \\(\\circ&#39;\\) 是良定义的 (\\(\\forall c\\in [a], c eq a\\) 都有 \\([c\\circ b]= [a\\circ b]\\)), 应满足 \\(\\forall a_1,a_2,b_1,b_2\\), \\(a_1Rb_1\\wedge a_2 R b_2\\Rightarrow (a_1\\circ a_2)R(b_1\\circ b_2)\\). 这个特殊的关系又叫同余关系. 半群 \\(\\left&lt;G,\\circ\\right&gt;\\) 是半群当且仅当 \\(G\\) 是代数系统, 且 \\(\\circ\\) 有结合律 \\[ \\forall a,b,c\\in G, (ab)c=a(bc) \\] 幺半群 半群 \\(\\left&lt;G,\\circ\\right&gt;\\) 是幺半群当且仅当存在幺元 \\[ \\exists e\\in G \\text{ s.t. } \\forall a\\in G, ae=ea=a \\] 左右幺元 左幺元定义为 \\[ \\exists e_L\\in G \\text{ s.t. } \\forall a\\in G, e_La=a \\] 若左右幺元皆存在, 那么它们必然相等: \\[ e_L = e_Le_R = e_R \\] 幺元 \\(e\\in G\\) 为幺元当且仅当 \\(e\\) 既是左幺元, 又是右幺元. 若幺元存在, 那么其必然唯一: \\[ e_1 = e_1 e_2= e_2 \\] 直接推论: 若幺半群有多余一个左幺元, 那么其必然没有右幺元 (否则假设存在, 这个右幺元必然与某个左幺元相等. 又因为幺元唯一, 与有至少 \\(2\\) 个左幺元矛盾) 群 称幺半群 \\(\\left&lt;G,\\circ\\right&gt;\\) 为群当且仅当 \\(\\forall a\\in G, a^{-1}\\in G\\) 左右逆元 定义元素 \\(a\\in G\\) 的左逆元 \\(a_1\\) 为 \\[ \\exists a_1\\in G \\text{ s.t. } a_1a=e \\] 若左右逆元都存在, 那么它们相等且为 \\(a\\) 的 逆元 \\[ a_2 = (a_1 a) a_2 = a_1 a a_2 = a_1(a a_2) = a_1 \\] 逆元 \\(a\\in G\\) 的逆元定义为 \\[ \\exists a^{-1} \\text{ s.t. } a^{-1}a=aa^{-1}=e \\] 类似地, 若 \\(G\\) 中 \\(a\\) 有至少 \\(2\\) 个左逆, 那么其必然没有右逆. 消去律 右消去律定义为 \\[ \\forall a,b,c\\in G, \\; ac=bc\\Rightarrow a=b \\] 由逆元的存在性可知群满足消去律 群方程 如下形式的方程称为群方程, \\(x\\) 是未知元 \\[ \\begin{aligned} ax=b \\\\ xc=d \\end{aligned} \\] 由逆元存在性和唯一性可知群方程解存在且唯一. 群的等价定义 单侧定义 称半群 \\(G\\) 为群, 当且仅当 \\(\\exists e_L\\in G\\) 为左幺元 \\(\\forall a\\in G, \\exists a^{-1}\\text{ s.t. } a^{-1}a=e_L\\). 即任意元素存在左幺元 称为群的单侧定义, 等价性的证明需要一些技巧. 任取 \\(a\\in G\\), \\(a&#39;\\) 是 \\(a\\) 的左逆元, \\(a&#39;&#39;\\) 是 \\(a&#39;\\) 的左逆元 \\[ e_L=a&#39;&#39;a&#39;=a&#39;&#39;e_La&#39;=a&#39;&#39;(a&#39;a)a&#39;=(a&#39;&#39;a&#39;)aa&#39;=e_Laa&#39;=aa&#39;=e_L \\] 说明 \\(a&#39;\\) 亦是 \\(a\\) 的右逆元. 这个证明的关键在于添一个幺元, 且左幺元不在最右侧时效果等同于幺元. \\[ a=ae_L=a(a&#39;a)=(aa&#39;)a=e_La \\] 说明 \\(e_L\\) 亦是右幺元. 证明的另一个方向是显然的, 因此单侧定义与群的一般定义等价. 群方程有解定义 半群 \\(G\\) 中任意群方程有解, 则 \\(G\\) 为群. 任取 \\(a\\in G\\), 记方程 \\(ax=a\\) 的一个解为 \\(e\\), 下面证明 \\(e\\) 为 \\(G\\) 中一个右幺元. 任取 \\(b\\in G\\), 有 \\(aeb=(ae)b=ab\\) 此时构造群方程 \\(xa=b\\), 记一个解为 \\(f\\), 那么有 \\(fa=b\\) 因此 \\(be=fae=f(ae)=fa=b\\), 由 \\(b\\) 的任意性即得 \\(e\\) 为右幺元. 任取 \\(a\\in G\\), 方程 \\(ax=e\\) 的解即为 \\(a\\) 的右逆元. 由群的单侧定义可知 \\(G\\) 为群. 消去律定义 有限半群 \\(G\\) 中满足左右消去律, 则 \\(G\\) 为群 证明1 任取 \\(a\\in G\\), 定义 \\(Ga=\\Set{xa\\mid x\\in G}\\), 由右消去律可知 \\(|G|=|Ga|\\), 即 \\(G\\) 与 \\(Ga\\) 存在双射. 故存在 \\(e\\in G\\) 使得 \\(ea=a\\). 下证 \\(e\\) 是左幺元. 任取 \\(c\\in G\\), 由左消去律可知 \\(|aG|=|G|\\), 即存在 \\(d\\in G\\) 使得 \\(ad=c\\), 因此 \\(ec=ead=ad=c\\). 由 \\(c\\) 的任意性, \\(e\\) 是左幺元. 再证明左逆元的存在性. 类似存在 \\(a&#39;\\in G\\) 使得 \\(a&#39;a=e\\). 由 \\(a\\) 的任意性可知 \\(G\\) 存在左逆元. 由群的单侧定义可知 \\(G\\) 为群. 证明2 \\(G\\) 有限, 因此可写成 \\(G=\\Set{a_1,a_2\\ldots a_n}\\), 群方程有 \\(2n^2\\) 个, 形如 \\(a_ix=a_j\\), \\(xa_i=a_j\\) 由右消去律, \\(|G|=|Ga_i|\\), 因此必然存在唯一的 \\(a_x\\) 使得 \\(a_x a_i=a_j\\), 此时 \\(a_x\\) 即为方程 \\(xa_i=a_j\\) 的解. 同理可得方程 \\(a_i x=a_j\\) 存在唯一解 故 \\(G\\) 中任意群方程存在解, 由群方程有解定义可知 \\(G\\) 为群. 阶 元素 \\(a\\in G\\) 的阶定义为最小的自然数 \\(n\\in\\mathbb N\\) 使得 \\(a^n=e\\), 记为 \\(o(a)\\) 或 \\(\\left\\lvert a\\right\\rvert\\) \\(\\left\\lvert a\\right\\rvert=1\\) 当且仅当 \\(a=e\\), 并且有 \\(\\left\\lvert a\\right\\rvert=\\left\\lvert a^{-1}\\right\\rvert\\) 定理1 \\(a^k=e\\) 当且仅当 \\(o(a)\\mid k\\). 由反证法, 设根据带余除法得 \\(k=q\\cdot o(a)+r\\), \\(r eq 0, r&lt;o(a)\\), 那么有 \\[ a^k=a^{q\\cdot o(a)+r}=a^r=e \\] 这与 \\(o(a)\\) 的定义矛盾, 故 \\(r=0\\). 定理2 记 \\((a,b)\\) 为 \\(a, b\\) 最大公约数 \\[ o(a^k)=\\frac{o(a)}{(k,o(a))} \\] 先证明 \\(o(a^k)\\leq \\dfrac{o(a)}{(k,o(a))}\\) : 注意到 \\[ \\left(a^k\\right)^{\\frac{o(a)}{(k,o(a))}}=\\left(a^{o(a)}\\right)^{\\frac{k}{(k,o(a))}}=e \\] 根据定理1立得 \\(o(a^k)\\le \\dfrac{o(a)}{(k,o(a))}\\) 再证明 \\(o(a^k)\\geq \\dfrac{o(a)}{(k,o(a))}\\) : 注意到 \\[ \\left(a^k\\right)^{o(a^k)}=a^{k\\cdot o(a^k)}=e \\] 根据定理1立得 \\(o(a)\\mid k\\cdot o(a^k)\\). 又因为 \\(k\\cdot o(a^k)\\) 同时是 \\(k\\) 的倍数, 所以 \\(k\\cdot o(a^k)\\) 是 \\(o(a), k\\) 的公倍数. 而 \\(\\dfrac{k\\cdot o(a)}{(k, o(a))}\\) 是 \\(o(a), k\\) 的最小公倍数, 因此 \\(k\\cdot o(a^k)\\geq \\dfrac{k\\cdot o(a)}{(k, o(a))}\\) , 化简即得证. 因此 \\(o(a^k)=\\dfrac{k}{(k, o(a))}\\) 定理3 若 \\(a,b\\in G\\) 可交换 (即 \\(ab=ba\\)) 且 \\(o(a),o(b)\\in \\mathbb N\\), 那么 \\[ \\dfrac{nm}{d^2}\\mid o(ab) \\\\ o(ab)\\mid\\dfrac{nm}{d} \\] 方便起见, 记 \\(m=o(a), n=o(b), d=(m, n)\\) 首先证明 \\(o(ab)\\mid \\dfrac{mn}{d}\\) : \\[ \\left(ab\\right)^{\\frac{mn}{d}}=\\left(a^m\\right)^{\\frac{n}{d}}\\left(b^n\\right)^{\\frac{m}{d}}=e \\] 再证明 \\(\\dfrac{mn}{d^2}\\mid o(ab)\\). 方便起见可以分别证明 \\(m\\mid n\\cdot o(ab)\\), \\(n\\mid m\\cdot o(ab)\\). \\[ \\begin{aligned} \\left(ab\\right)^{n\\cdot o(ab)}&amp;=\\left[\\left(ab\\right)^{o(ab)}\\right]^{n}=e \\\\ \\left(ab\\right)^{n\\cdot o(ab)}&amp;=a^{n\\cdot o(ab)}\\left(b^n\\right)^{o(ab)}=a^{n\\cdot o(ab)}=e \\end{aligned} \\] 这说明 \\(m\\mid n\\cdot o(ab)\\), 即 \\(\\frac{m}{d}\\mid \\frac{n}{d}\\cdot o(ab)\\). 注意到 \\(\\frac{m}{d},\\frac{n}{d}\\) 互质, 因此 \\(\\frac{m}{d}\\mid o(ab)\\) 同理可得 \\(\\frac{n}{d}\\mid o(ab)\\), 又因为互质, 所以 \\(\\dfrac{nm}{d^2}\\mid o(ab)\\). 特殊地, 若 \\((o(a),o(b))=1\\), 则有 \\(o(ab)=o(a)o(b)\\) 这里一个反例是 \\(o(a)=o(a^{-1})=n\\), 但是 \\(o(aa^{-1})=o(e)=1 eq lcm(o(a),o(a^{-1}))=n\\)","tags":["Algebra"]},{"title":"操作系统03 内存管理","path":"/2022/07/04/OS03-内存管理/","content":"虚拟地址 通过地址翻译, 可以利用统一的虚拟地址空间来为用户程序提供内存, 同时还能隔离不同进程增强安全性 基于分页的地址翻译涉及到页表和radix-tree这种数据结构, 以及 TLB 这样的部件, 具体看ICS/CSAPP 虚拟地址的引入也可以让进程间共享数据, 只需要将某段物理内存同时映射给两个不同的进程即可. 当然per-process的地址空间带来了进程切换时的cache问题: 需要冲刷TLB, 预热缓存等等. 解决方法很多, 可以给 TLB 引入 ASID 使得不同地址空间的同一虚拟地址可以同时被缓存(会有安全问题吗?), 顺便增强 TLB 冲刷的粒度控制. 缺页异常 把内存视为硬盘的cache, 那么就会出现访问虚拟地址时cache miss的情况. 这时就产生了缺页异常, 需要进exception handler从硬盘上读出被缓存的页 这里还有很多可说的, 例如可以把不常用数据压缩以节省内存, 压缩后的数据也能更快在硬盘上读写. 换页策略 FIFO, 每次换出最早缓存的页 Second-Chance, 在FIFO的基础上给每个活跃的页一次机会(如果FIFO队尾曾被访问过, 就移回队头) LRU, 算频率, 通常通过定期打标记来判断近一段时间是否活跃 MRU, 在某些场合有用, 例如假设播放过的电影片段不会再立刻放一次 理论上最优的策略是换出在未来最晚被访问的页 大页 可以设置页的大小以减少TLB miss次数, 同时(在相同总内存下)降低页表的层数 物理内存管理 操作系统需要管理物理内存, 评价维度包括效率 利用率和Scalability 利用率 外部碎片: 未分配且无法利用的部分 内部碎片: 已分配但用不到的部分 感觉这两种划分得不是很有必要... Buddy-System 只能分配页面的 \\(2^k\\) 倍大小的物理内存. 每次找到能容纳需求的最小块, 将其分裂成相等的两半直至恰好能容纳需求, 取走一块. 本质上是个线段树, 但是可以用若干个独立的链表串联相同大小的块(也即写成非递归的线段树) 若只考虑分配不考虑回收，那么任意时刻块的总数量不会超过 \\(O(\\log n)\\) 个. 考虑 \\(n\\) 位二进制数字 \\(B_n=10\\ldots0\\), 其中第 \\(i\\) 位为 \\(1\\) 表示有一个大小为 \\(2^i\\) 的块 每次划分出大小为 \\(2^k\\) 的块等价于计算 \\(B_n&#39;=B_n - B_k\\), 其中 \\(B_k=0\\ldots010\\ldots0\\), 第 \\(k\\) 位为 \\(1\\) 每次分裂等价于二进制向高位借位, 利用均摊分析可以轻松得到比特翻转操作的上界, 根据位数立即可得任意时刻块数量的上界 实际上任意时刻, 特定大小的块至多有一个 freelist 因为空闲内存是空的, 因此可以任意摆布. 一种玩法就是用链表的形式串联起来, 这样每次的分配就是遍历链表-删除节点 SLAB 关键在于, 大块分配次数少且生存周期比较长, 小块分配频繁释放也频繁. 利用这一点可以结合 freelist + Buddy-System 来实现小块本地取 + 大块上锁统一分配。 利用Buddy-System分配大块内存, 然后划分成相等的若干小块, 这样小块内存分配直接从 SLAB 中取走, 几乎是 \\(O(1)\\) 的, 多核也有很好的表现. 一个小细节是可以把元数据放在 SLAB 的头部并保证 SLAB 按页面对齐, 这样释放的时候就可以快速查询元数据了.","tags":["Operating System"]},{"title":"操作系统02 并发","path":"/2022/07/03/OS02-并发/","content":"并发 如果两件事情在逻辑上同时发生了, 那么我们就说它们并发了. 在Linux上很容易用POSIX API创建新的执行流(线程), 可以利用 strace top man 等工具观察具体创建的原理 跑起来的效果和参数含义 并发Bug 并发带来的问题是指令原子性的破坏. 例如语句 a = a + 1; 可能会被编译为多条指令, 而中断到来的时刻是任意的, 甚至多条指令是并行执行的(显然在逻辑上并发) 原子性是重要的多线程程序性质, 一般的库函数都能看到 MT-Safe 这样的描述. 互斥 为了保证一段代码的原子性, 引入互斥锁的概念. 经典的互斥锁利用硬件提供的原子操作机制可以实现目标代码的串行化(所有线程至多一个在当前时刻执行这段代码) 被原子化的代码叫做 Critical Section 编译优化 GCC 的编译优化是基于单线程假设的, 因此在多线程下可能会行为不一致(回忆volatile) 甚至会出现指令的重排(编译器级别)导致的原子性丧失/语义不一致. 然而CPU也是动态多发射的, 类似的指令乱序仍然可能发生. 这使得某些关于指令顺序的假设会改变(翻阅TSO内存模型). 注意到内存的读/写相对运算指令而言是副作用指令, 因此这也意味着两段程序的 observable behaviour 会与顺序执行的单线程程序不一样. 具体如何需要看 ISA 手册. 软件和硬件支持 编译器指令 barrier, 硬件指令 fence 都可以一定程度上保证可观测行为与预期一致. Model Checking 为了说明并发程序的正确性(证明某段程序的性质) 可以使用 Model Checking 工具来建模, 本质是画状态机+设计谓词. 锁 用户程序眼中的锁就是两个有特殊语义的API typedef struct &#123; ... &#125; lock_t; void lock(lock_t *lk); void unlock(lock_t *lk); 对于同一把锁 lk, 多个调用 lock(&amp;lk) 的线程只能有一个返回, 其余都将等待直至持有锁的线程调用 unlock(&amp;lk). 硬件支持 利用 x86 提供的 lock xchg 指令即可原子地读和写 risc-v 则是提供了 load-reserved/store-conditional 的操作. 注意到一次原子操作可以拆解成load-exec-store三个步骤, risc-v 认为只有 load/store 是重要的. 每次 load 完都将对内存地址打上标记, 所有修改/访问了改地址的操作都将清除标记. 若 store 时标记仍然留着(说明没有别的操作修改过这个地址) 那么直接修改, 否则重新load-exec-store 关中断 对于单线程系统而言, 只需要关掉中断即可终止所有的并发执行流. 然而并非所有中断都可以忽略, 并且长时间关中断将失去中断本身带来的好处(IO性能) 自旋锁 有了原子指令就可以原子地 compare_and_swap() 来实现互斥锁了, 所有没得到锁的线程都将 while(1) 等待(即自旋) 优点是简单, 缺点: 浪费 CPU 获得锁的线程可能被操作系统休眠(即最trivial的实现中, 操作系统无法感知锁的持有) 这就引入了Scalability的概念, 即我们希望算法的效率能够随着硬件的增加而增加(至少不会减少). 很显然自旋锁的 Scalability 不是太好 睡眠锁 为了让操作系统能感知锁的持有, 可以把锁的两个API换成两个系统调用, 对于不成功的lock()直接睡眠, unlock() 则唤醒等待当前锁的线程. 需要注意操作系统自身需要用自旋锁来保证共享数据结构操作的原子性. 实际上就是把锁集中管理起来. Futex 先自旋, 再睡眠(我全都要) 条件变量 支持两种操作的抽象数据类型: wait() 和 signal() 可以看成是一个管道(广播), wait() 操作将本线程挂起直至条件变量被广播. signal() 则会向所有等待着该条件变量的线程广播并唤醒 这是一个共享的数据结构, 很显然要锁保护(RTFM) 信号量 支持两种操作的抽象数据类型: P() 和 V()(也叫 wait() 和 signal()) 可以看成一个容器, 所有的 wait() 都将先检查是否有空余, 然后进入临界区并占有容器中的一个位置. 当信号量被初始化为 1 时, 这就是一个互斥锁了. 应对并发Bug 防御性措施 多加Assertion LockOrdering 不同的获取锁的顺序将带来死锁, 可以给锁定一个acquire顺序 但事实上很难实现这一点, 因为很多锁并不是透明可见的... Lockdep 这样的工具可以观察锁的执行序列来判断是否会出现顺序问题(Debug工具) 读写锁 单纯的一把大锁很难实现高效, 因此考虑如下读写模型: 某个共享区有数据, 有若干线程(读和写)将进入操作共享数据 多个读线程在没有写线程时可以同时进入 任何时刻, 至多一个写线程可以写 这样可以保证读线程的并行, 但不恰当的处理将导致写线程饥饿 RCU机制 针对单链表, Read-Copy-Update 机制可以实现读和写的并行化, 避免了写线程的饥饿 本质是观察到单链表的插入和删除都只涉及至多一个指针的修改, 那么写线程可以预先写好内容, 然后原子地修改一个指针 这里巧妙之处在于把确认没有其余线程正在访问旧的数据延迟到了这些线程离开单链表. 注意到所有写线程修改之后进入单链表的读线程都将读到新的内容, 因此只需要等待所有在修改之前进入单链表的读线程全部离开单链表时即可回收旧的节点.","tags":["Operating System"]},{"title":"操作系统01 导引","path":"/2022/07/03/OS01-导引/","content":"写在前面 本科期间最期待的一门课, 就这么结束了有些可惜, 决心回头看看slides, 结合交大的教材再看看有没有什么可以新发掘一下的点. 现在看来是因为拔尖班的实操课程实在太少, 这门课承担了太多夯实写代码能力的责任... 什么是操作系统 本质是一个程序, 只不过这个程序需要 向上提供抽象的接口以供用户程序使用硬件 向下隐藏硬件的细节 管理任务 分配资源 显然操作系统并非一开始就存在, 它的形态和功能都是随着软硬件发展和需求发生变化的. 什么是程序 程序描述了一个运行时的状态机, 我们说操作系统管理程序的含义就是管理这些状态机(的状态) 为什么需要操作系统 希望有一个特殊的程序来执行\"脏活\", 把纯粹的计算留给普通程序, 产生副作用的操作交由这个特殊程序来完成. 如果知道 IO monad 的话, 大致可以想象成操作系统是若干个 IO(State), 所有的纯计算都是 monad, 而只有操作系统可以对带来副作用的部分求值. 类似的还有安全方面的考虑, 所有需要获取高特权级的操作都必须经由操作系统之手. 什么是编译 两个程序等价当且仅当它们所有的 observable behaviour 一致. 又因为纯粹的计算无法产生副作用(因而无法被观测到结果), 因此编译的等价性需要依靠产生副作用的那些指令来定义. 对!就是syscall. 一个极端的例子是 for (int i = 0; i &lt; 1000; ++ i) &#123; int g = 0; &#125; 这段代码与nop无异. strace 要会用strace看syscall trace(程序的 observable behaviour)","tags":["Operating System"]},{"title":"ANTLR4 笔记","path":"/2022/07/02/ANTLR4-笔记/","content":"写在前面 最近需要用到这个工具，官方推荐的Definitive ANTLR Book写得比较杂乱，于是想记一下比较有用的一些点 meta language Parser和Lexer 一条rule形如 `rule_name` : `product 1` | `product 2` | ... ; 若rule_name以小写字母开头则为Parser规则，否则为Lexer规则 存在二义性的时候ANTLR默认选择最先出现的Rule推导 fragment 为了不在Parser中引入新的token name但是又要用一个名称来指代一些符号以简化书写（类似macro expansion） NUM: DIGIT | DIGIT_NONZ DIGIT*; fragment DIGIT: [0-9]; DIGIT_NONZ: [1-9]; assoc 指定符号的结合性（默认左结合） expr: expr &#39;^&#39;&lt;assoc=right&gt; expr; RuleName 一对多的产生式，可以标记上名字以在用Parser Visitor的时候可以单独处理每一种语法推导的情况 expr: NUM # Number | expr &#39;*&#39; expr # Mult | expr &#39;+&#39; expr # Add ; 这个时候gen/下的Parser代码就会区分出NumberContext、MultContext和AddContext以及对应的visit方法 Member 可以在.g4文件里给Parser类声明field或method @members &#123; int count = 0; void helperMethod() &#123; &#125; &#125; 后续就可以继承生成的Parser类然后覆写这些方法了... return value 这个主要和visitor、listener的模式有关系 为了给树上的节点加上注解field，可以这么写 expr returns [int value] : expr &#39;*&#39; expr # Mult | expr &#39;+&#39; expr # Add | NUM # Number ; 此时MultContext会继承exprContext，而exprContext则会被ANTLR加上int value这个field"},{"title":"操作系统 Lab3 uproc","path":"/2022/06/23/OSLab3-uproc/","content":"Lab3-uproc 设计 在 L3 的时候实现了侵入式链表，这样就可以把原先 pmm 的 free_list、kmt 的 task_list、handler_list、sem_t 的 wait_list 都统一起来。 对进程（任务）操作封装了单独的 API，包括创建、删除、拷贝、页操作等等。 把任务的地址空间抽象为映射的集合，物理页封装出 page_t 来储存物理地址、引用计数和访问权限。 kill() 的设计参考了 xv6 的代码，即只设置 kill 标志位，下一次该进程被调用就直接执行 exit() 系统调用。注意到信号量和自旋锁都只在内核空间，因此当这个 victim 进程刚刚进入 os_trap() 时不会持有锁。 注意到实验简化了僵尸进程的处理，因此我给每个任务保留了一个field用于回收其任意子进程的返回值，不同的子进程在exit()时会往其父进程的field里写入返回值，这样父进程就可以异步获得返回值了。 印象深刻的Bug 主要是 fork() 相关。 需要拷贝内核栈，并且亲子进程的 ctx 被我实现为指针了，那么就应当指向各自栈上相同的相对位置（而非直接复制指针）。 L1-pmm 在实现的时候写了一些针对 OJ 的参数化代码，在本地就直接挂掉了（例如我会判断一下 CPU 的数量和空间大小来预留多少堆区空间给线段树的节点结构体，以区分 hard test 和 easy test）。具体表现为会分配超出堆区的内存作为某些进程的内核栈，然后这些地址就会被 init 线程的栈覆写，使得 fork() 之后得到的两个进程在内核栈上存在差异。 接上条。因为要支持系统调用开中断，所以我在实现 fork() 的时候思考过“在自己的内核栈上赋值自己的内核栈”这样看起来很扯淡的问题。事实上只有第一次 os_trap() 时的内核栈指针以前的栈内容有意义，因为那才是我们希望精确赋值的进程状态。之后的部分也许会因为开中断而被写入其他东西，但是这些部分都不需要被子进程复制。因此在自己的内核栈上操作，也只会弄脏不需要的部分，这就不成问题了。 系统调用要求开中断。期初我用一个 per-CPU 的数据结构来维护上下文指针，那么就会发生这样的情况： 用户进程A -&gt; os_trap(): 系统调用自陷 os_trap() -&gt; do_syscall(): 识别、执行 do_syscall() -&gt; os_trap(): 时钟中断 os_trap() -&gt; 用户进程B: 调度切换 用户进程B -&gt; os_trap(): 时钟中断 os_trap() -&gt; do_syscall(): 调度切换 此时经历了两次 os_trap()，因此这个 per-CPU 的数据结构（其实就是一个指针）就被覆盖成了第二次 os_trap() 的栈上的上下文，这点我也写在了注释里。 解决方法也很粗暴，注意到一个 CPU 至多嵌套两次 os_trap()（首先至少可以两次，其次第二次必然不会是系统调用，因此第二次必然关中断），那么就只需要多一个 task_t *fork_ctx 来指示需要 fork() 的进程第一次 os_trap() 的时候的上下文即可。 在实现 exit(int *status) 的时候，status 是一个指向虚拟地址的指针，因此需要先做一次地址翻译再写入内容。","tags":["Operating System"]},{"title":"大二下摸鱼记录","path":"/2022/06/20/大二下摸鱼记录/","content":"大二下摸鱼记录 感觉已经失去了初入大学的那种锐气。越学越觉得自己不会的还多着，想做的东西总要被迫让位给GPA和考试。某些课程越发觉得无用，上课的老师也只是混混日子，我也就渐渐只会混混日子了。 二月 在寒假的时候看完了ML4CS的大半本，顺便又看了一次冰菓。最开始看冰菓的时候并没有特别的感觉，只对万人的死角里的idea感到有些惊艳。等到我的高中生活离我远去了才觉着有趣，也是蛮新奇的体验吧。 在湖南待着的时候只觉得自己是异乡人。熟悉的亲人都在老去，我的同龄人都与我陌生了。走亲戚的时候只能尴尬笑一下。我承认这种感觉很不好，但这是我出生的地方，这大概算一点责任。 开学之后选课，思考了半天还是选上了OS和数电，免掉了算法课。说实话这么做我是心里没有底的，但是实在太想上jyy的课了。在呢喃读书四年，我不想临近毕业再去体验号称是最难作业的oslab。但事实上作业也没有想象的那么难，并且一轮下来发现讲课的风格也并没有那么喜欢...还是有点落差的。 二月份月底的时候和Charlie Jiang去了一趟新街口吃网红快餐，最后因为找不到座位打包去了另一家快餐店吃。Jiang同学是非常有个性的人，且不说坐1h地铁来回吃网红店这个行为，光是提着网红店的包装盒坐地铁就让我难以忍受别人的目光了。不过能够有这么一次体验也算是新奇。 三月 在上大学之前就加入了526，算是社交生活的起点了。本来的教室被征用了，于是只好拖家带口搬到了楼下的411临时凑合。在526刷过丘维声的网课和教材，拆过破碎的智商税笔记本，玩过舞力全开，吹过口琴打过UNO，现在都成回忆了。 大概在三月底开始封校了，封校前和znr去了一趟玄武湖，结果最终也没有成功打卡相亲角，有点遗憾。 今年没有去鸡鸣寺。 四月 封校的日子里，炜华变得很热闹。每晚都有唱歌、广场舞、夜跑、桌游，甚至还有调酒和蹦迪的，可以说是很快乐了。 526的一支成立了桌游分队，在炜华进行了第一次（大概也是最后一次）大型线下桌游活动。挺有意思的。看样子本来是个情感聊天局，最后变成了词穷telepath了 中途一直在咕咕咕oslab，因为L1实在写得太费心...听OS的感觉就是台上这个人看起来好厉害但是这和我有什么关系吗 的心情。OS的很大一部分都在PA4整完了，但是高阶内容又不可能一下子干完，所以就听得有点累，每次上完都要花一整晚看教材，再花另一整晚去看代码和翻wiki，感觉不是太值得。 五月 花了三天就写完了oslab2，可以吹一年 校庆整了一个小小的摆摊，社团的活动仍然是老三样：认识网线，接网线，测网线。在最热闹的一天里也没有见到那个人，大概是再也见不到了。 520晚学校有看起来很屌的晚会和无人机表演，可惜门票限量发放。炜华里只放了1200人，剩下的都在教学楼的窗边、铁栏杆外围、图书馆前的大屏幕看表演，说实话不太人道。 晚上和znr还有lhr喝电气白兰，感觉不怎么好喝。顺便听来了几个小故事，还行吧。 后面还有校园打卡的活动，最后一站是天文台。上去了只觉得空虚，大热天也许不该走上天文山就为了拍一张照片，更不应该在天文台瘫坐着，于是就编了个理由回宿舍了。 月底去看了Groundhog Day的音乐剧。这部电影本来在waitlist里，现在也不太想再看一次了。歌魅还是很厉害的，动作和台词都很到位，场景道具精美。可惜的是因为莫名其妙的疫情管控原因，倒数第三排的我被赶到了二楼。这个角度不好又吹不到空调的地方着实不算好位置。第二场就更惨了，多余的同学直接被挡在了门外，事后也没有道歉和解释，差评 六月 月初和桌游组出门唱歌，感觉大家虽然风格迥异但还是能玩得和谐的。 突然萌生了留长发的想法，在这之后越来越发觉周围的长发男变多了。 准备期末考试，然后考到了今天。 课程踩坑 交换生系统里的ranking日常下掉，感觉自己是真的小丑 操作系统 刚入学就听说很牛逼的一门课，事实上也很牛逼。这门课基本上都是jyy在单打独斗，所以能有完整的实验体验、丰富的拓展和充分的代码例子是很难得的。 实验是基于AM写的，因此屏蔽了很多ISA相关的细节（例如怎么做地址空间的虚拟化）。和大班最大的不同就是有完整的多核心支持，可以轻松写出并发bug，以及L1对性能有一些些要求，别的就没了。 理论都是些说烂了的东西，难的是怎么实现一个理论。再加上这门课提供了完整的课程录像，因此我觉得完全可以旁听然后做完整的实验，这样体验是相差无几的。 算法设计与分析 4.65 为了修OS就免修了算法，事实证明这一决定非常正确。算法课只会讲一些经典的简单算法，难的分析和论证老师自己也不太行...由于是三个班一起上，某些老师的水平就非常不够看，连带的助教也很睿智（无褒义），oj的题目非常迷惑，同学们也不知道到底学懂没有。 最后考试是两个原题+四个莫名其妙的设计和分析，1.5h就写完跑路了。就这还有人作弊... upd: 感觉分数没有预期的高，但是无所谓了 图论 4.6 莫名其妙的本研共修课，实际上全是本科生。作业难度一般，属于思考两天就能全部写完的水平。考试莫名其妙，要你模拟算法，证明题纯纯的送分，让我摸不着头脑。 最莫名其妙的是这课还搞回答问题加分 &amp; 论文分享加分 &amp; OJ附加题加分 &amp; 签到算分 的申必碎片化给分，有点无语。 但这是你基必修，没法。 计算方法 4.95 可以感觉到新来的老师水平很高，也在努力想让大家学懂。不过看起来课程内容和一开始以为的计算方法不太一样，中途开始插入随机游走和谱图论的硬广。最后让yyt讲了两次的凸优化（感觉也没讲什么...） 平时作业不算难，两周交一次并且和课程内容比较相关。助教人很好（也许是因为和ofn关系很好...）会有答疑和参考答案。 期末考了很多计算题和线性规划，几道证明都是比较简单的结论应用，还是很友善的。 数理逻辑 4.95 非常纱布的课程。首先数理逻辑是一门非常严谨的符号学科，但是学校用的课本错漏百出，甚至需要专门出一个勘误。其次写这个课本的人已经不教计科了，因此这本写得并不完整有些跳跃的课本看懂很费力气。最后目前负责上这门课的老师水平很低，具体表现为不会念定理的名字、不理解定理的含义就上课。考试甚至出现了抄原题抄错定义的情况，并且事后拒绝承认自己造成的教学事故。 期末只需要提前找好往年题目开抄就行了，80%是有重合的。 综上所述，这门课非常对不起它的历史地位和重要性，我的建议是能跑就跑，实在跑不了也不要跟着qy上。 upd: 这个得分属实惊到我了，但我还是希望能看到秦老师悔恨的泪水 博弈论 4.8 意义不明的选修课。如果你从来没接触过博弈论，那么不妨一上；如果你之前见过类似的东西，那么可能这门课没法带来新知识。课程名叫博弈论及其应用，实际上更适合叫纳什均衡及其计算。 很多时候课程的内容都是要计算一个纳什均衡，然后就是针对不同的模型做不同的计算和建模，更像是金融会感兴趣的东西。考试不算难，基本没有什么障碍。中途一次的大作业做起来有些费力，需要学一些额外的知识才能做得比较好。最重要的，这门课在周五的晚上，比较折磨。 属于不太值得专门来选，但是有闲可以上一下的选修课。 数电计组 4.45 刚刚考完这门 课程内容没什么新东西，我之前上过ICS和OS了，这学期还做了数电实验，所以上起来没什么感觉...去年的时候同学普遍反馈学不懂，今年的学弟学妹反应也类似，感觉更像是课程安排和老师授课的问题。 实验很无聊，需要费眼睛硬连线，不过最后跑起来还是挺有成就感的。 考试似乎没啥难的，除了连线有点虚以外没什么槽点...等出分看看吧 数电实验 4.85 主要是写verilog，需要有一点debug的耐心和配环境的耐心（如果你恰好也是linux+AMD选手的话要忍受额外的折磨） 最后的大实验主要靠队友调好了基本的IO（主要是键盘和VGA）我才得以开始干活。最后花了点时间写了个可有可无的2段流水CPU，也算是差不多了。 最后给分还很不错，看来重修班确实不错。 太极拳 4.35 据说是这位老师的最后一年授课，感觉没什么参考价值。 这学期整整上了两个月网课，因此中途都没有线下体育。并且最后只考察四个基本动作，还是没什么压力的。 杂项 选了一门网课，还算行吧，以后都没了。 选了一门数电实验，主要是最后一个实验比较折磨人，别的都还挺好玩。","tags":["Eureka Moments"]},{"title":"计算方法07 电阻网络","path":"/2022/06/15/Numerical07-电阻网络/","content":"前置 前置的物理定律包括如下两条： 欧姆定律，即 \\(\\phi_i-\\phi_j=U_{i,j}=I_{i,j}R\\)。定义电导率为 \\(w=R^{-1}\\)，那么有 \\(I_{i,j}=U_{i,j}w\\)。 基尔霍夫定律，即对于电阻网络的任意节点 \\(v\\)，流入的电流等于流出的电流 。 电路网络与 \\(L\\) 对于单位电阻组成的电路网络 \\(G\\)，其拉普拉斯矩阵 \\(L\\) 可以通过上面两条物理定律和电流联系起来。 考虑电路网络内部的节点 \\(x\\)，根据基尔霍夫定律有 \\[\\begin{aligned} b_x=\\sum_{y\\in N(x)} I_{x,y}=\\sum_{y\\in N(x)} (\\phi_x-\\phi_y)w \\end{aligned}\\] 为了方便讨论，一般会规定电源电势为 \\(1V\\)，或流入电路网络的电流总量为 \\(1A\\)，此处采用第二个约定。 对于单位电阻有 \\(w=1\\)，此时上述方程组可以写成 \\[\\begin{aligned} L\\phi=b \\end{aligned}\\] 其中 \\(b_x\\) 表示流入 \\(x\\) 的电流（流入为正，流出为负）。将电源接在图上任意两点间（不妨设为 \\(s,tttt\\)），其含义为向量 \\(b\\) 满足 \\(b_s=1,b_t=-1\\)，其余均为 \\(0\\)。 再考虑欧姆定律，有 \\[\\begin{aligned} B^\\intercal \\phi=I \\end{aligned}\\] 其中 \\(B\\) 为图 \\(G\\) 的 \\(n\\times m\\) 关联矩阵，形如 \\(\\begin{bmatrix}\\cdots&amp;\\cdots&amp;\\cdots \\\\ \\cdots&amp; 1&amp; \\cdots \\\\ \\cdots&amp;\\cdots&amp;\\cdots \\\\ \\cdots&amp; -1&amp; \\cdots\\\\\\cdots&amp;\\cdots&amp;\\cdots\\end{bmatrix}\\)，第 \\(i\\) 列表示边 \\(e_i\\) 关联哪两条边，正负表示方向。\\(m\\) 维向量 \\(I\\) 表示每条边上电流的流量。 如果要考虑非单位电阻的矩阵，那么需要引入 \\(m\\times m\\) 的对角阵 \\(W=\\text{diag}\\set{w_{e_1},w_{e_2}\\ldots w_{e_m} }\\) 来分别建模每条边的电导率，在需要的时候乘上就行了。 考虑 \\(L\\) 的另一形式有 \\[\\begin{aligned} L=\\sum_{e\\in E(G)} L_e=\\sum_{e\\in E(G)} w_e b_e{b_e}^\\intercal=BWB^\\intercal \\end{aligned}\\] 因此还可以写成 \\[\\begin{aligned} L\\phi=BWB^\\intercal \\phi=BWI=b \\end{aligned}\\] 这也是很直观的，对边上的电流分布做一次图上的按邻居求和，就能得到一个节点上的全局电流分布 \\(b\\)。 电路方程的解 定理： 若 \\(L\\phi=b\\) 有解当且仅当 \\(b\\perp \\textbf1\\) \"\\(\\Rightarrow\\)\" 注意到 \\(L\\) 实对称，取一组由 \\(\\textbf1\\) 扩充而来的正交基 \\(\\Set{v_i}\\)，则 \\(\\phi=a_1\\textbf1 + \\sum_{i=2}^n a_i v_i\\) 此时 \\(L\\phi=L\\left(a_1\\textbf1+\\sum_{i=2}^n a_iv_i\\right)=\\sum_{i=2}^n a_i\\lambda_i v_i\\)，根据正交基可知 \\(L\\phi\\perp b\\) 直观含义为电阻网络流入的电流要等于流出的电流。 \"\\(\\Leftarrow\\)\" 若 \\(b\\perp\\textbf1\\)，那么 \\(b=\\sum_{i=2}^n b_iv_i\\) 此时取 \\(\\phi=\\sum_{i=2}^n \\frac{b_i}{\\lambda_i}v_i\\) 即为一个解。 直观含义为对于一组外部电流的电势解，可以任意整体平移得到同方程的其余解（因为电流只和电势差有关）。在固定某个点电势为 \\(0\\) 的前提下，就能得到唯一解。 上面关于 \"\\(\\Leftarrow\\)\" 方向的证明用到了一个构造，实际上可以写成 \\[\\begin{aligned} \\phi^*=L^\\dagger b \\end{aligned}\\] 其中 \\[\\begin{aligned} \\begin{aligned} L^\\dagger&amp;=\\sum_{i=2}^n {\\lambda_i}^{-1}v_i{v_i}^\\intercal \\\\ b&amp;=\\sum_{i=2}^n {\\lambda_i} v_i \\end{aligned} \\end{aligned}\\] 这意味着，当 \\(b\\) 是一个合法的电流（满足 \\(b\\perp \\textbf1\\)）时，\\(L\\) 存在伪逆。并且 \\(L\\phi=b\\) 的解集为 \\(\\Set{L^\\dagger b + k\\textbf1\\mid k\\in \\mathbb R}\\) 电势能和等效电阻 同样只考虑单位电阻。 考虑令 \\(b\\) 流入单位电流，电路网络为单位电阻，那么整个电路的等效电阻就是 \\(s,t\\) 间的电势差，即 \\[\\begin{aligned} R_{\\text{eff} }=\\phi_s-\\phi_t=b^\\intercal\\phi=b^\\intercal L^\\dagger b \\end{aligned}\\] 对于电势能同样可以通过等效电阻来算。注意到通的是单位电流，并且电阻为 \\(R_{\\text{eff} }\\)，因此电势能就是 \\(R_{\\text{eff} }\\)。 另一种对每条边单独推导的方法如下： \\[\\begin{aligned} E=\\sum_{e\\in E(G)} E_e=\\sum_{(x,y)\\in E(G)} {\\left({\\phi_x-\\phi_y}\\right)}^2 \\end{aligned}\\] 回忆关于 \\(L\\) 的二次型，有 \\[\\begin{aligned} E=\\phi^\\intercal L\\phi=R_{\\text{eff} } \\end{aligned}\\] 并且有结论：对与任意的 \\(s,t\\) 流，其电势能不会比 \\(R_{\\text{eff} }\\) 更小。即这样的电势分布会最小化单位流的能量损耗，非常神奇的物理意义。","tags":["Numerical Method"]},{"title":"计算方法06 FFT","path":"/2022/06/14/Numerical06-FFT/","content":"多项式的表示 最熟悉的是系数表示法，例如 \\(P(x)=\\sum_{i=0}^n a_i x^i\\) 系数表示法有它的好处：例如将 \\(\\Set{1,x,x^2\\ldots}\\) 视为一组基时，系数恰好就是多项式在这组基下的坐标；例如对于任给的点 \\(x\\) 可以计算多项式的值（\\(O(n)\\) 次乘法）。但是在给出系数表示法后，对两个 \\(n\\) 次多项式很难高效算乘积（还记得天才本科生Karatsuba吗） 还可以是点值表示法。根据lagrange插值可知，一个 \\(n\\) 次多项式可以由 \\(n+1\\) 个点唯一确定。因此 \\(P(x)\\overset{\\text{def} }= \\Set{(x_0,y_0),(x_1,y_1)\\ldots}\\) 此时计算 \\(P(x)Q(x)\\) 只需要求出 \\(2n+1\\) 个点值，然后将这些取值对应相乘即可得到 \\(R(x)=P(x)Q(x)\\) 的点值，也就唯一确定了 \\(R(x)\\) 因此我们希望有一种方法，对于给定的系数表示多项式 \\(F(x),G(x)\\)，能够选取 \\(2n+1\\) 个点 对这 \\(2n+1\\) 个点做插值后得到 \\(F^*(x),G^*(x)\\) 的点值表示，相乘最后还原得到 \\(F(x)G(x)\\) 的系数表示。 这就是FFT FFT DFT 不妨假设多项式是 \\(n=2^k\\) 次的 考虑方程 \\(x^n=1\\) 的所有复数解，它们构成了复平面单位圆上等距分布的 \\(n\\) 个点，记为 \\(\\Set{\\omega_i\\mid \\omega_i^n=1}\\) 选择这些点出自如下考虑：取 \\(\\Omega_n\\)，那么有 \\({\\Omega_{2n} }^2\\overset{\\text{def} }=\\Set{x^2\\mid x\\in \\Omega_{2n} }=\\Omega_n\\)。这暗示我们可以通过类似分治的算法来每次将问题的规模减小一半。并且对于 \\(\\Omega_n\\) 而言，只需要其生成元 \\(\\omega_1=e^{\\frac{2\\pi}{n+1}i}\\) 即可完全表示。 假设需要对 \\(F(x)=\\sum_{i=0}^{2n} a_ix^i\\) 插值，那么记 \\(G(x)=\\sum_{i=0}^{n} a_{2i}x^{i}\\)，\\(H(x)=\\sum_{i=0}^n a_{2i+1}x^{i}\\)，显然有 \\(F(x)=G(x^2)+xH(x^2)\\) 原本要对 \\(F(x)\\) 在 \\(\\Omega_{2n}\\) 上插值，现在要对 \\(G(x),H(x)\\) 在 \\(\\Omega_n\\) 上插值 主定理算一下就是 \\(O(n\\log n)\\) 的。 IDFT 考虑如下矩阵形式的DFT \\[\\begin{aligned} \\begin{bmatrix} \\omega^0 &amp; \\omega^0 &amp; \\cdots &amp; \\omega^0 \\\\ \\omega^0 &amp; \\omega^1 &amp; \\cdots &amp; \\omega^n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\omega^0 &amp; \\omega^n &amp; \\cdots &amp; \\omega^{n^2} \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n \\end{bmatrix} = \\begin{bmatrix} F(\\omega^0) \\\\ F(\\omega^1) \\\\ \\vdots \\\\ F(\\omega^n) \\end{bmatrix} \\end{aligned}\\] 即我们的 \\(\\Omega_n\\) 是一组基，\\(\\Set{a_n}\\) 则是基下的坐标，作用一下就能得到多项式在 \\(\\Omega_n\\) 上的点值。 这个矩阵 \\(W\\) 非常经典，算一下行列式和逆就会发现对于IDFT的操作只需要取 \\(\\omega_1&#39;=-\\omega_1\\) 做DFT即可实现逆变换，最后需要除掉一个系数 \\(\\frac{1}{n+1}\\)","tags":["Numerical Method"]},{"title":"博弈论02 零和游戏","path":"/2022/06/09/博弈论02-零和游戏/","content":"Zero Sum Games 即原本讨论的收益矩阵有两个，分别对应于玩家1和玩家2。零和游戏保证了 \\(A+B=O\\)，这说明只需要一个唯一的矩阵即可建模游戏收益，通常规定为玩家1的收益 考虑一个混合策略outcome \\((p,q)\\)，对于玩家1而言收益就是 \\(p^\\intercal Aq\\)，玩家2就是 \\(-p^\\intercal Aq\\)。对于纯策略只需要让概率分布坍缩为一个点就行了。 Min-Max 以下只讨论玩家1，玩家2是类似的。 对于任意的玩家2的混合策略 \\(q\\)，玩家1必然会选择使得 \\(p^\\intercal Aq\\) 最大化的 \\(p\\)，即 \\(p=\\text{argmax } p^\\intercal Aq\\) 而对于任意玩家1的混合策略 \\(p\\)，玩家2必然会选择使得 \\(p^\\intercal Aq\\) 最小化的 \\(q\\)，这说明 \\(p=\\text{argmax}_p\\min_qp^\\intercal Aq\\) 定理1 \\[\\begin{aligned} \\min_q \\max_p U(p,q)\\geq \\max_p \\min_q U(p,q) \\end{aligned}\\] 证明比较玄妙，就是一堆绕来绕去的min-max 首先对于 \\(U(p,q)\\) 将其视为关于 \\(q\\) 的函数，那么有函数在任意点处的函数值不小于其最小值 \\[\\begin{aligned} U(p,q)\\ge \\min_q U(p,q) \\end{aligned}\\] 此时将 \\(U(p,q)\\) 和 \\(\\min_q U(p,q)\\) 视为 \\(p\\) 的函数，那么两侧加上关于 \\(p\\) 的最大值仍然成立 \\[\\begin{aligned} \\max_p U(p,q)\\geq \\max_p \\min_q U(p,q) \\end{aligned}\\] 此时RHS是一个数，LHS是一个关于 \\(q\\) 的函数，这说明函数的最小值至少为RHS，即 \\[\\begin{aligned} \\min_q \\max_p U(p,q)\\geq \\max_p \\min_q U(p,q) \\end{aligned}\\] 定理2 若 \\(p^*,q^*\\) 分别是min-max和max-min时，有如下定理： \\((p^*,q^*)\\) 是MNE当且仅当它们得到的收益相等。 证明是某次作业 定理3 有限策略游戏的混合策略纳什均衡必然存在。 这说明必然存在 \\((p^*,q^*)\\) 这样的均衡局面，且这样的局面分别是min-max和max-min 定理4 在对称零和游戏中，均衡点必然收益为 \\(0\\)。 这是显然的，对称零和说明 \\(A^\\intercal=B=-A\\)，即对角线上收益为 \\(0\\)。对于正收益的局面，玩家2总能移动到对角线上获得一个更高的收益；负收益局面玩家1同理。 求解 对于玩家1而言即为求解 \\(\\max_p \\min_q p^\\intercal Aq\\)，可以等价地转化为如下线性规划： \\[\\begin{aligned} \\text{maximize }v \\\\ \\text{s.t.} \\\\ p^\\intercal A\\geq v\\textbf1 \\\\ \\text{where $p$ is a distribution over all strategies} \\end{aligned}\\]","tags":["Game Theory"]},{"title":"操作系统 Lab2 kmt","path":"/2022/05/28/OSLab2-kmt/","content":"Lab2-kmt 花了整个五一三天假期，最后是听了答疑才知道怎么解决栈的数据竞争的.... 痛苦的部分主要是多核 logging 和怎么用 qemu debug 的问题。搞定了这些技术上的难题，剩下就是老老实实写代码了。 设计 spinlock 去观摩了 xv6 的代码，发现不仅要自旋，还得关中断。并且关中断这事还得是嵌套的。 semaphore 每个信号量有一个等待队列 wait_tid，一个 value，还有一个 wakeup 表示需要唤醒多少个睡眠中的进程。 我的实现需要在 sem_signal 和 sem_wait 中修改全局的任务链表。由于 os_trap() 中途也可以 sem_signal，所以需要保证对链表读写的互斥。同时由于 os_trap() 的第一个操作必须是保存上下文、最后必须是切换，因此需要保证这两个操作对进程状态的修改是符合 save_context 后和 switch_task 前的语义的。 在调试过程中遇到过一个印象深刻的Bug 一开始我认为只能调度 RUNNABLE 的任务，但实际上可以调度所有非 RUNNING 的任务。注意到等待信号量进入睡眠后需要一次切换让出 CPU，这就是一种从 SLEEP 调度的情况。 栈的数据竞争 一开始的上下文切换是通过记录栈上的指针完成的，即每个任务记录一个上下文指针，指向栈上由 AM 的 cte 保存的上下文。 于是就可以观察到，某些时候 os_trap() 会返回到空的 %rip。 后面每个任务的结构体里都单独拷贝一份上下文，这样就会在多核时出现经典的 triple fault。然后STFW发现可以用 -d exec 来打印 trace，用 -d cpu-reset 来打印寄存器的值。然后就可以发现每次都是一个线程的 %rip 跑飞了，triple fault 就恰好是三次越界指令访问。并且可以发现每次都是在 cpu_current() 调用后返回到了错误地址，意味着栈被改写了。 然后我去翻了聊天记录，发现有同学问了一样的问题，但是没有看懂他的解决方案。于是中午去听了答疑，知道了怎么延迟任务T的调度来确保T的栈不会被两个 CPU 同时操作。感觉这个想法还是很厉害的。 但是这样做会出现新的问题：如果用smp=2跑3个任务，那么就会出现问题。CPU[0]从 idle[0]-&gt;print，而 CPU[1] 此时无法从 idle[1] 跳到任何任务（一个正在运行，另一个由于栈切换必须等到 CPU[0] 下一次 os_trap() 才能调度，但是 yield() 的语义是让出 CPU[1]，因此会被我的 assert 抓到） 解决方案也很简单。我开了2倍smp的 idle 任务，用于保证每个 CPU 至少可以切换到另一个 idle 上。这样虽然不太优雅，但也还能跑起来。 tty的神秘Bug 一开始我开了 128 个 task_struct，然后在跑 dev 的时候滚键盘就会出现某个任务的结构体被修改了的情况。通过 assert 和断点找到了是 tty_render 会 memset 一段内存，然后这段内存恰好处在某两个结构体中间，结果就是改写了我的结构体信息。 这个 Bug 比较难抓到，每7、8次才能复现一次，并且每次导致出错的 memset 地址都是一样的（非常整齐，恰好是页面的整数倍）。一开始我以为是 pmm.c 的问题，分配的内存和设备地址重叠了。但我打印之后发现并不是这样。而且更神奇的是，我把 task_struct 的数量减少到 64 之后，这个 Bug 就再也没法触发了。。。","tags":["Operating System"]},{"title":"操作系统 Lab1 pmm","path":"/2022/05/28/OSLab1-pmm/","content":"Lab1-pmm 这里不是实验报告，可以随便吐槽和说很多废话吧（大概） 也没有泄漏啥代码，纯纯的唠嗑，应该不违反学术诚信。 感觉这次实验被我做难了....事实上只需要链表就可以实现所有的操作，buddy system 纯纯的没必要，写出来也不容易 debug （也许熟练工可以做到一次对，但是这有啥用呢） 途中卡 smp=8 的时候尝试过 bitmap 和 slab。 bitmap 就是把一个大页分配成64份，用一个 uint64_t 状压使用情况，每次直接 lowbit(bitmap) 的得到未使用的小块。然后只需要维护两个链表：全满 和 存在空闲。那么一次 fast path 只需要 bit flip 就行了，但事实上过不了。 slab 直接看下面的 设计 我区分 thread_list 和 local_list 的设计参考了微软 mimalloc 的实现 分配 将 512MiB 的堆区大致分成 8 份，每份用一棵线段树维护以 4KiB 为最小单位的大块内存区间，这样每个 CPU 都有恰好一棵树。因为大块内存的分配需要对齐，且最大分配 8MiB 的内存，因此线段树的端点需要同时 8MiB 对齐和 4KiB 对齐。 对于大小超过 4KiB 的内存分配直接从该CPU的线段树上分配。注意到可能出现该 CPU 分配、其它 CPU 释放的情况，因此每个 CPU 的树在访问时都需要上锁，并且释放时需要释放到分配地。 对于大小不超过 4KiB 的内存分配，先按照 2^k 向上对齐。这里每个 CPU 都有 8 条链表，每个链表专门用于分配固定大小的内存块（例如 16KiB 32KiB 64KiB 128KiB...），这样不同大小的内存分配可以并行（好像没什么卵用，因为每个 CPU 目前都是单线程的） 根据对齐后的大小进入该 CPU 对应大小的链表中，此时每个链表下又分出两个子链表 local 和 thread local 表示本 CPU 分配且本 CPU 释放的内存块 thread 表示本 CPU 分配，但是由其它 CPU 释放的内存块 这样做的好处是 local 子链表不需要上锁，因为每个 CPU 都可以看作是单线程的，不存在数据竞争。对于跨 CPU 释放的情形，最坏情况是剩余 n-1 个 CPU 争抢 thread 链表的一把锁。 每次分配小内存，按照如下顺序分配： 查询local链表，有即分配，这是无锁的，且只涉及一个指针的读写操作。 查询thread链表，上锁，移动空余块到local链表，解锁。这部分只涉及到几个指针的读写操作，因此很快。 所属线段树上锁，申请大块空间（16KiB），等分成固定大小，插入到local链表中，解锁。这是 slow-path。 从上到下分别是 快 -&gt; 慢 的顺序 释放 对于大小超过 4KiB 的内存块将直接找到所在区间对应的线段树，上锁，释放，解锁。 对于大小不超过 4KiB 的小块内存释放，直接找到其分配时所属的 CPU、分配大小，然后分两类情况 本地分配本地释放，插入到 local 链表中 本地分配外地释放，上锁，插入到 thread 链表中，解锁 注意到要做到上述事情需要记录一些元信息。起初我是通过粗暴地翻倍分配空间、把 header 存在相邻的块中。这样如果每次都分配 129 KiB 的空间，那么就会产生 75% 的浪费。后面想到每个小块内存都是从一个特定大块中划分出来的，且线段树保证了大块内存是按块对齐的。因此直接将给定的地址低位清零找到对应大块的起始位置，在这里留出一个小块记录元信息即可。但是实现完这个之后oj就重测了，也不知道这样子做能不能过掉最后一个 low usage。 还有一个问题是如何区分大块释放（访问线段树）和小块释放（直接修改链表）。一个办法是在元信息中加入MAGIC NUMBER，这样就可以以很小的出错概率区分二者了。 印象深刻的Bug 最初的oj有smp=8的狂野case，卡了整整一周都卡不过去，写了8棵线段树都卡不过去，最后的解决方案是干掉case，这样大家就都可以过了，皆大欢喜（摔键盘）","tags":["Operating System"]},{"title":"计算方法05 图的代数性质","path":"/2022/05/10/Numerical05-图的代数性质/","content":"upd：最后两节课突然就悟了，因为tcs组主要是做这个的...看了看感觉就是硬广，那就学着吧。 Courant-Fischer 对于实对称矩阵 \\(A\\)，其最大特征值 \\(\\lambda_\\max(A)\\geq \\frac{x^\\intercal Ax}{x^\\intercal x}\\)，其中 \\(x^\\intercal x eq \\textbf 0\\) 图的代数性质 即对于给定的图 \\(G\\)，通过观察 \\(G\\) 的邻接矩阵/拉普拉斯矩阵的性质来获得某些 \\(G\\) 的性质。 邻接矩阵 记为 \\(A\\)，规定 \\(A_{x,y}=[xy\\in E(G)]\\)，其中 \\([\\cdot]\\) 为指示函数。 \\(A\\) 还可以写成如下形式 \\[\\begin{aligned} A=\\sum_{xy\\in E(G)} A^{(xy)} \\end{aligned}\\] 其中 \\({A^{(xy)} }_{x,y}=1\\) 其余位置皆为 \\(0\\)。即一张图的邻接矩阵可以由所有的边的邻接矩阵拼起来。 显然，对于无向图而言，\\(A^\\intercal=A\\) 特征值 \\(\\lambda_\\max(A_G)\\leq d_\\max(G)\\) 不妨设最大特征值为 \\(\\lambda\\)，属于 \\(\\lambda\\) 的特征向量 \\(x\\) 的绝对值最大的分量为 \\(x_i\\)（不妨设 \\(x_i&gt;0\\)），那么有 \\[\\begin{aligned} \\lambda x_i=(\\lambda x)_i=(Ax)_i=\\sum_{j=1}^n A_{i,j}x_j\\leq x_i\\sum_{j=1}^n A_{i,j}=d(i)x_i \\leq d_\\max(G) x_i \\end{aligned}\\] \\(\\lambda_\\max(A_G)\\geq d_{\\text{avg} }(G)\\) 只需要用到前置中的定理，令 \\(x=(\\frac{1}{\\sqrt{n} },\\frac{1}{\\sqrt{n} }\\ldots \\frac{1}{\\sqrt{n} })^\\intercal\\) 即可。 二分图 若 \\(G\\) 为二分图，则其邻接矩阵 \\(A\\) 为 对称阵，\\(A^\\intercal =A\\)。 分块矩阵，有 \\(\\begin{bmatrix}0 &amp; B \\\\ B^\\intercal &amp; 0\\end{bmatrix}=A\\)。这是因为存在点集 \\(V(G)\\) 的划分 \\(\\set{U,V}\\) 使得 \\(V,U\\) 的内部没有边，即邻接矩阵存在子矩阵为全 \\(0\\) 矩阵。 对于二分图，若 \\(\\lambda\\) 为邻接矩阵 \\(A\\) 的 \\(k\\) 重特征值，那么 \\(-\\lambda\\) 必然也为 \\(A\\) 的 \\(k\\) 重特征值。 证明： 不妨设属于 \\(\\lambda\\) 的特征向量为 \\(v=\\begin{bmatrix}{x}\\\\{y}\\end{bmatrix}\\)，那么有 \\(Av=\\lambda v\\)，即 \\(\\left\\{\\begin{aligned}By&amp;=\\lambda x\\\\B^\\intercal x&amp;=\\lambda y\\end{aligned}\\right.\\) 此时构造 \\(v&#39;=\\begin{bmatrix}x\\\\-y\\end{bmatrix}\\)，那么有 \\(Av&#39;=\\begin{bmatrix}-By\\\\B^\\intercal x\\end{bmatrix}=-\\lambda\\begin{bmatrix}x\\\\-y\\end{bmatrix}\\)，这说明 \\(v&#39;\\) 是属于特征值 \\(-\\lambda\\) 的特征向量。 当 \\(\\lambda\\) 重数为 \\(k\\) 时，有 \\(k\\) 个属于 \\(\\lambda\\) 的线性无关的特征向量，这 \\(k\\) 个分别取反就得到了 \\(k\\) 个属于 \\(-\\lambda\\) 的特征向量。 反之，若按顺序排布特征值 \\(\\lambda_1,\\lambda_2\\ldots \\lambda_n\\)，有 \\(\\forall i, \\lambda_i=-\\lambda_{n-i+1}\\) 成立，那么 \\(G\\) 是二分图。 证明： 对于任意的奇数 \\(k\\)，有 \\[\\begin{aligned} tr(A^k)=\\sum_{i=1}^n {\\lambda_i}^k=\\frac12\\sum_{i=1}^n \\left({\\lambda_i}^k + {\\lambda_{n-i+1} }^k\\right)=0 \\end{aligned}\\] 考虑 \\((A^k)_{i,j}\\)，其组合含义为 \\(i,j\\) 点对之间长度恰为奇数 \\(k\\) 的路径数量。根据矩阵迹的定义又有： \\[\\begin{aligned} tr(A^k)=\\sum_{i=1}^n (A^k)_{i,i}=0 \\end{aligned}\\] 由于 \\(A\\) 所有元素非负，因此 \\(\\forall i, (A^k)_{i,i}=0\\)，这说明对于任意奇数长度的圈，图 \\(G\\) 中都不存在。这恰好是二分图的充要条件。 拉普拉斯矩阵 也叫调和矩阵，在矩阵树定理里面叫做基尔霍夫矩阵。 规定 \\(L=D-A\\)，其中 \\(D=\\text{diag}\\{d_1,d_2\\ldots d_n\\}\\) 是度数对角阵。 对于边 \\(e=(x,y)\\) 定义 \\(L_{e}\\) 是 \\(L_e[x,y]=L_e[y,x]=1\\)，\\(L_e[x,x]=L_e[y,y]=-1\\)，其余为 \\(0\\) 的矩阵。那么有 \\[\\begin{aligned} L=\\sum_{e\\in E(G)} L_e \\end{aligned}\\] 类似 \\(A\\)，可以将 \\(L\\) 视为所有边相加得到的图。这个形式对于 \\(L\\) 的二次型非常有用，有如下形式： \\[\\begin{aligned} x^\\intercal Lx=\\sum_{e\\in E(G)} x^\\intercal L_e x=\\sum_{e=(i,j)\\in E(G)} (x_i-x_j)^2 \\end{aligned}\\] 由上式立刻得到 \\(L\\succeq 0\\) 为半正定矩阵，列举特征值将有 \\(0= \\lambda_1\\le \\lambda_2\\cdots \\lambda_n\\)。 特征值 \\(\\textbf 1\\) 是 \\(L\\) 的一个特征向量，对应特征值为 \\(0\\)。即 \\(L\\textbf 1=\\textbf 0\\) 证明：拆开即得。 \\(G\\) 连通，当且仅当 \\(L\\) 的特征值 \\(0\\) 重数为 \\(1\\)。 \"\\(\\Leftarrow\\)\"： 假设 \\(G\\) 不连通，则 \\(L\\) 可以写成 \\(\\begin{bmatrix}B &amp; 0 \\\\ 0 &amp; C\\end{bmatrix}\\)。注意到至少存在两个属于特征值 \\(0\\) 的特征向量\\(\\begin{bmatrix}\\textbf 1\\\\ \\textbf0\\end{bmatrix}\\) 和\\(\\begin{bmatrix}\\textbf 0\\\\ \\textbf1\\end{bmatrix}\\) \"\\(\\Rightarrow\\)\"： 已知 \\(G\\) 连通，那么取属于 \\(0\\) 的特征值 \\(x\\)，有 \\(x^\\intercal Lx=\\sum_{(i,j)\\in E(G)} (x_i-x_j)^2=0\\)。这说明 \\(\\forall (i,j)\\in E(G)\\) 都有 \\(x_i=x_j\\)，由连通性立即得到 \\(x=k\\textbf1\\)，即重数为 \\(1\\)。 结合半正定性立即有 \\(0=\\lambda_1&lt; \\lambda_2\\le \\cdots \\lambda_n\\)，也就是 \\(G\\) 连通当且仅当 \\(\\lambda_2&gt;0\\) 剩下的咕咕咕","tags":["Numerical Method"]},{"title":"计算方法04 图的随机游走","path":"/2022/05/10/Numerical04-图的随机游走/","content":"为了偷懒只讨论有限的情形 前置 离散概率分布可以表示为 \\({\\mathbb R}^n\\) 上的向量 \\(x\\)，满足 \\(\\sum_{i=1}^n x_i=1\\) 且 \\(\\forall i,x_i\\in[0,1]\\) 对于用向量表示的概率分布，可以定义两个分布的“距离”： \\[\\begin{aligned} d_{TV}(p,q)=\\frac{1}{2}\\left\\lVert p-q\\right\\rVert_1=\\frac{1}{2}\\sum_{i=1}^n\\left\\lvert p_i-q_i\\right\\rvert \\end{aligned}\\] 这里 \\(d_{TV}(,)\\) 表示 total variation distance。这样就可以定义一列分布的收敛性和极限了。系数的选择是任意的，但是这里可以思考为啥是 \\(\\frac{1}{2}\\) Markov Chain 对于一系列数量有限的状态，给出每个状态转移到下一个状态的概率 \\(Pr[s_i=y\\mid s_{i-1}=x]\\)，这就构成了一个状态机。把状态看成点，转移概率看成边权，就得到了一个有向带权图，并且这个图满足一些特殊的性质。 考虑怎么算出现在状态 \\(i\\) 的概率，这本质上是一个一阶递推，写出来就是 \\[\\begin{aligned} p_{k+1}=Tp_k \\end{aligned}\\] 这里 \\(p_k\\) 表示走了恰好 \\(k\\) 步后，处在每个状态上的概率分布 定义 周期 对于状态 \\(i\\)，其周期定义为 \\(gcd\\{t\\mid {P^t}_{i,i}&gt;0\\}\\)，记为 \\(period(i)\\)。称 Markov Chain 非周期当且仅当所有状态的周期都是 \\(1\\) 直观理解：从 \\(i\\) 出发后走恰好 \\(t\\) 步回到 \\(i\\)，所有这样的圈的长度的 gcd 即为周期。 这么定义的用处可以在后面看到。 不可约 有限图不可约当且仅当其为强连通图。此处强连通的定义为：任取 \\(x,y\\in V(G)\\)，存在两条有向路径 \\(P_1,P_2\\) 使得 \\(P_1=x\\rightsquigarrow y,P_2=y\\rightsquigarrow x\\)。不要求 \\(P_1,P_2\\) 点不相交 性质 若 Markov Chain 不可约、非周期，则存在常数 \\(T\\) 使得当 \\(t&gt;T\\) 时，\\({P^t}_{i,j}&gt;0\\) 对任意 \\(i,j\\) 成立 直观理解：走了足够多步后不存在走不到的状态。 只需要证明存在常数 \\(L\\)，使得任意长度至少为 \\(U\\) 的路径，都能在任意两点间找到。 不可约，则 \\(i,j\\) 存在有向路径 \\(i\\rightsquigarrow j\\)。 非周期，则存在最大的无法由两个圈线性组合出的正整数 \\(a\\times b-a-b\\) （NOIP2017，哈哈），记为 \\(mn(i)\\)，则此后的任意长度都可以由两个互质的圈线性组合出来。 只需要取 \\(T=n+\\max\\{mn(i)\\}\\)，则此后可以在任意节点对之间游走。 稳态分布 考虑给定初始分布 \\(p_0\\)，则极限 \\(\\lim\\limits_{k\\to\\infty} P^k p_0\\) 称为 Markov Chain 的极限分布。 若分布 \\(\\pi\\) 满足 \\(P\\pi=\\pi\\)，则称 \\(\\pi\\) 为平衡分布。可以证明极限分布若存在则必然为平衡分布。 设极限分布存在 \\(\\lim\\limits_{k\\to\\infty}P^k\\pi_0=\\pi&#39;\\)，那么有 \\[\\begin{aligned} P\\pi&#39;=P\\lim\\limits_{k\\to\\infty} P^k\\pi_0=\\lim\\limits_{k\\to\\infty} P^{k+1}\\pi_0=\\pi&#39; \\end{aligned}\\] 这说明 \\(P\\pi&#39;=\\pi&#39;\\) 是一个平衡分布。 Markov Chain 基本定理 若 Markov Chain 不可约、非周期，那么 存在稳态分布 \\(\\pi\\) 对于任意的 \\(p_0\\) 都有 \\(\\lim\\limits_{k\\to\\infty} P^k p_0=\\pi\\) \\(\\pi\\) 是唯一的 \\(\\pi_i=\\dfrac{1}{E[H_i]}\\)，其中 \\(H_i\\) 为随机变量，表示从 \\(i\\) 出发后第一次回到 \\(i\\) 的行走步数。\\(E[H_i]\\) 称为期望回归时间。 出现这个结论的原因在于，足够久之后任意点出发都将能走到任何点，因此两个不同的出发状态在足够久之后将“无法区分” 具体的证明看不懂，咕咕咕 Page Rank Google 提出的给网页打分的算法。它假设 每个用户在页面 \\(x\\) 浏览完后，将等概率点击一个 \\(x\\) 中的超链接（即等概率走向一个邻居） 每个用户在页面 \\(x\\) 浏览完后，有一定概率直接跳转到任意一个页面 \\(y\\) 可以发现 2 本质上就是新建超级点 \\(S\\)，然后每个点连向 \\(S\\)，再从 \\(S\\) 连回所有点。 注意到 1 实际上就是在有向图上随机游走，转移矩阵恰好为度数导出的一个概率矩阵。2 保证了即使原图不是非周期、强连通时，用户这样的操作仍然可以使得随机游走存在一个稳态分布/极限分布（新图是强连通/非周期的，why？）。直觉也是符合的，每个人可能会突然停止浏览，然后从另一个完全不相关的页面重新开始冲浪。 并且这样的分数（概率分布）只与图的结构有关，与初始迭代向量没有关系。","tags":["Numerical Method"]},{"title":"计算方法03 线性方程组求解","path":"/2022/05/08/Numerical03-线性方程组求解/","content":"前置 内积 二元函数 \\(\\left&lt;,\\right&gt;\\) 被称为内积，则其满足： \\(\\left&lt;x,y\\right&gt;=\\left&lt;y,x\\right&gt;\\) \\(\\left&lt;ax+by,z\\right&gt;=a\\left&lt;x,z\\right&gt;+b\\left&lt;y,z\\right&gt;\\) \\(\\left&lt;x,x\\right&gt;\\geq 0\\)，等号仅在 \\(x=\\textbf{0}\\) 时取到 向量范数 一元函数 \\(\\left\\lVert\\cdot\\right\\rVert\\) 被称为范数，则其满足： \\(\\left\\lVert x\\right\\rVert\\geq 0\\)，等号仅在 \\(x=\\textbf0\\) 时取到 \\(\\left\\lVert kx\\right\\rVert=\\left\\lvert k\\right\\rvert\\left\\lVert x\\right\\rVert\\) \\(\\left\\lVert x+y\\right\\rVert\\leq \\left\\lVert x\\right\\rVert+\\left\\lVert y\\right\\rVert\\) 在内积空间上有一个天然的范数 \\(\\left\\lVert x\\right\\rVert=\\sqrt{\\left&lt;x,x\\right&gt;}\\)，容易验证满足上述三条要求。 矩阵范数 算子范数 矩阵可以看成线性变换，因此可以由向量范数来衡量矩阵作为线性变换（算子）的“长度”。定义为 \\[\\begin{aligned} \\left\\lVert A\\right\\rVert=\\max_{\\left\\lVert x\\right\\rVert=1} \\left\\lVert Ax\\right\\rVert \\end{aligned}\\] 注意这里的 \\(Ax,x\\) 都是向量，因此 RHS 出现的全都是向量范数，LHS 则是定义出来的算子范数。 有了这个定义，我们就可以写出这样的不等式 \\[\\begin{aligned} \\left\\lVert Ax\\right\\rVert\\leq \\left\\lVert A\\right\\rVert\\left\\lVert x\\right\\rVert \\end{aligned}\\] 同时会引入新的定义，称满足下述要求的算子范数具有相容性： \\[\\begin{aligned} \\left\\lVert AB\\right\\rVert\\le \\left\\lVert A\\right\\rVert\\left\\lVert B\\right\\rVert \\end{aligned}\\] 有了相容性同样可以做一些界的估计 矩阵范数 矩阵同样也可以看成线性空间中的元素，因此可以单独赋予范数的定义，当然这就和向量范数没什么关系了。 一个例子是这样的，容易验证其满足三条要求 \\[\\begin{aligned} \\left\\lVert A\\right\\rVert=\\sum_{i,j}\\left\\lvert A_{i,j} \\right\\rvert \\end{aligned}\\] 高斯消元 对于给定的线性方程组 \\(Ax=b\\)，可以用简单 \\(O(n^3)\\) 的高斯消元法来求解。并且这样可以很容易地求出解空间的一组基，进而得到所有解。 解的稳定性 不妨假设 \\(A=xb\\) 中 \\(\\left\\lvert A\\right\\rvert eq0\\)，则有 \\(x=A^{-1}b\\) 通常 \\(A\\) 是给定的，而 \\(b\\) 是若干计算和观察的结果，因此解的误差主要来源于 \\(b\\) 引入的误差，可以写成 \\[\\begin{aligned} \\hat x=A^{-1}\\hat b \\end{aligned}\\] 考虑相对误差的计算，即为 \\[\\begin{aligned} \\max_{\\hat b,b eq0} {\\frac{\\left\\lVert A^{-1}\\hat b\\right\\rVert }{\\left\\lVert A^{-1}b\\right\\rVert } }/{\\frac{\\left\\lVert\\hat b\\right\\rVert }{\\left\\lVert b\\right\\rVert } } \\end{aligned}\\] 即后向相对误差与前向相对误差的比值，称这个值为方程组 \\(A\\)（矩阵 \\(A\\)）的条件数 \\(\\text{cond}(A)\\) \\[\\begin{aligned} \\begin{aligned} \\text{cond}(A)&amp;=\\max_{\\hat b,b eq 0} {\\frac{\\left\\lVert A^{-1}\\hat b\\right\\rVert }{\\left\\lVert A^{-1}b\\right\\rVert } }/{\\frac{\\left\\lVert\\hat b\\right\\rVert }{\\left\\lVert b\\right\\rVert } } \\\\ &amp;=\\max_{\\hat b eq 0} \\frac{\\left\\lVert A^{-1}\\hat b\\right\\rVert }{\\left\\lVert\\hat b\\right\\rVert }\\max_{b eq 0}\\frac{\\left\\lVert b\\right\\rVert }{\\left\\lVert A^{-1}b\\right\\rVert } \\end{aligned} \\end{aligned}\\] 注意到 \\(A^{-1}b=x\\)，且 \\(b=Ax\\)，带入即得 \\[\\begin{aligned} \\begin{aligned} \\text{cond}(A)&amp;=\\max_{\\hat b eq 0}\\frac{\\left\\lVert A^{-1}\\hat b\\right\\rVert }{\\left\\lVert\\hat b\\right\\rVert }\\max_{x eq 0} \\frac{\\left\\lVert Ax\\right\\rVert }{\\left\\lVert x\\right\\rVert } \\\\ &amp;=\\left\\lVert A^{-1} \\right\\rVert\\left\\lVert A\\right\\rVert \\end{aligned} \\end{aligned}\\] 矩阵的条件数反映了矩阵所对应线性方程组的不稳定程度。条件数越大则不稳定程度越大，误差的传递放大也就越严重。 迭代算法 \\(O(n^3)\\) 太昂贵，考虑迭代算法。一般而言迭代的次数是人为规定的，不少算法能够保证在 \\(\\Omega(n)\\) 次迭代之后必然得到精确解。 Jacobi 迭代 对于矩阵 \\(A\\)，将其分解为 \\(A=L+D+U\\)，其中 \\(L,U\\) 分别为上三角矩阵和下三角矩阵，\\(D\\) 为对角阵。 那么有 \\[\\begin{aligned} Ax=(L+U+D)x=b \\\\ x_{k+1}=D^{-1}(b-(L+U)x_k) \\end{aligned}\\] 收敛性 给出一个充分条件：若 \\(A\\) 是主对角线占优矩阵，则迭代必然收敛。 只需联立如下方程 \\[\\begin{aligned} \\left\\{ \\begin{aligned} x^*&amp;=D^{-1}(b-(L+U)x^*) \\\\ x_{k+1}&amp;=D^{-1}(b-(L+U)x_{k}) \\end{aligned} \\right. \\end{aligned}\\] 两式相减即得 \\[\\begin{aligned} x_{k+1}-x^*=-D^{-1}(L+U)(x_k-x^*) \\end{aligned}\\] 记 \\(W=D^{-1}(L+U)\\)，由对角占优可知 \\(Wx\\) 的每一项绝对值严格小于 \\(x\\)，因此迭代收敛。 正确性 假设收敛，则有不动点 \\(x\\)，简单替换即得不动点 \\(x\\) 是原方程的一个解。 结合收敛性的充分条件即得：若 \\(A\\) 为主对角线占优矩阵，则解唯一（\\(A\\) 可逆），且迭代收敛至唯一解。 看起来还是比较好的。 Gauss-Seidel 迭代 用到了一个观察，即在计算解向量 \\(x_{k+1}\\) 的第 \\(r\\) 项时，它的前 \\(r-1\\) 项都已经算出来了（废话） 因此可以写成 \\[\\begin{aligned} x_{k+1}=D^{-1}(b-Ux_k-Lx_{k+1}) \\end{aligned}\\] 证明和上面是类似的，结论也是类似的。 写代码可以发现这两个方法没有绝对的好坏之分，迭代速度也没有一般性的结论（至少俺不知道）。 谱半径 为了分析一类迭代算法的收敛性，引入谱半径的概念 定义 \\(A\\) 的谱半径为其绝对值最大的特征值 \\(\\left\\lvert\\lambda_\\max\\right\\rvert\\)，记为 \\(\\rho(A)\\) 对于这样的迭代算法 \\[\\begin{aligned} x_{k+1}=Ax_k+b \\end{aligned}\\] 取不动点做差得 \\[\\begin{aligned} x_k-x^*=A^k(x_0-x^*) \\end{aligned}\\] 如果能说明 \\(\\lim\\limits_{k\\to\\infty} A^k=O\\)，那么就能说明迭代是收敛的，且收敛到方程组的解。 有定理：\\(\\lim\\limits_{k\\to\\infty}A^k=O\\) 当且仅当 \\(\\rho(A)&lt;1\\) 分析 Jacobi 和 Gauss-Seidel 迭代矩阵的谱半径可以知道，当系数矩阵 \\(A\\) 是严格对角占优时，这两个算法以任意初始向量开始迭代都会收敛。 伪逆 对于满秩矩阵 \\(A\\)，方程组 \\(Ax=b\\) 的解是显然存在的。但是对于非满秩的矩阵 \\(B\\) 来说就不一定了。在最小二乘法中可以求得一个二范数最小的“逼近”解，实际上是解了一个方程 \\(A^\\intercal Ax=A^\\intercal b\\)，即 \\(x=(A^\\intercal A)^{-1}A^\\intercal b\\) 问题的关键在于 \\(A^{-1}\\) 不一定存在，这使得 \\(Ax=b\\) 不一定存在唯一解。因此引入记号 \\(A^\\dagger=(A^\\intercal A)^{-1}A^\\intercal\\)，称为 \\(A\\) 的伪逆（pseudo inverse），那么最小二乘法可以写成 \\(x=A^\\dagger b\\)，形式与满秩方程组 \\(x=A^{-1}b\\) 是一致的。这样求出的解为最佳平方逼近解。 当 \\(A\\) 的列线性无关时，\\(A^\\intercal A\\) 满秩，\\(A^\\dagger=(A^\\intercal A)^{-1}A^\\intercal\\) 存在。此时原方程组 \\(Ax=b\\) 无解（超定方程组） 由反证法假设存在 \\(x eq 0\\) 使得 \\(A^\\intercal Ax=0\\)，那么 \\((Ax)^\\intercal (Ax)=0\\)，根据内积的正定性可知 \\(Ax=0\\)，这说明存在列的线性组合为 \\(0\\)，这与列线性无关矛盾；故假设不成立。 类似取 \\(A^\\intercal\\) 可知，当 \\(A\\) 的行线性无关时，\\(A^\\intercal\\) 列线性无关，\\(AA^\\intercal\\) 满秩，\\(A^\\dagger=((A^\\intercal)^\\dagger)^\\intercal=A^\\intercal(AA^\\intercal)^{-1}\\)。此时方程组 \\(Ax=b\\) 有多解（欠定方程组），但是我不知道有啥意义，因为 \\(A^\\dagger\\) 是个右逆....","tags":["Numerical Method"]},{"title":"计算方法02 插值与函数逼近","path":"/2022/05/08/Numerical02-插值与函数逼近/","content":"函数逼近 考虑的是对于给定函数 \\(f\\) 和度量 \\(\\left\\lVert\\cdot\\right\\rVert\\)，求一个多项式函数 \\(p\\) 使得 \\(\\left\\lVert f-p\\right\\rVert\\) 尽可能小。这里不关注特定点上的值，而更在意两个函数总体的距离。 Weierstrass 逼近定理 若 \\(f\\in C[a,b]\\)，那么存在多项式列 \\(\\left\\{F_n\\right\\}\\)，使得 \\(\\left\\lVert\\lim\\limits_{n\\rightarrow\\infty} F_n -f\\right\\rVert_{\\infty}=0\\) \\(\\left\\lVert f\\right\\rVert_{\\infty}\\) 的含义是 \\(\\sup R(f)\\)，即值域的上确界。 注意到 \\([a,b]\\) 是任意的。为了便于讨论，一般通过伸缩平移到 \\([-1,1]\\) 上考虑。 Bernstein（又是他）给了一个构造性的证明，他构造出的多项式即为大名鼎鼎的Bernstein多项式。证明的技巧比较强，这里就不放了，感觉再抄一遍也没啥用.... Chebyshev 多项式 特殊的多项式族，规定了 \\(n\\) 次多项式一致逼近的误差下界。 定义 是一组多项式列 \\(\\left\\{T_n\\right\\}\\)，其中 \\(T_n\\) 是次数为 \\(n\\) 的多项式 \\[\\begin{aligned} T_n(x)=\\cos(n\\arccos x) \\end{aligned}\\] 令 \\(\\arccos x=\\theta\\) 注意到 \\[\\begin{aligned} T_{n+1}(x)=\\cos((n+1)\\theta)=\\cos n\\theta\\cos\\theta-\\sin n\\theta\\sin\\theta \\\\ T_{n-1}(x)=\\cos((n-1)\\theta)=\\cos n\\theta\\cos\\theta+\\sin n\\theta\\sin\\theta \\end{aligned}\\] 可得等价的递推式如下 \\[\\begin{aligned} \\begin{aligned} T_0(x)&amp;=1 \\\\ T_1(x)&amp;=x \\\\ \\cdots \\\\ T_{n+1}(x)&amp;=2xT_n(x)-T_{n-1}(x) \\end{aligned} \\end{aligned}\\] 性质 \\(T_n(x)\\) 是 \\(n\\) 次多项式，首项系数为 \\(2^{n-1}\\)。归纳可得。 \\(T_n(x)\\) 的值域为 \\([-1,1]\\)。这是个 \\(\\cos\\) 函数。 \\(T_n(x)\\) 的零点恰好为 \\(n\\arccos x=\\dfrac{\\pi}{2}+k\\pi\\) 的解，解恰有 \\(n\\) 个。 \\(T_n(x)\\) 在 \\([-1,1]\\) 之间震荡，并恰好变号 \\(n+1\\) 次。 \\(T_n(x)=\\prod_{i=1}^n (x-x_i)\\)，其中 \\(x_i\\) 是性质 3 中的第 \\(i\\) 个解。 并且有：任给 \\(n\\) 次首一多项式 \\(P_n(x)\\)，都有 \\(\\left\\lVert\\frac{1}{2^{n-1} }T_n(x)\\right\\rVert_\\infty\\leq \\left\\lVert P_n(x)\\right\\rVert_\\infty\\)，且 \\(\\left\\lVert\\frac{1}{2^{n-1} }T_n(x)\\right\\rVert_\\infty=\\frac{1}{2^{n-1} }\\) 由反证法，假设存在更小的 \\(P&#39;_n(x)\\)，则 \\(\\Delta(x)=\\frac{1}{2^{n-1} }T_n(x)-P&#39;_n(x)\\) 将会存在至少 \\(n\\) 个零点。而 \\(\\Delta(x)\\) 至多是 \\(n-1\\) 次多项式，这说明 \\(\\Delta(x)\\equiv 0\\) Chebyshev 多项式给了我们一个最小化形如 \\(\\prod (x-x_i)\\) 这样多项式的办法：设定 \\(x_i\\) 的值为 Chebyshev 多项式的零点，这样本身就得到了一个 Chebyshev 多项式。由其性质即得多项式的最值是同次首一多项式中最小的。 函数插值 Lagrange Interpolation 假设给了 \\(n\\) 个二维平面上的点对 \\(\\left\\{(x_i,y_i)\\right\\}\\)，如何求出一个函数恰好经过这 \\(n\\) 个点？ 考虑这么一个函数 \\(L_i^*(x)=\\prod_{j eq i}(x-x_j)= (x-x_1)(x-x_2)\\cdots(x-x_{i-1})(x-x_{i+1})\\cdots(x-x_n)\\)，它有如下性质： \\(\\deg L_i^*(x)\\leqslant n-1\\) \\(L_i^*(x_j)=0,i eq j\\) \\(L_i^*(x_i)=(x_i-x_1)(x_i-x_2)\\cdots(x_i-x_{i-1})(x_i-x_{i+1})\\cdots(x_i-x_n)\\) 为了方便我们可以配上一个系数，那么就得到 \\(L_i(x)=\\dfrac{L_i^*(x)}{L_i^*(x_i)}\\) 于是就有 \\(L_i(x_j)=\\delta_{i,j}\\)，其中 \\(\\delta_{i,j}\\) 为kronecker记号。 再继续构造 \\(F(x)=\\sum\\limits_{i=1}^n y_iL_i(x)\\)，根据上面的性质可知 \\(\\forall i\\in[n],F(x_i)=y_i\\) 这样拉格朗日就得到了这么一个经过 \\(n\\) 个点的多项式，且 \\(F(x)\\) 是多项式，有 \\(\\deg F(x)\\leqslant n-1\\) 唯一性 假设存在多项式 \\(G\\) 使得 \\(F(x) eq G(x)\\) 且 \\(\\deg G(x)\\leqslant n-1\\)，但 \\(\\forall i\\in [n], G(x_i)=F(x_i)=y_i\\) 此时构造 \\(H(x)=F(x)-G(x)\\)，则 \\(H(x)\\) 至多 \\(n-1\\) 次且有至少 \\(n\\) 个零点，由代数基本定理可知 \\(H(x)\\equiv 0\\)，这与 \\(G eq F\\) 矛盾。 误差分析 不妨假设 \\(x_0\\le x_1\\le\\cdots \\le x_n\\) \\[\\begin{aligned} R_n(x)=f(x)-P_n(x)=\\frac{f^{(n+1)}(\\xi)}{(n+1)!} {\\prod_{i=0}^{n} (x-x_i)} \\end{aligned}\\] 其中 \\(\\xi\\in(x_0,x_n)\\)，\\(f\\in C^{n+1}[x_0,x_n]\\) 固定一个 \\(x\\)，构造关于变元 \\(t\\) 的函数 \\[\\begin{aligned} \\varphi(t)=f(t)-P_n(t)-R_n(x) \\end{aligned}\\] 则 \\(\\varphi(t)\\) 在 \\([x_0,x_n]\\) 上有至少 \\(n+2\\) 个零点： \\(t=x_i\\)，\\(\\varphi(x_i)=f(x_i)-P_n(x_i)-R_n(x)=0\\) \\(t=x\\)，\\(\\varphi(x)=f(x)-P_n(x)-R_n(x)=0\\) 因此相邻的两个零点两两用中值定理即得 \\(\\varphi^{(n+1)}(t)\\) 在 \\([x_0,x_n]\\) 上有至少一个零点，展开即为 \\(R_n(x)\\) 的形式。 最小化误差 即我们既想插值，又想让插值多项式与目标函数尽量逼近。 回顾插值余项 \\(R_n(x)\\) 的定义，令 \\(\\omega_n(x)=\\prod_{i=0}^n (x-x_i)\\)，最小化 \\(\\omega_n\\) 即可最小化 \\(R_n\\)，这一点通过选取特殊的插值点来实现。 回想 Chebyshev 多项式的性质，我们只需要选取 \\(\\{x_i\\}\\) 使得 \\(n\\arccos x_i=\\dfrac{\\pi}{2}+k\\pi\\) 即可。这样插值出来的多项式被称为 Chebyshev 插值多项式。 最小二乘法 很多时候度量的选取是任意的，例如上面就选择了 \\(\\left\\lVert\\right\\rVert_\\infty\\) 作为函数逼近的度量。在选取 \\(\\left\\lVert\\right\\rVert_2\\) 作为度量时，则可以使用最小二乘法的办法来找到最佳平方逼近。 我们知道次数至多为 \\(n\\) 的多项式函数构成了 \\(n+1\\) 维线性空间。最小二乘法的本意即为用一个低维的线性空间来最佳地表征高维空间中的向量（或者反过来，求一个高维向量在低维空间中的投影），这样用于求平方逼近就是很自然的想法了。 为了方便坐标表示和运算，通常要求出一组多项式的正交基，然后就可以在坐标下讨论多项式逼近了。","tags":["Numerical Method"]},{"title":"博弈论01 策略游戏","path":"/2022/03/12/博弈论01-策略游戏/","content":"Pure Strategy Games 策略博弈说的是有限个玩家，每个玩家都有有限个决策，并且每个玩家的决策必须同时作出（即不能知悉其他人的决策），每个玩家都知道每一种决策组合的结果给每个玩家带来的收益。 形式化地，有 \\(N\\) 个玩家构成有限的玩家集 每个玩家 \\(i\\) 都有一个有限的决策集 \\(A_i\\)，表示其所有可以选择的决策 一个局面 \\(a\\in A=\\prod\\limits_{i=1}^N A_i\\)，包含了场上所有玩家所作出的决策 每个玩家 \\(i\\) 都有一个收益函数 \\(u_i\\colon A\\mapsto \\mathbb K\\)，其中 \\(\\mathbb K\\) 是一个全序集 之所以定义为全序集是因为并非所有收益都是可以计算出来的数值，但是我们希望任意两个收益是可以比较好坏的 为了简化，规定 \\(a_{i-1}\\in A_{-i}=\\prod\\limits_{i eq j} A_j\\) 表示玩家 \\(i\\) 的所有对手的某种决策局面 由1，某个局面 \\(a\\) 就可以写成是 \\((a_i,a_{-i})\\)，注意到我们并不关心局面的枚举顺序，因此这里实际上是集合而不是笛卡尔积.... 记 \\(B_i(a_{-i})=\\underset{a_i}{\\arg\\max}\\; u_i(a_i,a_{-i})\\) 为玩家 \\(i\\) 的所有对手选择了 \\(a_{-i}\\) 决策时，所有能最大化玩家 \\(i\\) 的收益的决策集，称为 \\(i\\) 的 Best Response Dominant Strategy 对于玩家\\(i\\)而言，某些决策严格劣于其余决策，因此是绝对不会选的，可以直接删掉。 上述过程可以持续进行直至不存在被严格支配的决策，这样可以化简求解时的难度 Nash Equilibrium 若一个局面 \\(a=(a_i,a_{-i})\\) 满足对于一切的玩家 \\(i\\) 都有命题 \\(\\forall c\\in A_i\\) ，\\(U_i(a_i,a_{-i})\\succeq U_i(c,a_{-i})\\) 成立，那么称局面 \\(a\\) 为一个（单一策略的）纳什均衡点 可以这么考虑：在纳什均衡的局面下，每个玩家单方面地作出决策变动都将让ta的收益下降。 一个简单的求解套路是对每个玩家 \\(i\\) 求出 \\(U_i(\\cdot)\\)，然后求 \\(\\dfrac{\\partial}{\\partial a_i} U_i(a_i,a_{-i})\\) 来得到一个 \\(a_i^{*}\\in B_i(a_{-i})\\)。显然在均衡点时，每个玩家都要最大化，那么联立求解就得到了一组解 \\(a^*=(a_i^*,a_{-i}^*)\\)。 注意这里的纯策略纳什均衡不一定存在，反例非常好找。并且若存在也不一定唯一，例如\"剪刀石头布\"的博弈模型就有三个均衡点。 求解PNE 可以枚举所有的outcome来逐一判断每个人是否都达到了最优，可以看成寻找一个超立方体上的“鞍点”。 Mixed Strategy Game 纯策略意味着每个人的决策是唯一确定的，这个前提太强。一个放松是给出选择每个决策的概率，即一个 \\(A_i\\) 上的概率分布 \\(p_i\\) 记 \\(\\Delta(A_i)\\) 为玩家 \\(i\\) 的决策集 $A_i $ 上所有可能的概率分布的集合，那么每个玩家的决策就会是一个决策集上的分布 \\(p_i\\in \\Delta(A_i)\\) 同样记 \\(p=(p_i,p_{-i})\\) 给出 \\(U_i(p)=\\sum\\limits_{a\\in A}Pr[X=a]u_i(X)\\)，其中 \\(X\\sim p\\) 为一个随机变量 注意到每个玩家互不交流，因此它们的决策相互独立，拆开就得到 \\(U_i(p)=\\sum\\limits_{a\\in A}u_i(a)\\prod\\limits_{i=1}^N p_j(a_j)\\)，这其实就是一个收益的期望，随机性来源于 \\(N\\) 个独立分布的叠加。 定理1 若 \\(p=(p_i,p_{-i})\\) 是一个纳什均衡，那么所有使得 \\(p_i(a)&gt;0\\) 的决策 \\(a\\) 都将会是局面 \\((a,p_{-i})\\) 的 Best Response。 意思是对于一个纳什均衡的局面 \\(p=(p_i,p_{-i})\\)，在固定了对手的所有决策分布之后，玩家 \\(i\\) 可能选择的单一决策（概率不为 \\(0\\) 的那些决策）的每一个都将是应对 \\(p_{-i}\\) 的最佳选择。 证明只需要反证一下，假设某个 \\(p_i(a)&gt;0\\) 但 \\(a ot\\in B_i(p_{-i})\\)，那么构造一个新的分布 \\(p_i&#39;\\) 满足 \\(p_i&#39;(a)=0\\) 且其余非零位置都乘上 \\(\\frac1{1-p_i(a)}\\)，可以证明新的分布下将达到更大的 \\(U_i(p_i&#39;)\\)，这与 \\((p_i,p_{-i})\\) 是纳什均衡矛盾；但是反过来不一定成立，例如可以存在多个单独最优解，但是我们不随机，只选择其中一个策略。 这个引理还是比较强的，说明即使引入了随机性，每个玩家的选择仍然是固定的，只不过现在变成了固定的集合。 Mixed Nash Equilibrium 每个finite strategy game都有至少一个混合策略的纳什均衡，称为Mixed Nash Equilibrium(MNE) 这个定理也是Nash证明的，算是比较漂亮的定理了。 求解MNE profile enumeration 对于一个非退化的2人游戏，可以 \\(2^n\\) 枚举概率非 \\(0\\) 的决策解一个不等式组。不妨设枚举出的行抽取出的矩阵为 \\(A\\)，那么对于玩家1而言必然有 \\(Aq\\) 的每一元素都相等（根据定理1）且严格大于剩余行的期望收益。 vertex enumeration 可以发现上面等价于给定原收益矩阵 \\(M\\)，要求对于玩家1而言满足 \\[\\begin{aligned} \\forall \\text{distribution } q \\\\ Mq\\ge v\\textbf1 \\\\ \\text{maximize } v \\end{aligned}\\] 这样的解构成了一个闭凸多边形，边界上的解代表某些约束取到了等号。","tags":["Game Theory"]},{"title":"Ubuntu下的数电实验环境配置","path":"/2022/03/08/Ubuntu下的数电实验环境配置/","content":"针对南京大学 数字逻辑与计算机组成实验 课程的环境配置，本机是Ubuntu 21.10 1 访问 这个网页，选择Individual Files，只需要下载 QuartusLiteSetup-20.1.1.720-linux.run (1.9GB) cyclonev-20.1.1.720.qdz (1.3GB) 2 进入下载目录，执行以下操作 chmod +x QuartusLiteSetup-20.1.1.720-linux.run ./QuartusLiteSetup-20.1.1.720-linux.run 途中选择带有 Free License 字样的 Modelsim-Questa 3 再参照这个回答，命令行执行以下操作 sudo apt-get install libxft2 libxft2:i386 lib32ncurses6 第二个是必须的，这样就解决了 RTL Simulation 时，弹窗报错需要 LD_LICENSE_FILE 环境变量的问题。 4 参照Intel官方文档，以su权限修改/etc/udev/rules.d/51-usbblaster.rules文件，添加： # Blaster I BUS==&quot;usb&quot;, SYSFS&#123;idVendor&#125;==&quot;09fb&quot;, SYSFS&#123;idProduct&#125;==&quot;6001&quot;, MODE=&quot;0666&quot; BUS==&quot;usb&quot;, SYSFS&#123;idVendor&#125;==&quot;09fb&quot;, SYSFS&#123;idProduct&#125;==&quot;6002&quot;, MODE=&quot;0666&quot; BUS==&quot;usb&quot;, SYSFS&#123;idVendor&#125;==&quot;09fb&quot;, SYSFS&#123;idProduct&#125;==&quot;6003&quot;, MODE=&quot;0666&quot; # Blaster II BUS==&quot;usb&quot;, SYSFS&#123;idVendor&#125;==&quot;09fb&quot;, SYSFS&#123;idProduct&#125;==&quot;6010&quot;, MODE=&quot;0666&quot; BUS==&quot;usb&quot;, SYSFS&#123;idVendor&#125;==&quot;09fb&quot;, SYSFS&#123;idProduct&#125;==&quot;6810&quot;, MODE=&quot;0666&quot; 具体关于udev是干什么的，可以看archwiki 5 在/usr/share/applications/目录下以su权限新建quartus.desktop，输入以下内容 [Desktop Entry] Type=Application Version=0.9.4 Name=Quartus (Quartus Prime 20.1) Lite Edition Comment=Quartus (Quartus Prime 20.1) Icon=/home/jjppp/intelFPGA_lite/20.1/quartus/adm/quartusii.png Exec=/home/jjppp/intelFPGA_lite/20.1/quartus/bin/quartus --64bit Terminal=false Path=/home/jjppp/intelFPGA_lite/20.1 这样就可以找到quartus作为程序的图标了 6 可以安装wine，那么就可以利用System Builder来生成已经分配好引脚的工程文件了。 总结 中途遇见了很多奇怪的问题，最奇怪的是LD_LICENSE_FILE的问题，明明是免费版本却出现了需要一个不存在的license.dat文件的情况.....最后是通过\"Questa-Modelsim LD_LICENSE_FILE\"搜到的解决方案，不然就得去翻log了。","tags":["DLCO"]},{"title":"计算方法01 函数求根","path":"/2022/03/07/Numerical01-函数求根/","content":"Fixpoint Theorem 定义函数 \\(f\\) 的不动点 \\(r\\) 为满足 \\(f(r)=r\\) 的所有取值 考虑函数 \\(f\\)，定义不动点迭代算法如下： 任取 \\(X\\in D(f)\\) 计算 \\(X=f(X)\\) 重复步骤2 \\(k\\) 次 记出现的所有 \\(X\\) 按顺序构成数列 \\(\\left\\{x_n\\right\\}\\)，定理如下 若 \\(f\\) 是连续函数，且 \\(\\lim\\limits_{n\\rightarrow \\infty}x_n=r\\)，那么 \\(f(r)=r\\) 证明： \\(f(r)=f\\left(\\lim\\limits_{n\\rightarrow\\infty}x_n\\right)=\\lim\\limits_{n\\rightarrow\\infty}f(x_n)=\\lim\\limits_{n\\rightarrow\\infty}x_{n+1}=r\\) 根据定义，\\(r\\) 是一个不动点 具体解释就是，当 \\(k\\) 充分大的时候，我们会得到一个充分接近 \\(r\\) 的近似解（极限的定义） Convergence Theorem 不动点定理说的是：如果迭代收敛，那么收敛到不动点 收敛定理则给出了迭代收敛的一个充分条件，也就是挑出了一类特殊的可以收敛的函数，给了一个判别条件。 定义 \\(e_n=\\left|x_n-r\\right|\\)，若 \\(\\lim\\limits_{n\\rightarrow\\infty}\\frac{e_{n+1} }{e_{n} }=S&lt;1\\)，那么称这个迭代是线性收敛的，收敛率为 \\(S\\) 若函数 \\(f\\) 连续可导，并且 \\(r\\) 是 \\(f\\) 的一个不动点，那么由 \\(|f&#39;(r)|&lt;1\\) 可以推出 \\(f\\) 在以一个足够接近 \\(r\\) 的初值开始迭代时线性收敛，收敛率为 \\(S\\) 这句话很难理解，但是结合证明就不太难了： 考虑 \\(x_{n+1}=f(x_n)\\)，那么有 \\(\\frac{e_{n+1} }{e_n}=\\left|\\frac{f(x_n)-r}{x_n-r}\\right|=\\left|\\frac{f(x_n)-f(r)}{x_n-r}\\right|=\\left|f&#39;(\\xi)\\right|\\)，其中 \\(\\xi\\in(x_n,r)\\)，最后一个等号是微分中值定理 取极限就得到了一个 \\(x=r\\) 处的导数，根据条件有这个极限的绝对值小于 \\(1\\) 又因为 \\(f\\) 连续可导，所以 \\(f&#39;\\) 连续，所以存在邻域 \\(U=(r-\\delta,r+\\delta)\\) 使得 \\(\\forall x\\in(r-\\delta,r+\\delta)\\) 都有 \\(|f&#39;(x)|&lt;1\\) 结合比值就知道，在 \\(U\\) 内任取一个元素作为初值开始迭代，每次的误差会严格递减。又因为误差单调有下界，所以收敛，并且收敛率就是 \\(|f&#39;(r)|&lt;1\\) 的 我们把这类收敛称为局部收敛 具体解释就是，如果函数连续可导并且不动点处的导数比较好，那么存在一个区间 \\(U\\)，如果我们在 \\(U\\) 内开始迭代时，就能线性收敛到不动点，并且收敛率是不动点处的导数。 但是定理反过来不成立，意思是一个并非局部收敛的函数可能在别的地方收敛到此，这是完全可行的。 这个定理可以是后验的，即先算出一个收敛的点，然后求导验证是否满足定理前提。 根的敏感性 假设 \\(f\\) 的计算存在误差，例如给定 \\(x\\) 时我们只能计算 \\(f(x)+\\epsilon g(x)\\)，其中 \\(\\epsilon\\) 是一个小常数，\\(g\\) 是一个关于 \\(x\\) 的误差函数。那么我们在求根 \\(r\\) 时引入的误差就会进一步被放大。 不妨设求得的数值根为 \\(r+\\Delta r\\)，那么我们此时求得的实际上是带误差的函数的根，满足 \\[\\begin{aligned} f(r+\\Delta r)+\\epsilon g(r+\\Delta r)=0 \\end{aligned}\\] 两边泰勒展开一下就有 \\[\\begin{aligned} f(r)+\\Delta r f&#39;(r) + \\epsilon g(r) + \\epsilon \\Delta r g&#39;(r) + O\\left({\\Delta r}^2\\right)=0 \\end{aligned}\\] 舍去高阶无穷小就是 \\[\\begin{aligned} \\Delta r=\\frac{-\\epsilon g(r)-f(r)}{f&#39;(r)+\\Delta r g&#39;(r)} \\end{aligned}\\] 注意到 \\(f(r)=0\\) 是 \\(f\\) 的根的定义，且 \\(\\Delta r\\) 很小，因此上式约为 \\[\\begin{aligned} \\Delta r\\approx -\\epsilon\\frac{g(r)}{f&#39;(r)} \\end{aligned}\\]","tags":["Numerical Method"]},{"title":"数理逻辑03 一阶逻辑","path":"/2022/02/13/Logic03-一阶逻辑/","content":"写在前面 命题逻辑（或者 零阶逻辑）到一阶逻辑的变化，在于描述的粒度。命题逻辑只能描述命题之间的关系，以及它们如何构成更大的结构（新的命题）。在一阶逻辑中我们可以深入原子命题的内部，讨论命题的构成。 为了做到这一点，需要引入集合上的n元关系。从集合论作为基础的角度看，这么做是比较和谐的。 命题逻辑 命题逻辑的不同之处在于，我们既可以描述一个系统内的某些运算（通过函数、变量和常元），又可以描述由这些运算的结果得到的命题（通过谓词和逻辑连接符）。 一个例子就是标准算术模型 \\(\\scr A\\)： 我们希望能够描述一个系统 \\(\\scr A\\)，即描述清楚 系统内部存在一些元素（自然数） 这些元素互相可以通过运算（加法、后继）得到新的自然数 可以判断两个元素的大小关系（\\(&lt;\\) 是二元谓词）、相等关系（\\(=\\) 是二元谓词） 可以把若干判断组合成一个更大的判断（通过命题构造子组合命题） 注意到上述4条是有层级的，12位于系统内部，3可以根据需要构造，4在不同的系统中可以完全相同。 n元关系 定义集合 \\(D\\) 上的n元关系为n元函数 \\(f\\colon D^n\\mapsto \\left\\{T,F\\right\\}\\)，取 \\(P=\\left\\{x\\mid x\\in D,f(x)=T\\right\\}\\)，那么就可以仅用集合来表示n元关系。 特别的，\\(&lt;\\) 是一个二元关系。它同时是 \\(\\mathbb R,\\mathbb N,\\mathbb Z\\) 上的二元关系，因此可以看出n元关系的定义还要看其定义域，此处即论域 语法 记 \\(\\scr P,C,V,F\\) 分别为 谓词、常量、变量、函数 符号的可数集，规定每个谓词 \\(P\\in{\\scr P}\\) 和函数 \\(F\\in{\\scr F}\\) 都有arity（有多少参数）\\(\\mu(P),\\mu(F)\\in\\mathbb N\\)。\\(0\\) 元谓词就是命题逻辑中的命题，\\(0\\) 元函数即为常元。 一阶逻辑中仍然存在命题构造子 \\(\\{\\wedge,\\vee,\\rightarrow, eg\\}\\)，通常取一个完备集即可。 规定量词 \\(\\forall,\\exists\\) 分别读作 任意 和 存在 项 项集 \\({\\scr T}\\) 由如下递归定义： \\(\\scr C\\subseteq T\\)，常元是项 \\(\\scr V\\subseteq T\\)，变量是项 \\(f\\in{\\scr F},f(t_1,t_2\\ldots t_{\\mu(f)})\\in{\\scr T}\\)，由若干项作为实参的函数作用是项 项仅限于此 所谓的项集就是在描述系统内部的元素，产生新的项的方法只有函数作用。 原子公式 形如 \\(p(t_1,t_2\\ldots t_{\\mu(p)}),p\\in{\\scr P}\\) 的公式是原子公式 原子公式承担了从项过度到公式（命题）的角色 公式 给出BNF \\[\\begin{aligned} \\begin{aligned} formula &amp;:= atomic\\_formula\\;|\\; eg formula\\;|\\;formula\\vee formula\\;|\\;formula\\wedge formula \\\\ formula &amp;:= \\exists x\\,formula\\;|\\;\\forall x\\,formula \\end{aligned} \\end{aligned}\\] 一阶逻辑是对命题逻辑的简单拓展，仍然保持了树状结构，之前的证明套路仍然适用。 在一阶逻辑中既然有变量就同样有作用域的问题。具体的定义类似\\(\\lambda\\)-演算： 公式 \\(\\forall x\\, F\\)，\\(\\exists x\\, F\\) 中，\\(x\\) 的作用域为公式 \\(F\\)，\\(F\\) 不要求有 \\(x\\) 出现 称变量 \\(x\\) 是公式 \\(F\\) 中的自由变量，当且仅当 \\(x\\) 在 \\(F\\) 中出现，且不在限定 \\(x\\) 的任何作用域中。 非自由变量就称其为约束变量。 若公式 \\(F\\) 中，任意变量都是约束变量，则称 \\(F\\) 为封闭公式（Closed Formula），宋公的书叫做句子。 替换 和 \\(\\lambda\\)-演算中的替换是一模一样的 语义 命题逻辑的语义由解释给出，在一阶逻辑则不够 回忆命题逻辑中解释的定义：\\(\\scr I\\) 是函数 \\(U_A\\mapsto \\left\\{T,F\\right\\}\\)，其中 \\(U_A\\) 表示公式 \\(A\\) 中全部原子命题构成的集合 考虑还差了哪些。为了实现类似的效果，我们需要 给常量赋予含义 给变量赋予含义 给项赋予含义 给谓词赋予含义 给原子公式赋予含义 解释 宋公的书把这个叫做结构，也行吧。 规定公式 \\(A\\) 的解释是一个三元组 \\((D,\\left\\{R_1\\ldots R_m\\right\\},\\left\\{d_1\\ldots d_k\\right\\})\\) 其中 \\(D\\) 是论域，\\(R_i\\) 是论域 \\(D\\) 上的关系，\\(d_j\\) 是 \\(D\\) 中的元素，其赋予了 \\(A\\) 中常量含义。 一个例子是 \\(\\textbf 1&lt;\\textbf 2\\) 和 \\(壹&lt;贰\\)，此处的 \\(\\left\\{壹，贰\\right\\}\\) 都是常量，在规定解释为 \\((\\mathbb N,\\left\\{&lt;\\right\\},\\left\\{1,2\\right\\})\\) 时才能说等式成立（讨论其真值）。可能存在这么一个神奇的国度，它们把 \\(1\\) 写作 \\(\\textbf 2\\)，把 \\(2\\) 写作 \\(\\textbf 1\\)，那么式1在它们的国度（特定解释下）就不成立了。 但这是不够的。考虑公式 \\(x&lt;a\\)，其在任意解释下都不能讨论真值，因为自由变量 \\(x\\) 的值无法确定，由此引入赋值的定义。 赋值 记 \\({\\scr I}_A\\) 是公式 \\(A\\) 的解释，\\(A\\) 的赋值 \\(\\sigma_{[{ {\\scr I}_A}]}\\) 是函数 \\({\\scr V}\\mapsto D\\)，其赋予了 \\(A\\) 中所有自由变量唯一的论域中的元素作为值。 可以通过类似于 \\(\\sigma_{[{\\scr I}_A]}\\left\\{x\\rightsquigarrow v\\right\\}\\) 来对映射进行单点修改，非常熟悉的味道 项的语义 base case都很简单，需要注意的只有这么一点：项集是必然可数的，因此项的解释必然是可数的。 大概可以这么理解：对于实数 \\(\\R\\)，我们必然没法用一阶语言来遍历（穷举）它，因为一阶的语言必然是可数的。 这里就出现了一个gap，我们对任意的项进行解释，不一定能得到整个论域。 公式的语义 base case都很简单，需要注意 \\(\\forall x.P\\) 的解释为：对于一切 \\(t\\in d\\) 都有 \\(P[\\frac{d}{x}]\\) 为真。 没了，就这么简单。","tags":["Mathematical Logic"]},{"title":"数理逻辑02 推演系统","path":"/2022/01/24/Logic02-推演系统/","content":"写在前面 在上一章给出了命题逻辑的语法、语义，公理化的定义，以及一种判定公式是否可满足/永真的算法，但是这还不够，因为： 并不是所有逻辑都有Decision Procedure，因此这种方法不够普遍 即使有，在有无穷多的公理时，Decision Procedure很可能没法处理无穷项的公式（算法不一定终止） 即使终止，Decision Procedure也只能展示一件事情：命题的真值。我们没法得到中间结果，对理解逻辑没有帮助 于是就有了推演系统（Deductive System），这是对推理证明的形式化。同样的，推演系统有很多种，每一个都有自己的语法（公理+推导规则）和语义（对应逻辑的语义）。在命题逻辑中，不同的一致完备推演系统相互等价，这可以看成是对同一个东西的不同解释（模型）。我们所有的推演都是基于语法的推导，而一致性和完备性则是连接了语法和语义的桥梁，它保证了推演系统足够强大又不会出错。 推演系统 包括一组公理集（Set of Axioms）和一组推演规则（Rules of Inference） 证明 一个证明（Proof）指的是一段长度有限的公式序列 \\(A_1,A_2\\ldots A_n\\)，其中每个公式 \\(A_i,i\\in\\left\\{1\\ldots n\\right\\}\\) 要么是公理 要么可以由前面的公式+推演规则得到 要么是前面出现过的公式 要么是已经证明过的定理 用 \\(\\vdash A\\) 表示 \\(A\\) 在推演系统中可证明，即存在一个证明使得 \\(A\\) 是最后出现的公式，称 \\(A\\) 为一个定理（Theorem）。对于中间出现的公式我们称为引理（Lemma）。 \\(\\scr G\\) 给出 Gentzen 系统的（语法）定义 公理 含有互补对的公式集是公理 推演规则 有两类规则 $$ \\[\\begin{align*} &amp;\\frac{\\vdash U\\cup\\left\\{\\alpha_1,\\alpha_2\\right\\} }{\\vdash U\\cup\\left\\{\\alpha\\right\\} } &amp;\\frac{\\vdash U\\cup\\left\\{\\beta_1\\right\\}\\text{\\quad}\\vdash U\\cup\\left\\{\\beta_2\\right\\} }{\\vdash U\\cup\\left\\{\\beta\\right\\} } \\end{align*}\\] $$ 一个常见的例子是令 \\(\\alpha=\\alpha_1\\vee\\alpha_2\\)，\\(\\beta=\\beta_1\\wedge\\beta_2\\) 在 \\(\\scr G\\) 中，一个证明即是一个由公式集组成的序列。 正确性 \\(\\scr G\\) 的正确性由如下重要定理给出： 设 \\(\\scr U\\) 是一个公式集，\\(\\scr \\bar U\\) 定义为 \\({\\scr\\bar U}=\\left\\{\\bar A\\mid A\\in{\\scr U}\\right\\}\\)，那么有 \\(\\vdash {\\scr U}\\) 当且仅当 \\({\\scr \\bar U}\\) 存在closed Semantic Tableaux 该定理的特殊情况是 \\({\\scr U}=\\left\\{U\\right\\}\\)，此时定理表述为：在 \\(\\scr G\\) 中 \\(\\vdash U\\) 当且仅当 \\( eg U\\) 存在closed Semantic Tableaux 证明 先证明充分性，即：若 \\(\\scr\\bar U\\) 存在closed Semantic Tableaux \\(\\scr T\\)，则 \\(\\vdash\\scr U\\) 考虑对 \\(\\scr T\\) 这个树形结构做结构归纳 \\(\\scr\\bar V\\) 是 \\(\\scr T\\) 的叶子，则其存在互补对，此时 \\(\\scr V\\) 也存在互补对，因此 \\(\\scr V\\) 是 \\(\\scr G\\) 中的公理，\\(\\vdash\\scr V\\) \\(\\scr\\bar V\\) 不是 \\(\\scr T\\) 的叶子，不妨设 \\(\\scr\\bar V\\) 中的新增公式为 \\(\\phi\\)，分类讨论： \\(\\scr\\bar V\\) 是一个 Semantic Tableaux 上的 \\(\\alpha\\) 推导，则不妨设 \\(\\phi=\\alpha_1\\wedge\\alpha_2\\)，则其子树 \\(\\scr\\bar V&#39;\\) 根据归纳假设有 \\(\\vdash\\scr V&#39;\\)，并且有 \\(\\scr\\bar V&#39;=\\bar V-\\left\\{\\alpha_1\\wedge\\alpha_2\\right\\}\\cup\\left\\{\\alpha_1,\\alpha_2\\right\\}\\)。 此时可得 \\({\\scr V}={\\scr V&#39;}\\cup\\left\\{ eg(\\alpha_1\\wedge\\alpha_2)\\right\\}-\\left\\{ eg\\alpha_1, eg\\alpha_2\\right\\}\\)，再根据 \\(\\scr G\\) 中的 \\(\\alpha\\) 推导规则就可得到 \\(\\vdash\\scr V\\)。对于 \\(\\phi\\) 为其它情况的证明是类似的。 \\(\\scr\\bar V\\) 是一个 Semantic Tableaux 上的 \\(\\beta\\) 推导，则不妨设 \\(\\phi=\\beta_1\\vee\\beta_2\\)，则其子树 \\(\\scr\\bar V_1,\\bar V_2\\) 根据归纳假设有 \\(\\vdash\\scr V_1,\\vdash V_2\\)，并且有 \\(\\scr\\bar V_1=\\bar V-\\left\\{\\beta_1\\vee\\beta_2\\right\\}\\cup\\left\\{\\beta_1\\right\\},\\;\\bar V_2=\\bar V-\\left\\{\\beta_1\\vee\\beta_2\\right\\}\\cup\\left\\{\\beta_2\\right\\}\\)。 此时可得 \\({\\scr V}={\\scr V_1\\cup V_2}\\cup\\left\\{ eg(\\beta_1\\vee\\beta_2)\\right\\}-\\left\\{ eg\\beta_1, eg\\beta_2\\right\\}\\)，再根据 \\(\\scr G\\) 中的 \\(\\beta\\) 推导规则就可得到 \\(\\vdash\\scr V\\)。对于 \\(\\phi\\) 为其它情况的证明是类似的。 再证明必要性，即：若 \\(\\vdash \\scr U\\)，那么 \\(\\scr\\bar U\\) 存在closed Semantic Tableaux \\(\\scr T\\) 注意到 \\(\\scr G\\) 中的证明本身也有某种树形结构（不过是倒置的），因此也考虑对 \\(\\scr U\\) 做结构归纳 \\(\\scr U\\) 是 \\(\\scr G\\) 中的公理，故存在互补对，此时 \\(\\scr\\bar U\\) 也存在互补对，故 \\(\\scr\\bar U\\) 必然构造出 \\(\\scr T\\) \\(\\scr U\\) 经推导而来，不妨记新增公式为 \\(\\phi\\)，记 \\({\\scr U}={\\scr U&#39;}\\cup\\left\\{\\phi\\right\\}\\) \\(\\phi = \\alpha_1\\vee\\alpha_2\\)，此时记前提（Premise）为 \\(\\scr U_1=\\scr U&#39;\\cup\\left\\{\\alpha_1,\\alpha_2\\right\\}\\)，必然已有 \\(\\vdash {\\scr U_1}\\)。根据归纳假设有 \\(\\scr\\bar U_1\\) 存在closed Semantic Tableaux，并且有 \\({\\scr\\bar U} = {\\scr \\bar U_1}\\cup\\left\\{ eg(\\alpha_1\\vee\\alpha_2)\\right\\}-\\left\\{ eg\\alpha_1, eg\\alpha_2\\right\\}\\)，只需要根据Semantic Tableaux中的 \\(\\alpha\\) 推导规则就可以通过 \\(\\scr\\bar U_1\\) 的Tableaux来得到 \\(\\scr\\bar U\\) 的Tableaux了。其余形式的 \\(\\phi\\) 的证明是类似的。 \\(\\phi = \\beta_1\\wedge\\beta_2\\)，此时记前提（Premise）为 \\(\\scr U_1=\\scr U&#39;\\cup\\left\\{\\beta_1\\right\\},U_2=U&#39;\\cup\\left\\{\\beta_2\\right\\}\\)，必然已有 \\(\\vdash {\\scr U_1},\\vdash{\\scr U_2}\\)。根据归纳假设有 \\(\\scr\\bar U_1\\) 和 \\(\\scr\\bar U_2\\) 都存在closed Semantic Tableaux，并且有 \\({\\scr\\bar U} = {\\scr \\bar U_1}\\cup{\\scr\\bar U_2}\\cup\\left\\{ eg(\\beta_1\\wedge\\beta_2)\\right\\}-\\left\\{ eg\\beta_1, eg\\beta_2\\right\\}\\)，只需要根据Semantic Tableaux中的 \\(\\beta\\) 推导规则就可以通过 \\(\\scr\\bar U_1,\\bar U_2\\) 的Tableaux来得到 \\(\\scr\\bar U\\) 的Tableaux了。其余形式的 \\(\\phi\\) 的证明是类似的。 回过头看这个证明，无非是把 \\(\\scr G\\) 中的每个公式集取反后就对应到了 Semantic Tableaux 上，并且可以发现两个系统里的推导规则可以一一对应。再结合对公式集可满足性的定义就会发现，本质上是对整个公式集组成的大公式做了取反，仅此而已。 Soundness &amp; Completeness 在 \\(\\scr G\\) 中，\\(\\vdash A\\) 当且仅当 \\( eg A\\) 存在closed Semantic Tableaux 当且仅当 \\( eg A\\) 不可满足 当且仅当 \\(\\models A\\) 这样就证完了。只需要建立起 \\(\\scr G\\) 到 \\(\\scr T\\) 的一一对应，就可以利用 \\(\\scr T\\) 的一致完备性得到 \\(\\scr G\\) 的一致完备性，这正是推演证明的一种。 \\(\\scr H\\) 我靠这个花体字实在是太帅了，奈何我怎么都写不出这种感觉。 在 \\(\\scr H\\) 中用大写字母表示命题变元（即可带入任意的命题） 下面给出Hilbert 系统的定义 公理 $$ \\[\\begin{aligned} \\textbf{Axiom 1 } &amp;\\vdash A\\rightarrow (B\\rightarrow A) \\\\ \\textbf{Axiom 2 } &amp;\\vdash (A\\rightarrow (B\\rightarrow C))\\rightarrow((A\\rightarrow B)\\rightarrow (A\\rightarrow C)) \\\\ \\textbf{Axiom 3 } &amp;\\vdash ( eg B\\rightarrow eg A)\\rightarrow(A\\rightarrow B) \\end{aligned}\\] $$ 推演规则 只有一条 $$ \\[\\begin{aligned} \\frac{\\vdash A\\;\\;\\;\\vdash A\\rightarrow B}{\\vdash B} \\end{aligned}\\] $$ 也叫做肯定前件（modus ponens），记为MP 对证明的拓展 设 \\(\\scr U\\) 是公式集，\\(A\\) 是某个公式，则符号 \\({\\scr U}\\vdash A\\) 表示 \\(\\scr U\\) 中的公式是证明 \\(A\\) 的假设 \\(\\scr H\\) 中的证明是一系列形如 \\({\\scr U_i}\\vdash A_i\\) 的公式，其中 \\(A_i\\) 要么是公理 要么是已经证明过的引理 \\(A_i\\in {\\scr U_i}\\) 可由已经证明过的定理+MP得到 这一拓展反映的是证明形如“如果 \\(A\\) 那么 \\(B\\) ”的命题时，我们可以假设 \\(A\\) 成立，然后检查 \\(B\\) 是否可被证明。 衍生规则 虽然只有MP是足够的，但是只用MP就像在裸奔，因此需要包装MP和公理来得到一些更抽象的推演规则。We are now doing composition! 具体可以把这些规则看成是一些语法上的宏，展开就能得到纯MP和公理组成的推演规则。 Deduction Rule $$ \\[\\begin{aligned} \\frac{ {\\scr U}\\cup \\left\\{A\\right\\}\\vdash B}{ {\\scr U}\\vdash A\\rightarrow B} \\end{aligned}\\] $$ 这条规则就反映了拓展证明的意图，这条规则保证了我们可以在证明 \\(A\\rightarrow B\\) 时先假设 \\(A\\)，再证明 \\(B\\) 下面需要证明拓展是sound的，即不会引入原本不能证明的命题。其completeness是显然的。 对 \\({\\scr U}\\cup\\left\\{A\\right\\}\\vdash B\\) 的推导步骤数 \\(n\\) 作数学归纳法 \\(n=1\\)，此时 \\(B\\) 一步就得到了，因此 \\(B\\) 要么是公理，要么是 \\(A\\)，要么是已经证过的定理，下面给出 \\(B eq A\\) 的证明： \\[\\begin{aligned} \\begin{aligned} &amp;{\\scr U}\\vdash B \\\\ &amp;{\\scr U}\\vdash B\\rightarrow (A\\rightarrow B) &amp;\\textbf{Axiom 1} \\\\ &amp;{\\scr U}\\vdash A\\rightarrow B &amp;\\text{MP 1, 2} \\end{aligned} \\end{aligned}\\] 故sound 当 \\(A=B\\)，该命题退化为 \\({\\scr U}\\vdash A\\rightarrow A\\) \\(n&gt;1\\)，因此 \\({\\scr U}\\cup\\left\\{A\\right\\}\\vdash B\\) 是一个MP 不妨设 \\[\\begin{aligned} \\begin{aligned} &amp;{\\scr U}\\cup\\left\\{A\\right\\}\\vdash C \\\\ &amp;{\\scr U}\\cup\\left\\{A\\right\\}\\vdash C\\rightarrow B \\end{aligned} \\end{aligned}\\] 上面两式根据归纳假设有 \\[\\begin{aligned} \\begin{aligned} &amp;{\\scr U}\\vdash A\\rightarrow C &amp;\\text{Deduction Rule} \\\\ &amp;{\\scr U}\\vdash A\\rightarrow (C\\rightarrow B) &amp;\\text{Deduction Rule} \\\\ &amp;{\\scr U}\\vdash (A\\rightarrow (C\\rightarrow B))\\rightarrow ((A\\rightarrow C)\\rightarrow (A\\rightarrow B)) &amp;\\textbf{Axiom 2} \\\\ &amp;{\\scr U}\\vdash (A\\rightarrow C)\\rightarrow (A\\rightarrow B) &amp;\\text{MP 2, 3} \\\\ &amp;{\\scr U}\\vdash A\\rightarrow B &amp;\\text{MP 1, 4} \\end{aligned} \\end{aligned}\\] 故sound Contrapositive Rule \\[\\begin{aligned} \\begin{aligned} \\frac{ {\\scr U}\\vdash eg B\\rightarrow eg A}{ {\\scr U}\\vdash A\\rightarrow B} \\end{aligned} \\end{aligned}\\] 只需要用一下 \\(\\textbf{Axiom 3}\\) 就可以得到的规则，注意这条规则和如下规则的区别： \\[\\begin{aligned} \\begin{aligned} \\frac{ {\\scr U}\\vdash A\\rightarrow B}{ {\\scr U}\\vdash eg B\\rightarrow eg A} \\end{aligned} \\end{aligned}\\] 因为我们目前都只在语法层面操作公式，因此二者是有本质区别的规则，需要分别单独证明 Transitivity Rule \\[\\begin{aligned} \\frac{ {\\scr U}\\vdash A\\rightarrow B\\;\\;\\;\\;\\;{\\scr U}\\vdash B\\rightarrow C} { {\\scr U}\\vdash A\\rightarrow C} \\end{aligned}\\] 证明如下： \\[\\begin{aligned} \\begin{aligned} &amp; {\\scr U}\\vdash A\\rightarrow B \\\\ &amp; {\\scr U}\\vdash B\\rightarrow C \\\\ &amp; {\\scr U}\\vdash (B\\rightarrow C)\\rightarrow(A\\rightarrow (B\\rightarrow C)) &amp;\\textbf{Axiom 1} \\\\ &amp; {\\scr U}\\vdash A\\rightarrow (B\\rightarrow C) &amp;\\text{MP 2, 3} \\\\ &amp; {\\scr U}\\vdash (A\\rightarrow (B\\rightarrow C))\\rightarrow((A\\rightarrow B)\\rightarrow (A\\rightarrow C)) &amp;\\textbf{Axiom 2} \\\\ &amp; {\\scr U}\\vdash (A\\rightarrow B)\\rightarrow (A\\rightarrow C) &amp;\\text{MP 4, 5} \\\\ &amp; {\\scr U}\\vdash A\\rightarrow C &amp;\\text{MP 1, 6} \\end{aligned} \\end{aligned}\\] Exchange of antecedent Rule \\[\\begin{aligned} \\frac{ {\\scr U}\\vdash A\\rightarrow(B\\rightarrow C)}{ {\\scr U}\\vdash B\\rightarrow (A\\rightarrow C)} \\end{aligned}\\] 证明如下： \\[\\begin{aligned} \\begin{aligned} &amp; \\left\\{A\\rightarrow(B\\rightarrow C),A,B\\right\\}\\vdash A\\rightarrow(B\\rightarrow C) \\\\ &amp; \\left\\{A\\rightarrow(B\\rightarrow C),A,B\\right\\}\\vdash A \\\\ &amp; \\left\\{A\\rightarrow(B\\rightarrow C),A,B\\right\\}\\vdash B\\rightarrow C \\\\ &amp; \\left\\{A\\rightarrow(B\\rightarrow C),A,B\\right\\}\\vdash B \\\\ &amp; \\left\\{A\\rightarrow(B\\rightarrow C),A,B\\right\\}\\vdash C \\\\ &amp; \\left\\{A\\rightarrow(B\\rightarrow C),B\\right\\}\\vdash A\\rightarrow C \\\\ &amp; \\left\\{A\\rightarrow(B\\rightarrow C)\\right\\}\\vdash B\\rightarrow(A\\rightarrow C) \\\\ &amp; \\vdash (A\\rightarrow(B\\rightarrow C))\\rightarrow(B\\rightarrow(A\\rightarrow C)) \\end{aligned} \\end{aligned}\\] 然后用这条定理对着前件MP一下就好了 Double negation Rule \\[\\begin{aligned} \\frac{ {\\scr U}\\vdash eg eg A}{ {\\scr U}\\vdash A},\\;\\frac{ {\\scr U}\\vdash A}{ {\\scr U}\\vdash eg eg A} \\end{aligned}\\] 证明如下： 引理： \\[\\begin{aligned} \\begin{aligned} &amp; \\left\\{ eg eg A\\right\\}&amp;\\vdash&amp; eg eg A\\rightarrow( eg eg eg eg A\\rightarrow eg eg A) &amp;\\textbf{Axiom 1} \\\\ &amp; \\left\\{ eg eg A\\right\\}&amp;\\vdash&amp; eg eg A \\\\ &amp; \\left\\{ eg eg A\\right\\}&amp;\\vdash&amp; eg eg eg eg A\\rightarrow eg eg A &amp;\\text{MP 1, 2} \\\\ &amp; \\left\\{ eg eg A\\right\\}&amp;\\vdash&amp; eg A\\rightarrow eg eg eg A &amp;\\text{Contrapositive Rule} \\\\ &amp; \\left\\{ eg eg A\\right\\}&amp;\\vdash&amp; eg eg A\\rightarrow A &amp;\\text{Contrapositive Rule} \\\\ &amp; \\left\\{ eg eg A\\right\\}&amp;\\vdash&amp; A &amp;\\text{MP 2, 5} \\\\ &amp; &amp;\\vdash&amp; eg eg A\\rightarrow A &amp;\\text{Deduction Rule} \\end{aligned} \\end{aligned}\\] 因此 \\[\\begin{aligned} \\begin{aligned} &amp; {\\scr U}\\vdash eg eg A \\\\ &amp; {\\scr U}\\vdash eg eg A\\rightarrow A \\\\ &amp; {\\scr U}\\vdash A \\end{aligned} \\end{aligned}\\] 另一个的证明是类似的 Reductio ad absurdum 也就是所谓的归谬法 即 \\[\\begin{aligned} \\frac{ {\\scr U}\\vdash eg A\\rightarrow false}{ {\\scr U}\\vdash A} \\end{aligned}\\] 规定 \\(false \\overset{def}=A\\wedge eg A\\overset{def}= eg(A\\rightarrow A)\\)，\\(true\\overset{def}=A\\vee eg A\\overset{def}=A\\rightarrow A\\)，注意这是语法上的定义，即两侧可以等价替换。 那么有 \\(\\vdash eg false\\)，且 \\(\\vdash true\\)，这个证明是显然的。 \"证明\"： \\[\\begin{aligned} \\begin{aligned} &amp; {\\scr U}\\vdash eg A\\rightarrow false \\\\ &amp; {\\scr U}\\vdash eg false\\rightarrow eg eg A &amp;\\text{Contrapositive Rule} \\\\ &amp; {\\scr U}\\vdash eg false &amp;\\text{By Lemma} \\\\ &amp; {\\scr U}\\vdash eg eg A &amp;\\text{MP 2, 3} \\\\ &amp; {\\scr U}\\vdash A &amp;\\text{Double negation Rule} \\end{aligned} \\end{aligned}\\] Commutativity \\[\\begin{aligned} \\vdash A\\vee B\\rightarrow B\\vee A \\end{aligned}\\] 在 \\(\\scr H\\) 中没有对 \\(\\vee \\wedge\\) 的直接讨论，通常通过定义 \\(A\\vee B\\overset{def}= eg A\\rightarrow B\\) 和 \\(A\\wedge B\\overset{def}= eg(A\\rightarrow eg B)\\) 来将其转化为 \\( eg \\rightarrow\\) 符号集上的证明。这也说明了 \\(\\left\\{ eg,\\rightarrow\\right\\}\\) 是完备集 交换律的证明是简单的，但是后面 \\(\\scr H\\) 的正确性证明需要用到这个。 Weakening \\[\\begin{aligned} \\begin{aligned} &amp;\\vdash A\\rightarrow A\\vee B \\\\ &amp;\\vdash A\\rightarrow B\\vee A \\\\ &amp;\\vdash (A\\rightarrow B)\\rightarrow((C\\vee A)\\rightarrow(C\\vee B)) \\end{aligned} \\end{aligned}\\] 证明也是简单的，只需要按定义换掉 \\(\\vee\\) 就可以了，第三条是MP的简单应用 Associativity \\[\\begin{aligned} \\begin{aligned} &amp;\\vdash A\\vee(B\\vee C)\\rightarrow (A\\vee B)\\vee C \\\\ &amp;\\vdash (A\\vee B)\\vee C\\rightarrow A\\vee(B\\vee C) \\end{aligned} \\end{aligned}\\] 证明需要用一下交换律，然后推就好了 Distributivity \\[\\begin{aligned} \\begin{aligned} &amp;\\vdash A\\vee(B\\wedge C)\\leftrightarrow (A\\vee B)\\wedge (A\\vee C) \\\\ &amp;\\vdash A\\wedge(B\\vee C)\\leftrightarrow (A\\wedge B)\\vee (A\\wedge C) \\end{aligned} \\end{aligned}\\] 这里规定 \\(\\vdash A\\leftrightarrow B\\) 当且仅当 \\(\\vdash A\\rightarrow B\\) 且 \\(\\vdash B\\rightarrow A\\) 这里是 \\(\\scr H\\) 中语法上的De morgan Law，在没有证明 \\(\\scr H\\) 的正确性之前是不能由语义上得到的。 这个本菜菜还没想到要怎么证，好像也没找到要怎么证/(ㄒoㄒ)/ \\(\\scr H\\) 的正确性 Soundness 对于 \\(A\\in{\\scr H}\\)，若 \\(\\vdash A\\)，那么 \\(\\models A\\) 即证明所有 \\(\\scr H\\) 可证明的公式都永真，只需要对 \\(A\\) 的证明步骤数 \\(n\\) 做归纳即可。这里可以仅考虑原始的三条公理+MP的 \\(\\scr H\\)，因为所有后面的定理和规则都可以规约回去。 \\(n=1\\)，此时 \\(A\\) 是三公理之一，构建 \\( eg A\\) 的Semantic Tableaux即可发现都是closed的，因此 \\(\\models A\\) \\(n&gt;1\\)，此时 \\(A\\) 经由MP得到，前面必然有 \\(\\vdash B\\rightarrow A\\)，\\(\\vdash B\\)。根据归纳假设有 \\(\\models B\\rightarrow A\\)，\\(\\models B\\)，根据 $$ 的语义即得 \\(\\models A\\)。注意我们这里在讨论 \\(\\models\\)，因此可以用语义上的定义来进行证明。 Completeness 对于 \\(A\\in{\\scr H}\\)，若 \\(\\models A\\)，那么 \\(\\vdash A\\)。注意到 \\(\\scr G\\) 是sound&amp;complete的，因此可以通过 \\(\\scr G\\) 作为媒介证明 引理1：若 \\(\\vdash{\\scr U}\\) in \\(\\scr G\\)，那么 \\(\\vdash \\bigvee{\\scr U}\\) in \\(\\scr H\\) 理解引理1是很简单的，只需要注意到 \\(\\scr G\\) 中的定理是一个隐含的 \\(\\bigvee\\) 公式，同时结合 \\(\\vdash A\\) 当且仅当 \\(\\models A\\) in \\(\\scr G\\) 就好了。 引理2：若 \\({\\scr U&#39;}\\subseteq{\\scr U}\\)，\\(\\vdash \\bigvee{\\scr U&#39;}\\) in \\(\\scr H\\)，那么 \\(\\vdash\\bigvee{\\scr U}\\) in \\(\\scr H\\) 之所以提出引理2是因为我们很难直接获得整个公式集 \\(\\scr U\\) 构成的公式在 \\(\\scr H\\) 中的推导。但是注意到 \\(\\scr G\\) 中的公理都有互补对，而互补对是我们可以在 \\(\\scr H\\) 中推导的，因此考虑证明这个弱化前提的引理2。 引理3：\\(\\vdash A\\rightarrow(B\\rightarrow A\\wedge B)\\) 引理2的证明： 由 \\(\\vdash\\bigvee{\\scr U&#39;}\\)，可以得到 \\(\\scr H\\) 中的一个证明。又根据 Weakening Theorem，可以将 \\(\\scr U-\\scr U&#39;\\) 中的公式一个个加上得到 \\(\\vdash\\bigvee{\\scr U&#39;&#39;}\\)，其中 \\(\\bigvee\\scr U&#39;&#39;\\) 是 \\(\\bigvee\\scr U\\) 的一个排列，这一步可以对 \\(|\\scr U-U&#39;|\\) 作数学归纳法。 又根据 Commutativity Theorem可以交换 \\(\\bigvee\\scr U&#39;&#39;\\) 中的任意两个相邻的公式，因此可以排序得到 \\(\\vdash\\bigvee\\scr U\\)，这一步可以给公式编号，然后对逆序对作数学归纳法。 引理1的证明可以对 \\(\\scr U\\) 在 \\(\\scr G\\) 中的推导用结构归纳法： \\(\\scr U\\) 是 \\(\\scr G\\) 中公理，因此存在 \\({\\scr U&#39;}=\\left\\{p, eg p\\right\\}\\subseteq{\\scr U}\\)。此时显然有 \\(\\vdash\\bigvee \\scr U&#39;\\) in \\(\\scr H\\)，根据引理2就有 \\(\\vdash\\bigvee \\scr U\\) in \\(\\scr H\\) \\(\\scr U\\) 在 \\(\\scr G\\) 中经过至少一步推导得到，不妨记新增公式为 \\(\\phi\\)，记 \\({\\scr U}=\\left\\{\\phi\\right\\}\\cup {\\scr V}\\) \\(\\phi=\\alpha_1\\vee\\alpha_2\\)，记 \\({\\scr U_1}={\\scr V}\\cup\\left\\{\\alpha_1,\\alpha_2\\right\\}\\)，则根据归纳假设有 \\(\\vdash\\bigvee{\\scr U_1}\\) in \\(\\scr H\\)，也就是 \\(\\vdash(\\bigvee{\\scr V})\\vee\\alpha_1\\vee\\alpha_2\\)。根据 Associativity Theorem即得 \\(\\vdash(\\bigvee{\\scr V})\\vee(\\alpha_1\\vee\\alpha_2)\\)，也就是 \\(\\vdash\\bigvee\\scr U\\) in \\(\\scr H\\) \\(\\phi=\\beta_1\\wedge\\beta_2\\)，记\\({\\scr U_1}={\\scr V_1}\\cup\\left\\{\\beta_1\\right\\}\\)，记 \\({\\scr U_2}={\\scr V_2}\\cup\\left\\{\\beta_2\\right\\}\\)，则根据归纳假设有 \\(\\vdash\\bigvee{\\scr U_1},\\;\\vdash\\bigvee{\\scr U_2}\\) in \\(\\scr H\\)，也就是 \\(\\vdash(\\bigvee{\\scr V_1})\\vee\\beta_1,\\;\\vdash(\\bigvee{\\scr V_2})\\vee\\beta_2\\) \\[\\begin{aligned} \\begin{aligned} &amp; \\vdash \\bigvee{\\scr V_1}\\vee\\beta_1 \\\\ &amp; \\vdash eg\\bigvee{\\scr V_1}\\rightarrow \\beta_1&amp;\\text{By Definition of $\\vee$} \\\\ &amp; \\vdash \\beta_1\\rightarrow(\\beta_2\\rightarrow\\beta_1\\wedge\\beta_2)&amp;\\text{Lemma 3} \\\\ &amp; \\vdash eg\\bigvee{\\scr V_1}\\rightarrow(\\beta_2\\rightarrow \\beta_1\\wedge\\beta_2)&amp;\\text{By Transitivity} \\\\ &amp; \\vdash \\beta_2\\rightarrow( eg\\bigvee{\\scr V_1}\\rightarrow\\beta_1\\wedge\\beta_2)&amp;\\text{Exchange of Antecedent Rule} \\\\ &amp; \\vdash \\bigvee{\\scr V_2}\\vee\\beta_2&amp;\\text{} \\\\ &amp; \\vdash eg\\bigvee{\\scr V_2}\\rightarrow\\beta_2&amp;\\text{By Definition of $\\vee$} \\\\ &amp; \\vdash eg\\bigvee{\\scr V_2}\\rightarrow( eg\\bigvee{\\scr V_1}\\rightarrow\\beta_1\\wedge\\beta_2)&amp;\\text{By Transitivity} \\\\ &amp; \\vdash (\\bigvee{\\scr V_2})\\vee(\\bigvee{\\scr V_1})\\vee(\\beta_1\\wedge\\beta_2)&amp;\\text{By Definition of $\\vee$} \\end{aligned} \\end{aligned}\\] 这样就证明了 \\(\\vdash \\scr U\\) in \\(\\scr H\\) 最后考虑怎么证明 \\(\\models A\\) 则 \\(\\vdash A\\) in \\(\\scr H\\) 构造 \\({\\scr U}=\\left\\{A\\right\\}\\)，\\(\\models A\\) 当且仅当 \\(\\vdash \\scr U\\) in \\(\\scr G\\) 当且仅当 \\(\\vdash \\bigvee\\scr U\\) in \\(\\scr H\\) 当且仅当 \\(\\vdash A\\) in \\(\\scr G\\) 于是就证明了 \\(\\scr H\\) 是Sound &amp; Complete的 一致性 称公式集 \\(\\scr U\\) 是不一致的，当且仅当存在公式 \\(A\\) 使得 \\({\\scr U}\\vdash A\\) 并且 \\({\\scr U}\\vdash eg A\\) 称 \\(\\scr U\\) 是一致的，当且仅当它不是不一致的 若 \\(\\scr U\\) 是不一致的，那么对于任意公式 \\(A\\) 都有 \\({\\scr U}\\vdash A\\) 证明就不写了咕咕咕咕 由此可以得到一个很有用的定理，\\({\\scr U}\\vdash A\\) 当且仅当 \\({\\scr U}\\cup\\left\\{ eg A\\right\\}\\) 不一致 Consistency &amp; Completeness 前文关于命题逻辑 \\({\\scr H}\\) 的完全性证明依赖于 \\({\\scr G}\\) 的完全性证明，而 \\({\\scr G}\\) 的特殊结构使得它的完全性证明可以比较简单地由归纳得到。因此这里实际上是走了一个小捷径的。 利用一致性可以给出完全性的另一定义，即下列命题等价： 若 \\(\\Gamma\\) 一致则 \\(\\Gamma\\) 可满足 若 \\(\\Gamma\\models \\phi\\) 则 \\(\\Gamma\\vdash\\phi\\) 因此完全性证明只需要针对(1)的情况即可。 假定有一个一致的公式集 \\(\\Gamma\\)，我们可以通过一些手段对其扩充，得到极大一致的公式集 \\(\\Gamma&#39;\\)。 然后我们根据 \\(\\Gamma&#39;\\) 可以构造一个赋值/指派，从而可以证明 \\(\\Gamma&#39;\\) 在这个赋值下可满足，自然有 \\(\\Gamma\\) 可满足，于是命题得证。","tags":["Mathematical Logic"]},{"title":"数理逻辑01 命题逻辑","path":"/2022/01/20/Logic01-命题逻辑/","content":"写在前面 第一次看这本书的时候看得比较急，也没有一个big picture的把握，所以在细节上面耗费了很多时间....现在算是重构一次笔记了 我们知道，形式逻辑是对推理的形式化（mathematical logic formalizes resoning），为了描述推理我们有各种各样的逻辑系统。对于一个逻辑系统，最关键的就是它的语法（Syntax）和语义（Semantics）。其中语法决定了逻辑系统中讨论的对象长什么样，而语义决定了我们如何解释这些逻辑系统中被研究的对象。 比如说我们有研究命题的命题逻辑（Propositional Logic）、包含了函数 算术的一阶逻辑（First Order Logic）、对真值进行扩充得到的模态逻辑等等。这些逻辑的区别就在于它们的语法和语义。 同时，在熟悉这些逻辑系统的同时，还要分清什么是我们的研究对象。因为逻辑在研究推理的形式化，因此需要区分什么是逻辑系统中的形式化推理（研究对象），什么是我们对于研究对象的推理。后者被成为元逻辑、元语言，前者就是对象逻辑、对象语言了。所有的对象逻辑都应根据相应的逻辑系统的定义赋予其含义，而在元语言的层面则可以随意一些，就是正常的平时推理。 本书是写给CSer的数理逻辑教材，因此会专门讲不同逻辑系统中的一类算法（Decision Procedure）的构造和正确性证明，这些是纯数不太感兴趣的东西，也是这本书多出来的东西。同时在看的时候，还要尤其注意什么是基础、什么是组合构造方法、这些逻辑对哪些东西做了抽象，这三板斧应该刻在DNA里。 废话不多说 命题逻辑 命题逻辑是比较常见易懂的逻辑，它主要关心在给出若干基本命题（atoms）之后，如何通过组合小命题来获得大命题、如何给组合命题递归地赋予含义（Compositionality），因此用树形结构来展示这样的命题构造就是很自然的了。 语法 给出BNF \\[\\begin{aligned} {stmt = atom\\;|\\;( eg\\;stmt)\\;|\\;(stmt_1\\;op\\;stmt_2)} \\end{aligned}\\] 为了方便，引入语法符号 \\(\\top,\\bot\\)，规定任意解释下都有 \\({\\mathscr I}(\\bot)=F,{\\mathscr I}(\\top)=T\\) 通常记所有公式组成的集合为 \\(\\mathscr F\\) 语义 定义公式 \\(A\\) 的一个解释 \\(\\mathscr I\\) 为一个函数 \\(U_A\\mapsto \\left\\{T,F\\right\\}\\)，其中 \\(U_A\\) 为公式 \\(A\\) 中包含的所有原子所组成的集合 通过 \\({\\mathscr I}(A_1)\\) 和 \\({\\mathscr I}(A_2)\\) 给出 \\({\\mathscr I}(A_1\\;op\\;A_2)\\) ，以此来定义命题构造子（别人都叫他运算符，但我更喜欢这么说）\\(op\\) 的含义 若在某个解释 \\(\\scr I\\) 下公式 \\(A\\) 满足 \\({\\scr I}(A)=T\\)，我们就说 \\(A\\) 是可满足的（satisfiable），此时的 \\({\\scr I}\\) 是 \\(A\\) 的一个模型（Model） 若在任意解释 \\({\\scr I}\\) 下公式 \\(A\\) 满足 \\({\\scr I}(A)=T\\)，我们就说 \\(A\\) 是永真的（valid），记作 \\(\\models A\\) 类似可以定义永假的（即不可满足的 unsatisfiable）、可假的（fasifiable）记作 \\( ot\\models A\\)，在某些操作下这四种性质可以互相转化 需要注意的是，我们在命题逻辑中讨论的全都是命题变元（语法上的公式），而赋予其真值是解释做的事情（语义上的含义）。这也是为什么我没有通过给“运算符”列真值表来定义，这里的定义完全是基于解释的。 定义和定理 公式组成的集合的解释 记 \\({\\scr U}=\\left\\{A\\right\\}\\) 为公式的集合（在这里我们忽略无穷集，因为对于无穷的二元运算没有定义） 若在某个解释 \\({\\scr I}\\) 下有 \\({\\scr I}(A)=T\\) 对 \\({\\scr U}\\) 中的每个公式都成立，则称 \\({\\scr U}\\) 是可满足的 若在任意解释 \\({\\scr I}\\) 下都存在 \\({\\scr U}\\) 中的某个公式 \\(A\\) 使得 \\({\\scr I}(A)=T\\)，则称 \\({\\scr U}\\) 是不可满足的 容易发现这个定义就是在讨论 \\(\\bigwedge_{A\\in {\\scr U} } A\\) 在解释 \\({\\scr I}\\) 下的真值 运算符 一个 \\(n\\) 元运算实际上是 \\(\\left\\{T,F\\right\\}^n\\mapsto \\left\\{T,F\\right\\}\\) 的一个函数，因此有 \\(\\scr F\\) 上的本质不同的 \\(n\\) 元运算有 \\(2^{2^n}\\) 种。 在结构归纳法中我们需要讨论所有形式的公式（即，在每一种运算下产生的所有公式），非常麻烦。一种想法是找出尽可能基本的运算，在此之上构造剩余的运算。 定义运算 \\(\\circ\\) 能被 \\(O=\\left\\{\\circ_1,\\circ_2\\ldots\\circ_n\\right\\}\\) 表示，当且仅当对于任意 \\(\\scr F\\) 中的公式 \\(A,B\\)，都有 \\(A\\circ B=C_X\\circ_Y C_X\\cdots C_X\\) 其中 \\(C_X\\in\\left\\{A,B\\right\\}\\)，\\(\\circ_Y\\in O\\) 特殊地，对于单目运算符 \\(\\sim\\)，我们修改定义为 \\(\\sim A=A\\circ_Y A\\cdots A\\)，其中 \\(\\circ_Y\\in O\\) 定义运算集 \\(O\\) 是完备的（Adequate），当且仅当所有运算都可被 \\(O\\) 表示。学过数电都知道 NAND和NOR 可以搭出所有电路，在逻辑系统中也有这样的性质，证明只需要简单的构造一下就好了。常用的完备集是 \\(\\left\\{\\wedge,\\vee, eg\\right\\}\\) 定理：二元运算的最小完备集只可能是 \\(\\uparrow\\) 或 \\(\\downarrow\\)，即NAND或NOR。具体的证明可以看后面的习题合辑（如果我没有咕咕咕的话） 等价、逻辑后承 定义 \\(\\mathscr F\\) 上的二元关系 \\(\\equiv\\) 逻辑等价（Logical Equivalence）为： \\(A_1\\equiv A_2\\) 当且仅当在任意解释下，有 \\({\\mathscr I}(A_1)={\\mathscr I}(A_2)\\) 定义逻辑后承（Logical Consequence）的含义为： \\({\\scr U}\\models A\\) 当且仅当在所有使得 \\({\\scr U}\\) 可满足的解释 \\(\\scr I\\) 下，都有 \\({\\scr I}(A)=T\\) 我个人觉得也可以叫语义后承 \\(A\\leftrightarrow B\\) 永真当且仅当 \\(A\\equiv B\\) \\(\\bigwedge_{A\\in {\\scr U} }A\\rightarrow B\\) 永真 当且仅当 \\({\\scr U}\\models B\\) 这两个定理实际上是为命题逻辑系统中的语法符号作出了解释，即我们可以用一些逻辑系统内部满足的性质来代替元语言的描述 子公式、替换 子公式仍然是 \\(\\scr F\\) 上的二元关系，在不影响上下文理解的时候，我们将把 \\(A\\) 是 \\(B\\) 的子公式简记作 \\(A\\subseteq B\\) 子公式的严格定义可以通过公式作为树结构导出，只需要照抄子树的定义就好 同样在树结构上操作，我们可以将一棵子树替换为另一棵子树，在公式中就表现为将一段子公式替换为另一个公式，记作 \\(A\\left\\{B\\leftarrow C\\right\\}\\)，其中 \\(B\\subseteq A\\)，解释为 把 \\(A\\) 中的所有子公式 \\(B\\) 替换为公式 \\(C\\) 若 \\(B\\equiv C\\) 且 \\(B\\subseteq A\\)，则有 \\(A\\left\\{B\\leftarrow C\\right\\}\\equiv A\\) 成立 具体的证明可以通过对树形结构来归纳做。 文字、互补对 这个翻译很怪 对于原子 \\(p\\)，我们把 \\(p\\) 和 \\( eg p\\) 称为一对文字（Literals），其中 \\(p\\) 是正文字（Positive），\\( eg p\\) 是负的（Negative） 同一个原子的两个文字组成一对互补对（Complementary Pair） 若公式集 \\({\\scr U}\\) 存在一对互补对，即存在 \\(p, eg p\\in {\\scr U}\\)，当且仅当 \\({\\scr U}\\) 不可满足 这是由定义即得的。互补对定理使得我们可以构造出一种正确的Decision Procedure 理论 若公式集 \\(\\scr F\\) 在 \\(\\models\\) 二元关系下满足封闭性，则称 \\(\\scr F\\) 是一个理论（Theory），\\(\\scr F\\) 中的公式为定理（Theorem） 对于一个理论 \\(\\scr F\\)，若存在 \\(\\scr U\\subseteq F\\) 使得 \\({\\scr F=}\\left\\{A|{\\scr U}\\models A\\right\\}\\)，则称 \\(\\scr F\\) 是可公理化的（Axiomatizable），\\(\\scr U\\) 是 \\(\\scr F\\) 的一组公理。 需要注意的是，这里对公理集的大小没有限制。考虑皮亚诺公理系统，我们对自然数集中的每个元素都作出了公理化的定义，因此皮亚诺公理系统的公理集合实际上是由一个Axiom Scheme产生的，我们把这个单独的Scheme作用到每个元素上就得到了无穷多个公理。 Decision Procedure 对于 \\(\\scr F\\) ，一个算法 \\({\\scr F}\\mapsto \\left\\{T,F\\right\\}\\) 被称为是一个Decision Procedure。记使其输出为 \\(T\\) 的公式集合为 \\(\\scr U\\)，显然有 \\(\\scr U\\subseteq F\\)，我们称这个算法是 Decision Procedure on \\(\\scr U\\) 通常我们会关注 Decision Procedure on Validity，即给定一个公式判断它是否永真 Semantic Tableaux ST是一种Decision Procedure，它能判定一个公式是否可满足 构造 令根节点的标记为由待判定的公式 \\(A\\) 组成的集合 \\(\\left\\{A\\right\\}\\)，记为 \\(label(l)=\\left\\{A\\right\\}\\) 选取一个未被标记为open或closed的叶子 \\(l\\)，若不存在则前往步骤8 若 \\(label(l)\\) 全是文字，则如果存在互补对，就标记为closed leaf并回到步骤2；如果不存在就标记为open leaf并回到步骤2。如果不全是文字，则前往步骤4 记 \\(label(l)\\) 中不是文字的公式为 \\(W\\) 若 \\(W\\) 形如 \\(A_1\\wedge A_2\\)，则构造 \\(l\\) 的儿子 \\(l&#39;\\)，其标记为 \\(label(l&#39;)=label(l)-\\left\\{W\\right\\}+\\left\\{A_1,A_2\\right\\}\\)，回到步骤2 若 \\(W\\) 形如 \\(A_1\\vee A_2\\)，则构造 \\(l&#39;,l&#39;&#39;\\)，其中 \\(label(l&#39;)=label(l)-\\left\\{W\\right\\}+\\left\\{A_1\\right\\}\\)，\\(label(l&#39;&#39;)=label(l)-\\left\\{W\\right\\}+\\left\\{A_2\\right\\}\\)，回到步骤2 若 \\(W\\) 形如 \\( eg A\\)，则根据De Morgan Law化进去，回到4 如果所有叶子都是closed，则称这棵树也是closed的，输出不可满足；否则称这棵树是open的，输出可满足 Termination 记公式 \\(A\\) 所含二元运算的数量为 \\(b(A)\\)，单目运算数量为 \\(s(A)\\)，定义 \\(b({\\scr U})=\\sum\\limits_{u\\in{\\scr U} }b(u)\\)，\\(s({\\scr U})=\\sum\\limits_{u\\in{\\scr U} }s(u)\\) 对于节点 \\(l\\)，构造其势能函数为 \\(W(l)=b(label(l))+s(label(l))\\)，则有如下性质： 任意树上的节点 \\(l\\) 都有 \\(W(l)\\geqslant 0\\)。这是因为 \\(b(\\cdot),s(\\cdot)\\) 都是非负的 任意树上的节点 \\(l\\) 及其子节点 \\(l&#39;,l&#39;&#39;\\)，都有 \\(W(l)&gt; W(l&#39;),W(l)&gt; W(l&#39;&#39;)\\)。这个只需要针对步骤5、6、7分别讨论就好了 因此上述构造必然终止。并且因为必然终止，所以每个叶子要么open，要么closed 正确性 对于Decision Procedure的正确性要分成Soundness和Completeness两方面证明 Soundness 即证明：如果Semantic Tableaux closed，那么公式不可满足 即证明：如果算法claim公式 \\(A\\) 是不可满足的，那么 \\(A\\) 确实是不可满足的 Completeness 即证明：如果公式不可满足，则所有的Semantic Tableaux都closed 即证明：如果公式 \\(A\\) 不可满足，则上述算法构造出来的所有Semantic Tableaux都closed，算法永远claim \\(A\\) 不可满足 证明就咕咕咕啦。这样，这一章就看完啦 Compactness 讨论的是任意大小的公式集 \\(\\scr U\\) 的可满足性 定义使得性质 \"任意 \\({\\scr S}\\subseteq {\\scr U}\\) 若 \\(|{\\scr S}|\\in\\mathbb N\\) 那么 \\({\\scr S}\\) 可满足\" 成立的集合 \\(\\scr U\\) 为有限可满足公式集。有限可满足的公式集的所有有限子集都可满足。那么有定理：若 \\(\\scr U\\) 有限可满足，那么 \\(\\scr U\\) 仍然可满足。 首先取有限可满足集 \\(\\Gamma_0\\)。注意到公式集 \\(\\scr F\\) 是可数集，因此可以构造公式序列 \\({\\scr F}=\\left\\{F_1,F_2,F_3\\ldots\\right\\}\\) 定义若 \\(\\Gamma_{n}\\cup\\{F_{n+1}\\}\\) 可满足，那么 \\(\\Gamma_{n+1}=\\Gamma_n\\cup\\{F_{n+1}\\}\\)，否则 \\(\\Gamma_{n+1}=\\Gamma_{n}\\cup\\{ eg F_{n+1}\\}\\) 有如下claim：\\(\\Gamma_n\\) 的存在性对于任意的正整数 \\(n\\) 成立。由反证法，假设存在最小的 \\(n&#39;\\) 使得 \\(\\Gamma_{n&#39;}\\cup\\{F_{n&#39;+1}\\}\\) 与 \\(\\Gamma_{n&#39;}\\cup\\{ eg F_{n&#39;+1}\\}\\) 都不是有限可满足公式集，则根据定义存在 \\(\\Delta_1,\\Delta_2\\subseteq \\Gamma_{n&#39;}\\) 使得 \\(\\Delta_1\\cup\\{F_{n&#39;+1}\\}\\) 与 \\(\\Delta_2\\cup\\{ eg F_{n&#39;+1}\\}\\) 都不是可满足的。又因为 \\(\\Delta_1,\\Delta_2\\) 是 \\(\\Gamma_{n&#39;}\\) 的子集，且 \\(\\Gamma_{n&#39;}\\) 有限可满足，因此存在解释 \\(\\scr I\\) 使得 \\({\\scr I}\\models \\Delta_1\\cup\\Delta_2\\) 取这个解释 若 \\({\\scr I}\\models F_{n&#39;+1}\\)，那么 \\({\\scr I}\\models \\Delta_1\\cup\\{F_{n&#39;+1}\\}\\) 若 \\({\\scr I}\\models eg F_{n&#39;+1}\\)，那么 \\({\\scr I}\\models\\Delta_2\\cup\\{ eg F_{n&#39;+1}\\}\\) 无论如何都与假设矛盾，因此假设不成立； 此时再构造 \\(\\Delta=\\bigcup\\limits_{i=0}^{\\infty} \\Gamma_{i}\\)，则任取 \\(\\Delta\\) 的有限子集 \\(S\\)，其中都存在下标最大的公式 \\(F_{max}\\in S\\)，那么根据定义有 \\(S\\subseteq\\Gamma_{max}\\subseteq\\Delta\\)。由 \\(\\Gamma_{max}\\) 有限可满足即得 \\(S\\) 有限可满足。由 \\(S\\) 的任意性即得 \\(\\Delta\\) 有限可满足 此时取所有原子命题的集合 \\({\\scr P}\\)，有 \\({\\scr P}\\) 中的任意原子 \\(p\\)，要么 \\(p\\in\\Delta\\)，要么 \\( eg p\\in\\Delta\\)，且二者不同时成立。 不同时成立只需要反证一下，结合 \\(\\Delta\\) 有限可满足，且 \\(\\{p, eg p\\}\\) 不可满足就能得到 同样反证 \\(p ot\\in\\Delta\\) 且 \\( eg p ot\\in\\Delta\\)，由 \\(p\\in{\\scr F}\\) 可知存在自然数 \\(k\\) 使得 \\(F_k=p\\)，构造一下 \\(\\Gamma_{k+1}\\) 就会发现必然有 \\(p\\in\\Gamma_{k+1}\\) 或 \\( eg p\\in\\Gamma_{k+1}\\)，由\\(\\in\\) 符号的传递性即得 \\(p\\in\\Delta\\) 或 \\( eg p\\in\\Delta\\) 对 \\(\\Delta\\) 中出现的原子进行 \\(\\scr I\\) 的构造： 若 \\(p\\in\\Delta\\)，那么 \\({\\scr I}(p)=T\\) 否则必然有 \\( eg p\\in\\Delta\\)，此时规定 \\({\\scr I}(p)=F\\) 下面证明 \\({\\scr I}\\models \\Delta\\)，只需要对 \\(\\Delta\\) 中的公式做结构归纳即可。其实可以证明一个更强的结论：\\(A\\in\\Delta\\) 当且仅当 \\(\\models A\\) 命题对原子命题显然成立； 考虑 \\(A= eg B\\) 形式的命题 \\(A\\)： 由 \\(A= eg B\\in\\Delta\\) 可得 \\(B ot\\in\\Delta\\)，故 \\({\\scr I} ot\\models B\\)，即 \\({\\scr I}\\models eg B\\)，即 \\({\\scr I}\\models A\\) 若 \\(A= eg B ot\\in\\Delta\\)，那么 \\(B\\in\\Delta\\)，故 \\({\\scr I}\\models B\\)，即 \\({\\scr I} ot\\models eg B\\)，即 \\({\\scr I} ot\\models A\\) 考虑 \\(A=B\\wedge C\\) 形式的命题 \\(A\\)： 必然有 \\(B,C\\in\\Delta\\)，否则不妨假设 \\(B ot\\in\\Delta\\)，则 \\(\\{ eg B,A\\}\\subseteq\\Delta\\)，此时 \\(\\{ eg B,A\\}\\) 不可满足，这与 \\(\\Delta\\) 有限可满足矛盾； 由归纳假设，\\({\\scr I}\\models\\{B,C\\}\\)，故 \\({\\scr I}\\models A\\) \\(A=B\\vee C,A=B\\rightarrow C\\) 同理....","tags":["Mathematical Logic"]},{"title":"大二上躺平经验","path":"/2022/01/11/大二上躺平经验/","content":"大二上躺平经验 2021年的记忆都停留在了五六月份...现在不是很想回忆，但是那段时间确实又欣喜又自卑，随后的七八月一直在学校里过着昼夜颠倒的生活。再往后就没什么印象了，那之后也不怎么拍照，人是越活越不明白了 七八月 七月初的时候和辅导员申请全程留校，然后就来消息说广东解封了，于是连夜说服辅导员第二天回家，并且也确实回成了。 回家那天本来买的是下午的飞机，结果延误到傍晚，再接着就直接取消了——当天全禄口的航班都G了。于是当机立断买高铁票去常州坐飞机，然后成功在凌晨两点到了广州。回到家第二天就是社会实践，于是睡了两个小时就又出门了。现在想想也是太猛了 回来就开始集训，说是集训，其实因为禄口的事情就变成了网友开黑。一周四场，开始还改一改题目，后面就彻底佛了。八月初的时候买了一对音箱，在宿舍连夜看完了半泽直树、空我、W、迪迦和泽塔，然后就进了假面骑士的特摄大坑。 中途开始看软件分析的网课，前面还挺有意思，后面就是很具体的实现了。第一次写java，还专门找了本Head First看了一下恶补。然后还看了三章的龙书+80页的数理逻辑，不得不说英文书看起来确实很慢，只能是慢慢来了，毕竟也就是看着玩玩。 在八月的时候还买了一整箱的泡面和玉米肠，过上了一段不能说不糜烂的生活。同时还掌握了栏杆交易外卖的技能，唯一可惜的是没有在封校期间尝试过翻墙出门，不然就没啥遗憾的了。 后面的都不太记得了，咕咕咕 课程踩坑 还是挺多坑的，这学期好像要刷分也不容易，不过以后不打算跟着培养方案混了，也不用特别在乎本来要上的那些课吧，虽然本菜菜也不知道以后想干啥，还是很迷茫的o(╯□╰)o 近代史 4.0 娜娜是很有个性的老师，上课很有趣，值得一听，但是后面基本都是听同学做pre，也就比较无聊了 平时要交一次作业，小组做一次pre，有一次点名。最后考试不划重点（给了七个重大历史事件和某某重要人物讲话数据库） 然后考试非常赤鸡，大概六个名词解释，一个送分，一个要看书，还有一个是翻往年卷的截图记下来的，剩下的三个一个都没见过：意思是开考就没了15分 目前还没出分，不过我觉得已经无所谓了。像这种思政课，如果选不到红的话，随大流和大班一起上也不是最坏的选择——起码大家都差不多 体质健康与提升 3.85 老师人很好，第一次上体育课也有课间休息，最后一次课也可以不用去，不像别的课需要过去听理论 平时会讲一些健身小技巧，然后就是各种负担不大的趣味小训练，最后可以玩飞镖射箭，算是比较好玩的课了 平时要跑步打卡，不过我基本都摸了... 出分力，只能说体测实在太拉了（ 密码学 4.0 每学期的痛苦面具，这学期就是它 课程说不上难，光听他讲还是蛮有意思的。但是题目都比较难。平时也没有花时间去看，更别说做题了。前半学期基本上看懂套路就可以考不错了，但是后期基本上在讲一些公钥密码的具体操作，而没有证明很多东西，但是考试又要考，所以就没怎么学懂.... 平时的作业都是课本后面的习题，期中考试也是课本后习题的真子集。习题答案可能要找一找才能找到，不过找到了没有坏处，毕竟平时分全靠作业算 而且因为这门课必修，还不敢免，就没办法选矿院的ICS，从而只能去上课时阉割版的AI特供。不过从结果来看是差不多的，就没什么所谓了。 最后期末考开卷，但是题都不太会，有一道有原题的也没有抄对....只能说求放过了，我是真的没学懂 概率论 4.75 AI特供版概率论，大概上了浙大课本的一半。额外的还讲了一些集中不等式和组合计数的东西，不过都是一些套路 gw上课名言：“这个我不记得了，你们背下来就可以了。” 由此可见这门课程的考核难度 期末基本上都不是难题，平时作业也都比较好做，就是每周都写还要交电子稿就很麻烦，第一次很想要一个平板写作业，但是想想又还是算了，大概以后也不怎么会有要手写的作业了。 上课推导还算好，讲义也比较清楚，算是比较好的老师和课程了，拿高分很容易。 计算思维导论 4.0 SB课程。本来是计科的必修课，性质类似于导论课程，但是后面被下放成了公选课。为什么呢，相信大家心里都有数。 课程比较的无聊，对于大二的学生没有什么上的必要，也许给高中生听听还是可以的，毕竟也没什么干货。总共有三次作业，分别是py处理表格、py做KNN、py做词云三个有手就行的实验，没啥好说的，不过居然还要写实验报告。期末是开卷考试，可以带笔记本抄PPT，那就基本没什么了。不过竟然还出算法题.....这门课真的是通识课吗 不过这门课有两个学分，如果最后给分还行的话，水一水也不是不可以。 upd: 事实证明给分非常烂...这种课也没法差卷子，十分的无语了。 世界早期近代文明史 4.0 陈老师的课。陈老师是老先生，不会用手机，也没有课程群，更不上网课。上课会讲他的很多见闻，非常好玩。认真上课的基本是P社游戏玩家，去课堂上基本一抓一个准。平时也不点名、没有作业，最后是一次开卷考，唯一的难点大概在于需要记住考试时间+准备好课本，也就是第一次课和倒数第二次课一定要去，因为分别决定了你知不知道课本是啥、知不知道啥时候考试。 如果想要考高分，可以考前再认真翻翻课本，这样至少不用现场把整本书从头到尾看一遍——像我一样 结果最后分也不是特别好，可能是因为选这门课的文科生偏多？ 数据结构 4.6 写着写着都快忘了我还有这门课了 老赵的豪华摸鱼套餐。本来是六学时的理论+实验课，硬生生变成3学时+1学时网课 只能说老赵特别会摸，基本上数据结构能讲的上学期都讲了，这学期就开始讲什么splay、左偏树、SA这些东西，讲法也基本就是教教做法，也没有特别的证明。属于是不要期待太多的课程体验，因此我开学就激情免修了。 平时的作业基本上都有答案可以参考，中途有一次读错需求手写压缩软件的代码作业，国庆节写了一整天.... 期末仍然是老赵特色考卷，也就是出一些普及组-的题目来撑场子，没啥好说的，1h写完走人 软件分析 4.5 开学之后冲到了线下课程，感觉讲的没啥区别。实验多了两次，并且难度也不太一样，这次换上了船新版本Tai-e。实验和考试是五五开，最后考试也不是特别难，基本上熟读PPT就可以上80了。如果实验做的好，90+不是太难。 实验做起来主要是文档太长，代码要读很多才敢下手。有些API是上次用过的，下次再想不起来还得慢慢翻，这个也许是我IDE还用不溜的原因，是我的锅 值得一吹的是，7、8两次作业都是DDL当天写完的，事实证明并不是特别难，即使第7次因为一些错挂掉了不少分。 计算机系统基础 4.9 原谅我用全名，这门课算是这学期收获最大的课程了 我选的是AI特供版，一周只有3学时。本来就是冲着jyy去追星的，虽然最后他本人只来了一节课 理论课部分比较无聊，基本上就是把CSAPP又来了一次，在某些细节讲了很多没啥用的东西，又省掉了一些东西，就得到了这个版本的课本 比较值得一说的是实验课，PA的水准确实很高，自己写NEMU写NANOS-LITE的成就感也是满满的。只不过课程群的氛围有点怪，大家好像不太愿意实名讨论，也不敢提问，不愿意为了解决一些问题来放下所谓的尊严。而一些大佬也只是默默不说话，感觉缺少一点交流，只能自己默默做了 PA其实不需要选课也可以做，但是为了督促自己搞完还是选课比较好，这样不容易中途摸掉。 成绩的构成是60%PA+LAB，10%作业，30%期末。顺带一提，课本上很多错误，习题解答的题目反而是正确的...看不懂这种操作。期末考题量很大，而且到处都是坑，需要小心 顺便欢迎没选课的同学选课，欢迎选了课的同学来和本菜菜交流心得都是可以的~ 阅读 4.5 这学期一口气选了两门阅读，还在亚马逊上买了两本书，结果愣是一点都没看（有一本为了期中交作业，连着看了三天） 讲一个好玩的事情，某门阅读要小组合作分工作展示，结果我一直都没有加群，中途还在思考“为啥还没有人牵头建群”这回事，结果到了展示前三天才发现原来群早就建好了，PPT也已经整完了.....于是我就成了划水摸鱼的可耻人。好在助教姐姐表示可以由我来做会议记录，然后换一点平时分，还贴心地给了我往年的参考，那就没啥可说的了。 感觉这种东西除非是真的很感兴趣（大一的两本），否则就不怎么想动....","tags":["Eureka Moments"]},{"title":"PA4 附加关卡","path":"/2022/01/11/PA4-附加关卡/","content":"试验进度 我完成了全部必做内容，最后可以用F1、F2、F3分别玩nterm、pal和bird，默认初始打开的是nterm 思考题 为什么不叫\"内核进程\"? 可以发现，内核中所有正在执行的函数都共用一个虚拟地址空间——也就是内核地址空间，这是线程和进程最大的区别，因为每个进程都将会有自己的地址空间。 这也是为什么会说线程更轻，因为构造虚拟地址空间需要时间开销。 虚存管理中PIC的好处 可以只保存一份程序而将其映射到不同虚拟空间中。因为PIC可以被加载到任意地方执行，因此也就可以被任意映射到不同的虚拟地址空间中，这样就可以省去静态链接时耗费的共有代码所占用的空间。 理解分页细节 为什么只有20位？因为一页是4KiB，因此就是12位。页表只需要指出页到页的映射关系，因此只需要说出这是2^20页中的哪一页就好了，所以只需要20位 为什么要用物理地址？因为虚拟地址到物理地址需要翻译，并且非平凡的翻译需要依靠查阅页表来实现，所以如果CR3也用了虚拟地址，就会出现“访问自己的物理地址需要先知道自己的物理地址”的问题。当然这里的物理地址不是必须的，如果能够实现一个比较简单的不依赖页表的翻译（例如说把x映射到MAX-x这样的映射）也是可以的，这样仍然是虚拟地址，但是没有什么意义，也不够物理地址来的简单。 为什么不采用一级页表？注意到一个页表项是32位，也就是4字节，所以一页只能存放210=1024项。假设只有一级页表，那么就只能储存1024页，也就是只支持1024*4KiB=4MiB的物理地址。而如果采用两级，就可以做到210*210*4KiB=4GiB的物理内存了。 空指针真的是\"空\"的吗? 不是，只是因为0在虚拟地址空间中没有映射/有较高的访问权限，所以在进行地址转换的时候会产生异常，异常处理程序则会杀死越权访问的进程（也就是产生了段错误） 解引用的时候：获得变量的值-&gt;访问0地址-&gt;mmu进行地址转换-&gt;在页表中找不到/没有对应权限-&gt;引发异常，进入异常处理程序-&gt;进程被杀死 内核映射的作用 回忆课本上的虚拟地址空间的布局图就可以发现，每个进程都包含了内核部分的地址映射（高地址的部分留给操作系统内核） 这是很好理解的，在用户程序请求系统调用服务的时候，需要陷入内核态执行内核的代码。假如没有这一段内核映射的复制，那么就需要在trap的时候进行地址空间的切换，这样的开销是很大的。 native的VME实现 可以看到用到了hsearch_r，也就是一个利用了hash函数实现的查找表。因为map实际上就是一个映射关系，因此用hash实现虚拟页地址到物理页地址的映射就能很自然的实现了。在完成翻译层面的map之后，还要调用mmap来把物理地址和虚拟地址联系起来 中断和用户进程初始化 可以 时钟中断的特点是： 中断发生后，陷入前的处理器状态与返回后的处理器状态完全相同 此时已经进入用户态，ksp有值，即使sp是0也可以正确保存上下文，从而使得1的性质成立 所以无论在中断到来时有没有sp，都相当于没有中断，因此可以正常初始化 用户态和栈指针 只需要注意到用户虚拟地址空间和内核虚拟地址空间存在差异，并且内核栈必然不在用户地址空间内即可。因此如果栈指针在内核空间而不在用户空间，就说明当前有访问内核数据的权限，因此处于内核态；否则处于用户态。 如何在操作系统中实现fork()? 注意到状态机的“状态”由CPU+内存决定，而外部设备则不在考虑范围内 复制地址空间的映射 复制CPU状态 一些记录 12月1号就已经写完了PA3+Lab4+报告，实际上一直等到了1月6号才开始着手写PA4.2后面的内容，因为手册上明明白白写着“这是最难的Phase了” 记录几个重大bug吧 STRACE的bug 我之前在nanos-lite的syscall中添加过一段用于识别系统调用号、并输出对应参数的调试代码，这样就可以在OS层面观察用户程序的一些行为：例如申请了多少内存、打开了哪些文件。而将调用号转变为字符串的过程我是用数组实现的（就是实现Int-&gt;String的函数）。对于后面添加的一些系统调用没有及时增加对应的映射，就导致会以空指针为参数调用printf，这样就会在vsprintf_r里面挂掉。 我发现这个问题是因为在打开STRACE后，程序crash的地点发生了变化，于是就开始观察测试代码，通过给测试代码打断点的方法定位crash的地方，最后发现是调用gettimeofday之后引发了问题，原因是我没有给gettimeofday的调用号分配对应的字符串 loader的bug（1） 其实一共有两个，每个都是惊天地泣鬼神的bug（其实并不） 背景是这样的：在执行用户程序的时候，用户程序会通过brk系统调用来设置program break，以此向OS动态申请内存。我们的OS就需要通过mm_brk来动态申请物理页，然后把对应新的虚拟地址页映射到物理页上。 第一个bug是这么观察到的：在执行用户程序的时候，mm_brk会一直分配物理页直到物理页耗尽！通过STRACE可以发现它第一次就用一个非常大的参数调用了brk（大概是0x81.....），这就让nanos-lite拼了命也搞不出足够的内存（欲求不满啊） 问题出在哪里呢？通过ITRACE我也只能发现问题现场在malloc_r函数里。经过一番激烈的思想斗争，我就开始翻阅libc里面的malloc函数的具体实现。最后发现问题是这样的：malloc函数会通过一个全局变量来记录已经分配了多少空间，这个全局变量应该是.bss节的，出现这样的问题是因为我的loader没有正确的实现对.bss的处理——而在没有VME的时候是可以的。 回去看loader就会发现问题是什么了：虽然同样是PT_LOAD段，但是并不是所有的页都需要写入内容，只需要改一改就可以解决了。 loader的bug（2） 第二个问题其实发生在很后面了，是我在写完了PA4.3之后尝试运行bird时发现的bug 背景是这样的：bird调用了库函数hsearch来进行一些快速的映射和查找，而在这个库里面有一个全局变量__default_hash，用来记录一个默认的hash函数选择。按照常理，用户的代码段应该是0x40...的一个地址，但是这个全局变量在hsearch_r函数里被load出来之后，再用jal命令跳转一下，就跑到了一个0x00...的一个地址。我百思不得其解，就开始怀疑自己的ldst写错了 中途我尝试在BirdMain.cpp中输出一下这个变量——做法是extern ...，然后直接在用户程序里printf，这样就得到了正确的结果，并且也可以正常玩bird 因为这样的调试可能改动了代码段和数据段，因此就开始用readelf命令观察前后两份二进制文件。在写出了不同段的地址和__default_hash的地址之后就可以发现：这个变量位于数据段的最后四个字节！如果观察一下错误的跳转地址就可以发现，只有最高位那个4神秘消失了！ 于是就开始回头看loader，果然是loader写错了：我没有正确处理好最后一页的最后一个字节的读取问题，这样就刚好丢掉了__default_hash的最后一个字节，在小端机上就表现为最高位的4被吃掉了。 其实中途也怀疑过loader，不过当时想的是两个数据段相互重叠导致某些页被映射了两次，但是实际上段是不相交的，不存在这样的问题。 mstatus.MIE的bug 在PA4.3的时候加入了时钟中断，并且引入了中断屏蔽的机制。也就是说，在产生一个中断/异常之后，我们需要屏蔽CPU的中断引脚（通过mstatus的某两个bit实现）防止出现中断的嵌套，例如防止在进行上下文切换时，时钟中断到来使得正在恢复的上下文丢失之类的情况。 结合PA3中启动新进程的做法：我们通过在栈上放置一个上下文，然后将栈指针移动到这个栈上来引用上下文，并通过一段汇编代码来实现上下文的恢复。这就使得我们需要在上下文中恰当地实现mstatus的功能。注意到恢复上下文的途中仍然不能响应中断，而必须等到mret指令执行完才能恢复响应，因此mstatus的实现就很重要了。具体的不多说，这里我写错了，就导致了这样的问题。并且关掉时钟中断后不会复发、每次出错现场不确定，属于比较难调的bug trap.S的bug 这里的bug似乎不太好直说，不然就没有乐趣痛苦了 asm_trap算是整个PA比较核心的部分了，并且对它的修改也需要比较清楚了解整个系统的工作原理，也是最容易出bug的地方 具体的不多说，大概可以这么check你的实现： 对于非进程切换的trap，都有 进入状态==离开状态； 对于所有的进程切换的trap，都可以在对应的ecall指令中通过log获得对应时刻的上下文信息，然后肉眼check是否正确恢复。 形式化地说，就是每个进程有一个颜色，一个切换A-&gt;B就是一个颜色为A的左括号和一个颜色为B的右括号，我们希望任意时刻同颜色的括号构成的子序列是匹配的（左右括号匹配当且仅当颜色相同且对应硬件上下文完全一样） 这样一旦发现了上下文不匹配的情况，就可以立刻发现了。这里可以通过两个简单的用户程序来回yield观察一些行为，然后debug kcontext的bug 原本的上下文是统一存放在栈上的（新进程和内核线程在内核栈上，而中断到来时的用户上下文则放在用户栈上），在PA4中我们改变了这一点，通过一通操作使得在进入irq_handle之前，上下文先被储存到内核栈上。然后从irq_handle返回之后，先从内核栈上恢复上下文，再把指针恢复到用户栈上（如果是用户程序的话） 与上面出现的bug不同，这里是产生一个新的上下文需要的东西，因此不好用assert，需要想清楚再下手 注意到手册给出的实现做出了一些变量的合并，因此需要在kcontext和ucontext中注意上下文结构的构造，使得在返回时不会出现栈指针乱飞的情况。不过查出来也很简单，只需要对着trap.S慢慢看就好了","tags":["ICS"]},{"title":"密码学07 OWF&HC","path":"/2021/12/21/Crypto07-OWF-HC/","content":"这一章比较轻松，主要是科普一些东西，都是概念性的 One-way function 注意到密码学中的很多构造都是不对称的：我们希望加密是简单的而解密是困难的、我们希望PRG是很容易产生的但是很难被区分的……这些不对称性归根结底都可以通过OWF来导出，这个概念还是很抓住了关键的 但是到目前为止，是否存在OWF仍然是未知的....因此存在OWF本身就是一个密码学假设，当然目前种种迹象看来这么假设是没有太大问题的。 定义 我们称函数 \\(f\\colon\\left\\{0,1\\right\\}^*\\mapsto\\left\\{0,1\\right\\}^*\\) 是OWF，当且仅当它满足： 存在PPT算法 \\(M_f\\) 使得 \\(\\forall x\\in Dom(f): f(x)=M_f(x)\\) 对于任意的PPT算法 \\(\\cal A\\)，有 \\(Pr\\left[{\\cal A}(1^n,f(x))\\in f^{-1}(f(x))\\right]\\leqslant\\text{negl}(n)\\)，其中 \\(x\\overset{\\)}{0,1}^*$ 有几个值得注意的地方：首先OWF的计算非常容易，但是对于随机均匀选取的定义域中的比特串，将其映射过后再求它的任意一个原象就很难（不一定是单射，逆是一个集合） 这里对 \\(x\\) 的选取说明求逆的难度是平均意义上的，可以存在某些点非常好求的情况。并且只需要求得任意一个逆就可以了，不需要全部求得。 这里的hardness同样可以定义成game \\(\\text{Invert}_{\\cal A,f}(n)\\)，我们称一个函数是OWF当且仅当对于任意的PPT攻击者 \\(\\cal A\\) 都有 \\(Pr\\left[\\text{Invert}_{\\cal A,f}(n)=1\\right]\\leqslant\\text{negl}(n)\\) 成立 两个例子 因数分解 最著名的OWF当然要属乘法函数了。这个函数写出来是这样的 \\(f:\\mathbb N\\times\\mathbb N\\mapsto\\mathbb N,\\; f(x,y)=x\\cdot y\\)，通常会规定 \\(x,y\\) 是 \\(n\\text{-bit}\\) 质数 离散对数 也就是OI里面讲过的BSGS算法用来解决的问题，规定 \\(f:\\mathbb N\\times \\mathbb N\\mapsto\\mathbb N,\\; f(g,x)=g^x\\pmod p\\)，其中 \\(p\\) 是一个给定的 \\(n\\text{-bit}\\) 质数 Hard-core predicate 所谓predicate，就是一个值域为 \\(\\left\\{0,1\\right\\}\\) 的函数，记为 \\(hc\\left(\\cdot\\right)\\)，通常还要求 \\(hc(\\cdot)\\) 要满足可以被PPT算法计算出来 如果对于任意的PPT算法 \\(\\cal A\\)，都有 \\(Pr[{\\cal A}(1^n,f(x))=hc(x)]\\leqslant\\frac12+\\text{negl}(n)\\)，其中 \\(x\\overset{\\)}{0,1}^n$ 上面说到OWF的单向性是通过容易计算、难于求逆定义的，那么自然就要问：求逆的难度能不能量化？ 很显然OWF内部仍然有很多区别，例如说一个OWF可以是难以计算逆的，但是仍然泄漏了一些原象的信息（例如说你和女神每天的对话，虽然内容难以计算，但是永远以“呵呵我去洗澡了”这样的字眼结束） hc就是这样一个最低限度的\"防止泄漏\"的量化——我们关注某个函数是否能够隐藏最小单位的信息，也就是一个比特 需要注意的是，上述定义并不要求 \\(f\\) 是OWF，一个例子是定义 \\(f(x_1x_2\\ldots x_n)=x_2x_3\\ldots x_n\\)，很显然它的一个hc就是比特串的第一位，并且这个函数不是OWF（猜对的概率是\\(\\frac12\\)） Goldreich-Levin THeorem 如果OWF成立，那么存在一个OWF \\(g\\) 和一个 \\(hc\\)，使得 \\(hc\\) 是 \\(g\\) 的硬核谓词 证明咕咕咕，这个结论的意义更大一些，有这个结论就可以更充分地相信hc的存在性了 OWF for PRG 假设 \\(f\\) 是一个OWF，\\(hc\\) 是 \\(f\\) 的一个hc，那么 \\(G(s)=f(s)||hc(s)\\) 是一个PRG，expansion factor 为 \\(n+1\\) 证明可以通过反证法，则存在一个区分 \\(G(s)\\) 和 \\(r\\) 的 \\(\\cal A\\)，那么就可以通过这个 \\(\\cal A\\) 来构造一个区分 \\(hc(s)\\) 和真随机比特的 \\(\\cal A&#39;\\)，从而得到与 \\(hc\\) 性质的矛盾。 注意到这个只“生长”了一位，实际上可以生长出任意poly(n)的长度，构造如下： \\(\\hat G(s)=hc(s)||hc(f(s))||\\ldots||hc(f^k(s))||f^k(s)\\) 可以把最后的 \\(s\\) 看作某个内部状态，前缀都是由状态产生的一个结果，这个前缀的PRG性质则是通过hc来保证的 随机性的证明可以通过差分构造若干个后缀完全一样，前缀完全随机的假的PRG，然后证明它们两两无法区分，最后用三角不等式叠加起来就好了。根据 \\(\\text{negl}(n)\\) 的性质可知，拓展poly次之后不等式的右端仍然是一个 \\(\\text{negl}(n)\\)","tags":["Cryptography"]},{"title":"密码学06 HASH","path":"/2021/12/21/Crypto06-HASH/","content":"Cryptographic Hash Function 所谓Hash，就是一个实现“压缩”比特序列的函数（有时也叫摘要）。与普通的散列和哈希不同，密码学上的hash要求要更严格一些： 要是抗碰撞(collision resistant)的 最好还有PRF的性质 类似PRF，Hash也是一个keyed function，只不过更多的时候 \\(k\\) 是由某种方法选出来的固化公开常量，而非随机选出来的。 定义 Hash \\(\\Pi=(\\text{Gen,H})\\) 是一对PPT算法，满足： \\(k=\\text{Gen}(n)\\) 是一个公开的参数 \\(H(k,x)=H^k(x)\\in\\left\\{0,1\\right\\}^{l(n)}\\)，其中 \\(l(n)\\) 是expansion factor 如果有 \\(l(n)&lt;n\\)，那么我们就称 \\(H\\) 是一个压缩映射(compression function) Collision resistant 首先要定义一个game \\(\\text{Hash-coll}_{\\cal A,\\Pi}(n)\\)，这个game很直白，就直接说了： 我们公开Hash的参数 \\(s\\)，然后让Adversary输出一对比特串 \\((x_1,x_2)\\)。若有 \\(H^s(x_1)=H^s(x_2)\\)，我们就说Adversary找到了一个碰撞(collision) 称一个Hash scheme是c.r.的，当且仅当有 \\(Pr\\left[\\text{Hash-coll}_{\\cal A,\\Pi}(n)=1\\right]\\leqslant\\text{negl}(n)\\) 对任意的PPT Adversary成立 此外还有一些更弱的对Hash的要求 Second preimage resistant 如果给定了参数 \\(s\\) 和一个比特串 \\(x\\)，有任意的PPT Adversary都不能找到 \\(x&#39; eq x\\) 使得 \\(H^s(x)=H^s(x&#39;)\\)，就称这个Hash scheme是2nd p.r.的 注意到如果Adversary没法对Hash找到任意一对碰撞（满足c.r.），那么它一定无法对一个给定的比特串找到碰撞（满足2nd p.r.），因此有 c.r. \\(\\Rightarrow\\) 2nd p.r. Preimage resistant 如果给定了参数 \\(s\\) 和一个比特串 \\(y\\)，有任意的PPT Adversary都不能找到 \\(x\\) 使得 \\(H^s(x)=y\\)，就称这个Hash scheme是p.r.的 注意到如果Adversary可以对给定的 \\(y\\) 找到原象 \\(x\\)（not p.r.），那么它就可以通过找出 \\(y\\) 的所有原象，从中挑一对出来就能找到一对碰撞了（not c.r.）。这是因为在Hash是压缩映射的时候，至少有一半的元素有两个原象 Merkel-Damgard Transform 其实很套路了。假如我们有一个定长的c.r.压缩映射，要怎么才能做到构造出一个任意长的压缩映射？ 不失一般性，假设我们已有的小映射是 \\(\\left\\{0,1\\right\\}^{a+b}\\mapsto\\left\\{0,1\\right\\}^{a}\\) 有一个很直观的图，大概是长这样的 插入图片 大概分成三步走： 先做补全（padding），得到长度为 \\(b\\) 的整数倍的一个比特串 按照每 \\(b\\) 位切成一块，每一块记作 \\(B_i\\)，最后额外加入一块 \\(L\\)，表示原始比特串的长度 确定一个 \\(z_0=IV\\)，有 \\(z_i=H(z_{i-1}|| B_i)\\)，最终的那一块 \\(z_{last}\\) 就是整个Hash的输出 c.r.的证明可以通过反证法和归纳来得到，同样是如果能构造出大映射的碰撞，那么一定是构造出了一个小映射的碰撞，因此就矛盾了。 和MAC的应用 回想前一篇MAC的内容，我们没有一个很好的办法能构造任意长度的MAC。但是现在有了任意长度到固定长度的c.r.压缩映射，我们就可以用这个c.r. Hash和secure fixed-length MAC一起来构造一个任意长度的MAC了 只需要规定 \\(\\widehat {\\text{Mac} }_k(x)=\\text{Mac}_k(H^s(x))\\) 即可，\\(\\text{Vrfy}\\) 的部分只需要用canonical方法就好了 安全性的证明可以通过反证法说明：我们下面要证明这是一个secure arbitrary-length MAC 假设 \\(\\cal A\\) 构造出了一个新的tag \\((m&#39;,t&#39;)\\)，那么有如下可能： 存在一个 \\(m&#39;&#39;\\)，使得 \\(\\cal A\\) 查询了 \\(m&#39;&#39;\\)，并且有 \\(H^s(m&#39;)=H^s(m&#39;&#39;)\\)。显然这与c.r.矛盾 所有 \\(\\cal A\\) 查询过的消息在Hash之后都不与 \\(m&#39;\\) 构成一对碰撞，此时意味着 \\(\\cal A\\) 构造出了一个全新的tag，这又与secure-MAC矛盾了 Hash攻击 这部分暂且咕咕咕","tags":["Cryptography"]},{"title":"密码学05 MAC","path":"/2021/12/20/Crypto05-MAC/","content":"Message Authentication Code 简称MAC。通常我们不仅希望保证一条消息的内容没有被泄露（例如你向女神告白的信的内容），还希望我们发出去的消息没有被篡改过（例如你给女神的告白信不会被换成/修改成除了告白信之外的别的东西）。前者属于secrecy的范畴，后者则是integrity的范畴。 之所以会出现这样的需求，是因为我们开始考虑更“强大”的adversary了。原本的EAV-secure模型只能对窃听者进行建模，而通常我们的adversary不仅可以看（窃听），还可以摸（篡改消息）。并且我们发现原有的加密算法是无法保证integrity的。以OTP为例，攻击者在得到密文 \\(c\\) 后，可以通过翻转 \\(c\\) 任意的比特来实现篡改出任意可能的信息。即使他不知道消息本身是什么，他也可以伪造出任意的消息——反正任意的定长比特串都是合法消息 于是就有了MAC的闪亮登场！这样你给女神的真情表白就不会变成痴汉发言，战书，或者是别的什么东西了。 定义 一个MAC包括三个PPT算法，\\(\\text{MAC=(Gen,Mac,Vrfy)}\\)，其中 \\(\\text{Gen}\\) 负责生成一个密钥 \\(k\\)，\\(\\text{Mac}_k(m)=t\\) 负责根据 \\(m,k\\) 生成一个标签(tag)，\\(\\text{Vrfy}_k(m,t)\\) 则会在 \\(t\\) 是 \\(m\\) 的一个合法tag时输出 \\(1\\)，否则输出 \\(0\\) 这里的 \\(\\text{Vrfy}\\) 是确定性算法，而 \\(\\text{Mac}\\) 则没有要求。这点是很显然，因为要保证： 正确的tag能被识别 错误的tag能被报出来 也就又是所谓的sound&amp;complete了 看起来很像私钥加密，但是又不太一样：\\(\\text{Vrfy}\\) 是需要借助 \\(m\\) 来确认tag的正确性的，因此MAC往往不会单独使用，这里的 \\(m\\) 还需要加密保护起来。 然后就定义了几个性质 Correctness \\(\\forall k=\\text{Gen}(1^n): \\text{Vrfy}_k(m,\\text{Mac}_k(m))=1\\) 没啥好说的，这个性质提示我们： 如果我们的 \\(\\text{Mac}\\) 是确定性算法，那么我们的 \\(\\text{Vrfy}\\) 就可以通过重新计算一次 \\(\\text{Mac}_k(m)\\)，然后比较 \\(\\text{Mac}_k(m)\\) 和 \\(t\\) 是否相同来判断这个tag是否合法。这被称为canonical way Secrecy 其实就是如何保证我们的消息被篡改后，能从tag中反馈出来？ 密码学给出的回答是这样的：如果任何PPT算法都不能造出一个对应的message-tag pair出来，那么我们就可以认为没有人能够给一个修改过后的消息 \\(m\\) 伪造tag，也即不存在消息被篡改后仍然能通过 \\(\\text{Vrfy}\\) 了 形式化地写出来是一个 \\(\\text{Mac-forge}_{\\cal A,\\Pi}\\) game： 首先生成一个 \\(k\\)，然后给一个 \\(\\text{Mac}_k\\) 的oracle给adversary \\(\\cal A\\) \\(\\cal A\\) 可以任意查询oracle，然后输出一对答案 \\((m,t)\\)。期间我们会维护一个集合 \\(\\cal Q\\)，表示 \\(\\cal A\\) 查询过的所有消息 检查 \\(\\text{Vrfy}_k(m,t)\\)，如果满足1. \\(\\text{Vrfy}_k(m,t)=1\\) 且 2. \\(m ot\\in \\cal Q\\)，就得到结果 \\(1\\)，否则为 \\(0\\) 我们称一个MAC scheme \\(\\Pi\\) 是安全的，当且仅当 \\(Pr\\left[\\text{Mac-forge}_{\\cal A,\\Pi}(n)=1\\right]\\leqslant\\text{negl}(n)\\)。这个安全性的全称也叫being existentially unforgeable under an adaptive chosen-message attack 这个定义就是很好地抓住(capture)了“不能伪造tag”这句话，看下来是很自然的。 如果细心一点可以发现，这里的 \\(\\text{Mac}\\) 不一定是确定性算法，因此可以有一个 \\(m\\) 对应多个不同的 \\(t\\)，这说明上述定义会出现一些问题——Adversary完全可以给一条已经查询过的消息 \\(m\\) 构造出一个全新的tag。于是自然引出strongly secure MAC的定义： 太懒了，只需要把上面的 \\(m ot\\in\\cal Q\\) 改成 \\((m,t) ot\\in\\cal Q\\) 就好了 结合canonical way的MAC可知，对所有canonical MAC有：如果它是secure的，那么它自然地就是strongly secure的 注意到这个strong secure的安全性要求仍然是很弱的。我们甚至并不要求Adversary输出的消息有含义，而只需要能构造出一条消息和对应的tag就好了。 构造 fixed-length MAC 构造一个定长MAC是比较简单的，只需要用一个PRF就好了。安全性的证明可以通过反证，然后构造一个区分PRF和真随机函数的攻击者 \\(\\cal A\\) 来完成，细节就留做习题吧~ arbitrary-length MAC 只讲讲怎么做到任意 \\(l(n)\\) 的整数倍长度，其中 \\(l\\) 是单个 \\(\\text{MAC}\\) 的extension factor（回忆上面说的用PRF构造MAC） 假设现在给了 \\(n\\) 个消息块，每个块都能单独用MAC构造tag，那么我们可以这么构造一个整体的tag： 规定 \\(t_0=IV\\)，其中通常 \\(IV=0\\)，initial vector的意思 规定 \\(t_i=MAC(t_{i-1}\\oplus m_i)\\) 最后输出 \\(t_n\\) 就好了 安全性的证明比较简单，只需要对着 \\(n\\) 做数学归纳法就好了，可以发现每次都没法伪造的Adversary对于整体的tag束手无策","tags":["Cryptography"]},{"title":"PA3 附加关卡","path":"/2021/12/11/PA3-附加关卡/","content":"这次做起来还是很有收获的，虽然最后阶段基本在写库函数，和ICS已经没啥关系了 思考题 AM究竟给程序提供了多大的栈空间呢? 观察$AM_HOME/scripts/linker.ld这个链接脚本可以发现，其中定义了一个符号_stack_pointer 而根据AM启动客户程序的流程可知，在am/src/riscv/nemu/start.S中的_start:中将会执行la sp, _stack_pointer，以此初始化栈指针。又注意到_stack_top符号的地址与之相差0x8000，因此可以回答AM中程序的栈空间大小为0x8000字节 对比异常处理与函数调用 在函数调用中，调用点只需要保存caller saved的寄存器 而在异常处理中，需要保存所有的通用寄存器、CSR寄存器 区别是由规约决定的。函数调用约定决定了我们只需要保存caller saved寄存器即可满足被调用函数对寄存器使用的需求，而异常处理结束后，我们仍然需要恢复到异常发生前的“状态”（具体而言就是所有的通用寄存器、CSR寄存器....等等），因此需要提前把它们保存起来 堆和栈在哪里? 首先要知道栈和堆的用途 只需要注意到栈的使用只发生在函数调用过程中，堆的使用只发生在malloc/free函数调用之后，因此它们都只在动态时有意义，这是为什么它们不需要出现在可执行文件中 AM中的栈从哪里来已经在上面回答过了，而堆的来头则可以在$AM_HOME/am/src/platform/nemu/trm.c中找到，是一个Area类型的结构体变量heap。可以发现它们都是在程序执行后（_start），main执行前被初始化的。 如何识别不同格式的可执行文件? 在ELF Header中可以看到OS/ABI一项，这里可以区分操作系统；还有大/小端机的信息、数据储存方式的信息（二进制补码格式等等） 冗余的属性? FileSiz是ELF文件中的大小，MemSiz是加载进内存后的大小。对于.bss段中的数据无需保存在ELF文件中（默认被初始化为0），但是加载之后应当在内存中占用空间、有它的地址。因此就会出现FileSiz&lt;=MemSiz的情况 为什么要清零？ 因为初始化为0的变量会被储存在.bss段中，数据被初始化为0这一步就是在加载过程中完成的 当然这么说还有一个前提，就是内存的初值是不确定的（至少不是全0的），这点可以从NEMU的menuconfig中看到，默认NEMU会初始化内存为随机内容 RISC-V系统调用号的传递 这题实在猜不出设计者的心意，说一下我的想法 需要注意到的是，进行系统调用的时候，系统调用的调用号和它的参数是一并被传给系统调用识别的处理函数的。而a0在calling convention中已经作为储存函数调用参数的第一个寄存器了。假如在识别出正确的事件后需要进一步调用函数进行处理，就需要逐个移动寄存器的内容后再call。而把系统调用号作为最后一个参数传入，则在必要时只需要覆盖就能进一步处理，这就可以更快处理各类系统调用（当然也更方便了）。 文件偏移量和用户程序 把偏移量放在文件记录表中维护有一个前提：无论文件被打开多少次，都共享同一个偏移量。这也是我在实现fs.c的时候产生的疑问：如果一个文件被打开多次，正确的读写行为应该是怎样的？ 这个问题可以通过audio.c中的pipe来理解。一个pipe是一个文件（队列），它支持同时从队头读取、从队尾写入。读取和写入两个操作可以由两个独立的进程分别完成，这就要求它们的读写操作必须彼此独立（至少偏移量是互相独立的）。 比较fixedpt和float 取： 可以不需要FPU也能实现小数运算 可以简单地保持序关系和实现简单的运算 在数字较大时仍然有比较好的精度（总共32bit的有效位） 舍： 在结果很小时答案的精度，浮点数能表示的最小数字远小于定点数的最小数，这使得浮点数在多次除法后仍然保持着相对准确的结果，而定点数很容易变成0 能表示数字的值域（最大值）。浮点数能表示的最大数字远大于定点数的最大数，这使得定点数在多次乘之后很容易符号溢出 神奇的fixedpt_rconst rconst的最外面有一个强制类型转换为fixedpt类型（也就是int），因此整个表达式最终会成为一个整数（这是编译期间进行的），所以最终不会出现浮点指令。 RTFSC??? 处理不是那么好玩，可以玩一玩怎么逆处理！ 不妨记处理后的程序为P'，原本的程序为P 首先注意到这种修改不能改变程序语义，并且有如下性质（性质1）：在P'中相同的符号，在P中一定也相同 而且还有一个很特殊的地方：硬编码的数据（包括AM的接口调用）是没法被处理的（不然就改变了程序的语义），因此这一部分也可以提供一些信息 同样以16122学长的跳一跳程序为例，可以明显发现硬编码了三种颜色（红黄紫），那么就可以根据这个得到绘制不同部位的代码；硬编码了欢迎和得分信息，因此可以确定一部分函数的功能；同时能发现几处AM接口调用，因此可以直接得到对应函数的用途。 做完这些还能更进一步：注意到for循环内的变量也是携带有语义的（如行优先枚举），并且根据性质1可以借此获得一些全局的语义信息（例如说小人的高度、小人的坐标等等） 到这里其实就差不多了，剩下的基本自己写也能写出来，于是就得到了一个重命名后的可读跳一跳实现。 感想&amp;反馈 主要想说说loader的问题。在PA3之前我从没有想过为什么.bss段的数据不需要特殊处理，直接就能加载到NEMU模拟执行，到了后面有同学问出这个问题我才意识到，这步loader的工作实际上是提前被做掉了（实际上如果能早一点问出“为什么我们同时需要一个.bin和一个.elf？”就能更快发现了，这个问题确实是一个看起来简单但是不那么容易回答明白的问题） 还想说说遇到的一个bug，仍然与gettimeofday()有关。阅读sys/time.h可以发现struct timeval的实现用的是long int，也就是一个架构相关的类型。通过简单的测试可以发现在timer-test/main.c中输出sizeof(struct timeval)的结果是8，而在nanos-lite/src/syscall.c中的结果则是16。这是因为 最初在实现gettimeofday系统调用的时候，我直接把struct timeval *指针作为参数传给了nanos-lite/src/syscall.c，这内存布局就导致了问题：用户程序中timeval的两个成员分别组成了syscall中timeval的第一个成员的高32位和低32位。这直接使得我的用户程序永远读不到usec的信息（恒为0），因此在用户程序那里最小时间间隔就是整1秒了，这也是为什么最初我的仙剑只能跑到1帧（他已经尽力了） 事实上这个问题并不罕见，这正是ICS理论课本上的例子：链接的双方用不同的类型解释了相同的内存区域，使得接口的语义发生了偏差。我的解决方法也简单粗暴：在系统调用中直接传递两个成员的指针，并规定它们都是uint32_t类型，这样就（暂时地）解决了问题 于是我的疑问在于，为什么一个大家经常使用的库函数要使用平台相关的类型？ upd： 实际上是libc更新了，新版的库函数规定struct timeval中的成员应该不管什么架构都使用64位长度的整型，而navy中带有的libc还没有修改，所以导致了这个问题。只需要在makefile中修改就可以正确实现了 在说到“一切皆文件”的哲学时，我还没有什么体会，直到看到通过更换具体文件读写的实现来达到“用统一的方法处理不同的事物”的目的，才理解什么叫一切皆文件了——就是抽象出统一的接口供程序操作，来达到简化设计的目的。在这一层面上，所有可以操作的东西看起来都是文件（可读可写） 说一点反馈吧。感觉PA3.3的部分内容实在太多了，后期库函数的编写也很费时间。我能明白这门课一直在强调的抽象层观点，但是用SDL实现一个抽象设备1，再给抽象设备1抽象出AM接口，然后给抽象的接口封装出一个NDL、miniSDL，最后用miniSDL实现libam中的抽象设备2，这样来来回回地编写功能相同、逻辑类似，而仅仅只是在抽象层之间传递信息的模块，是不是有点码农的感觉.....","tags":["ICS"]},{"title":"PA2 附加关卡","path":"/2021/12/01/PA2-附加关卡/","content":"完成了声卡的实现，可以流畅播放If I Could Tell Her~ 但是玩litenes就只有23帧，用fceux就卡得声音都变成了儿童鞋垫 收货 讲几个主要的 假设需要同时定义很多个东西，同时这些东西又有着相同的模式，并且我们希望能够方便地修改、添加这些东西（只需要维护一份唯一的列表），那么就可以这么做： #define LIST(F) F(item1) F(item2) ... #define FUNC(X) //blablabla LIST(FUNC) 这样非常像fp的写法，并且我们只需要维护一份item表，套用不同的模式生成函数就可以得到不同模式的列表了（这句话有点绕），其实就是一个X-macro 一系列trace能够使得debug非常简便，但是最强的还是difftest，通常只需要找到第一处diff就能查出错误了。 然后还想讲一个匪夷所思、困扰了我一整周的bug。NEMU默认通过一个get_time()函数获取宿主机时间，并且提供了两个选项：gettimeofday()和clock_gettime()。STFW之后可以发现，gettimeofday()是通过vdso实现的，即并没有真的系统调用，因此性能很高。并且使用的是tsc时钟源，所以精度也很高，是获取时间的首选方法。然而在我的笔记本上（机械革命Code01）同时安装了windows10和ubuntu21.04，在win10休眠后启动ubuntu，就会出现tsc时钟源无法访问的情况，这时候再调用gettimeofday()，最终仍然会通过clock_gettime()获取时间，从而花费更多时间，导致NEMU性能下降。STFW之后发现速度大约能差17倍，这也和我跑分400-&gt;23的变化趋势大致相符合。YZH的建议是让我看看为什么会频繁调用gettimeofday，但是我在callgrind之后发现，二者的区别仅仅在于调用所消耗时间，而调用次数则一模一样 解决方案就更那啥了，只需要重启进入win10，点击关机，然后再进入ubuntu即可。通过命令cat /sys/devices/system/clocksource/clocksource0/available_clocksource 可以观察是否有tsc 思考题 为什么不需要rtl_muls_lo 对于有符号乘法的低32位，结果来源于两个操作数的低16位，此时指令的行为和无符号整数的乘法行为一致 为什么执行了未实现指令会出现上述报错信息 在special.h中定义了inv指令，所有的指令模式匹配失败后最后会返回inv表示invalid opcode，在inv的辅助执行函数中就打印了错误信息 指令名对照 可以以关键词\"expand\"、\"expansion\"、\"pseudo\"来搜索伪指令，或者直接通过伪指令的二进制序列来搜索真正对应的指令。之所以能这么做是因为伪指令是软件层面的约定，伪指令最终对应什么指令由它的二进制序列决定 stdarg是如何实现的? 我能想到的一个实现方式是传入一个链表的表头作为参数，这样通过更改链表的长度就可以实现任意数量的参数传参了。而取参数的操作则是通过遍历链表完成的 消失的符号 宏在预处理时就被展开了，因此不会出现在符号表中 局部变量在链接时是不可见的，因此也不会出现在符号表中 一个符号要可以被跨文件使用，因此不能是局部变量。 update： 上完课之后理解更深了一点 所谓符号，必须是有确定地址的实体，这就是为什么macro和局部变量不是符号——macro不存在地址，直接被展开；局部变量分配在栈上，甚至连地址都不是确定的。 冗余的符号表 之所以会出现这样的情况，是因为ELF文件在运行时，并不需要符号表中的信息，只需要映射对应的段到虚拟内存的不同地址中即可。 而对于可重定位文件，符号表则是必不可少的——否则链接将无法进行，链接器对符号的查找也将无从下手。 寻找\"Hello World!\" 在.rodata段，因为.strtab虽然叫字符串表，但储存的是符号的名字的字符串，而不是程序中的字符串。程序中的字符串是以数据的形式储存着的 不匹配的函数调用和返回 f0和f1是尾递归，可以用一次ret代替连续的一整段ret 通过vscode的查找，可以发现f2和f3的call和ret是匹配的，而所有的f1只有在递归的base case处有ret，f2也是类似的。通过查找可以发现call的数量与check的数字一致，因此call一定不会少，少的就只能是ret了。再继续查找所有对f函数的call，刚好能对应上recursion.c最后的check中的常数，这就有力地证明了我生成的ftrace是正确的实现（至少对所有的call都有了正确的处理） 如何生成native的可执行文件 关键在于CFLAGS中的-D选项，根据传入ARCH参数的不同，Makefile会通过gcc的参数传入不同的宏定义，以此来产生不同平台上的编译产物 这是如何实现的？ 在定义了__NATIVE_USE_KLIB__之后，klib的库函数将会有函数体，因此成为强符号，在链接时所有对库函数的调用都将指向klib中的库函数 实现DiffTest 实际上我并不是RTFSC找到的这个顺序，而是直接翻阅了Spike的官方手册得到了这一顺序... Spike本身就是按照ABI的顺序来的，因此我的代码不需要改变就可以按顺序比对寄存器 捕捉死循环(有点难度) 我没有实现，但我猜可以通过记录程序的状态，然后如果多次经过一个位置就比对此时寄存器状态和上一次经过时的寄存器状态，如果相同则很有可能陷入了死循环 不过这样好像会漏掉一些死循环的情况.... 理解volatile关键字 感觉这里的优化过于激进了 关键在于这里的_end是extern变量，因此p指向的内存可能被其它函数/程序修改，从而导致 仅从函数func()来看，*p是定值 但是考虑了其它情况后，*p不是定值 这里显式地写出，是为了不让编译器做过度优化，从而保证可执行文件的行为与程序语言的描述一致 理解mainargs am-kernels/kernels/hello/目录下的Makefile会include $AM_HOME/目录下的Makefile $AM_HOME/目录下的Makefile则会include对应架构的($ARCH).mk文件，然后分别include硬件架构和软件平台的两份.mk文件 nemu 只需要观察abstract-machine/scripts/platform/nemu.mk中的run规则 这里会通过-DMAINARGS=\\\"$(mainargs)\\\"传入一个宏MAINARGS=\"xxx\"，这里双引号内的字符串将会在abstract-machine/am/src/platform/nemu/trm.c中被作为static const char mainargs[]的值初始化，然后这个字符串会被传入main中，也就是我们的hello.c中的main函数 native 这个和上面比起来就要简单一些 使用-nB命令就可以发现，最终的hello-native由hello.o一个am-native.a文件链接而成，寻找这个am-native.a就可以发现它源于platform.o这个文件，观察platform.o就可以发现在函数init_platform()中有const char *args=getenv(\"mainargs\")这样一行代码 STFW即可发现，getenv(const char *)用于找到特定名称的环境变量的值，在这里就是我们在编译时写下的mainargs=xxx了 神奇的调色板 只需要改变调色板的亮度，就可以不改变显示而实现渐暗效果了","tags":["ICS"]},{"title":"PA1 附加关卡","path":"/2021/12/01/PA1-附加关卡/","content":"写了很多代码，更宝贵的应该是这些思考和经验，所以就放上来了 必做题会被删去，和具体实验相关的内容也会被处理掉，应该看起来是不存在剧透的（大概） 收货 PA1主要是在做一些预备工作，例如阅读NEMU的主体框架，添加一些表达式求值、打断点之类的小功能，以及一些测试程序正确性的方法介绍 在提到表达式解析的时候引入了CFG的概念，还用到了正则匹配tokens，写起来不算太难。处理的时候要用一些库函数，也算是练练手 做gen-expr.c的时候，手册给了一个把表达式打印到一段代码中然后编译这段代码的方法来获得一个表达式的值——也就是说，我们在利用GCC替我们实现了表达式求值！再加上下面对于除0的排除，这样仅仅通过popen就可以快速测试我们的表达式解析器了。同时还学到一个小技巧：如果不会用sed awk之类的命令行工具，还可以通过管道连接python -c 'xxx' 的方法利用python来进行字符串的小处理，这样还是更方便一些的 思考题 计算机可以没有寄存器吗 STFW得到一个叫做stack oriented programming的编程范式 这样的编程范式依赖于一种叫stack machine的机器 当然,如果在内存中预留出数量固定的地址代替原本寄存器的功能,好像也就免去了寄存器的用处了 为什么 init_monitor() 中全是函数调用 把具体的操作按照阶段分成若干部分,再封装成函数,是抽象的一种 可以方便后期修改(不同函数间相对独立或几乎不相关),方便后期调试(方便打断点调试某个阶段的功能),可以把不必要的细节隐藏起来,提升代码的可读性 argc 和 **argv 的来头 猜想肯定是调用这个程序的人给的参数,在命令行敲就是命令行给的参数,被别的程序调用就是调用者给的参数 STFW得到如下结果 On Linux, a program is started by execve(2) and that system call passes argument to main . 也就是说调用者通过系统调用来运行别的程序,传参的过程实际上是操作系统完成的 又去看了CSAPP,得知值传参是通过寄存器传递值完成的,而具体字符串的参数则储存在栈上,实际上操作系统做的就是传递 argc 和 **argv 两个值,然后把参数压入栈中 究竟要执行多久? 观察到传入形参的类型是 uint64_t ,因此传入 -1 的意思就是传入一个MAX 继续观察,后面的循环是 for (;n&gt;0;n--) ,因此传入 -1 的时候就要执行MAX次 传入 -1 是UB吗 翻到了手册... 6.3.1.3 Signed and unsigned integers Otherwise, if the new type is unsigned, the value is converted by repeatedly adding or subtracting one more than the maximum value that can be represented in the new type until the value is in the range of the new type. 这里的adding和subtracting都指的是数学上的操作,和类型无关,是unbounded的 也就是说传入 -1 的时候究竟执行多少次取决于无符号形参的类型,这不是UB 为什么 printf 后要加 首先是为了可读性,所有输出挤在同一行很难看 接着STFW得到如下解释 Very often, if you don't end your printf format string with a , some of the output stays in the stdout buffer, and you need to call fflush to get all the output shown. 标准输出利用output buffer来暂时保存一整行的输出, printf(\"\") 实际上实行了刷新buffer的功能 如果不刷新buffer,那么标准输出的行为就不能保证是即时的 类似的还STFW到这样的说法 Standard output is line buffered if it can be detected to refer to an interactive device, otherwise it's fully buffered. 除0的确切行为 在开启较高级编译优化的情况下,常亮表达式的计算将会是编译期行为,因此最终编译器会得到一个 expr/0 的表达式,就会报warning 所以只需要修改编译的命令,加上 -O2 -Wall -Werror 就可以让编译器在编译期就得到表达式的值,并 且把warning变成error,然后根据GCC的返回值是否为 0 就可以判断是否存在除 0 行为了,如果存在 error就直接再生成一个 update: 事实上好像直接看 popen 的返回值就好了,我当时在想什么... static 关键字 static 变量储存在静态区,生命周期和全局变量一样,但是作用域和局部变量一样 static 变量只会被初始化一次 static 修饰的变量和函数对其他编译单元是不可见的 这里用的是性质3,提供了类似 class 中 private 成员函数的功能 随心所欲的断点 因为每次解析指令的时候都是从 $pc 开始的,并且 $pc 只会经历所有指令的首尾,所以从指令的中间设置断点会跳过这个断点","tags":["ICS"]},{"title":"形式语义05 Semantics","path":"/2021/11/07/Semantics05-Semantics/","content":"Formal Semantic 一般分三种 Operational Semantic，操作语义 Denotational Semantic，指称语义 Axiomatic Semantic，公理语义 主要记一下操作语义 Operational Semantic 对命令式语言而言，一个程序的运行状态可以用 状态\\(\\times\\)当前语句地址 二元组确定，那么就可以用 \\(\\left&lt;c,\\sigma\\right&gt;\\) 描述一个执行状况，表示接下来要执行 \\(c\\)，目前的状态为 \\(\\sigma\\) 状态一般指的是 \\(\\text{Value}^\\text{Var}\\) 的函数，这里规定 \\(\\text{Var}\\) 表示变量的集合，\\(\\text{Value}\\) 表示值（常量）的集合。注意这里的集合内都是符号，与语义无关。例如我们说\\(23456\\)是一个值，但它的具体含义是什么在这里并不关心。具体给值的符号赋予含义的操作，一般由另外的函数进行，例如说 \\(\\text{Meaning}:\\text{Value}\\mapsto \\mathbb N\\) 就是一个给所有Integer Tokens赋予具体整数含义的映射 操作语义一般分为大步语义（\\(\\Downarrow\\)Big-Step）和小步语义（\\(\\rightarrow\\)Small-Step），分别用于不同目的的研究。一般操作语义要依靠语法结构进行，即它是Syntax Directed的。因此有时候也叫做SOS（Structural Operational Semantics） 通常讲语义会基于一个具体的语言给一些例子，建议直接找PLDI和POPL的形式语义论文看 Evaluation \\[\\begin{aligned} \\frac{\\left&lt;x,\\sigma\\right&gt;\\rightarrow\\left&lt;{\\rm{n_1} },\\sigma&#39;\\right&gt;}{\\left&lt;x+y,\\sigma\\right&gt;\\rightarrow\\left&lt;{\\rm{n_1} }+y,\\sigma&#39;\\right&gt;} \\\\ \\\\ \\frac{\\left&lt;y,\\sigma&#39;\\right&gt;\\rightarrow \\left&lt;{\\rm{n_2} },\\sigma&#39;&#39;\\right&gt;}{\\left&lt;{\\rm{n_1} }+y,\\sigma&#39;\\right&gt;\\rightarrow\\left&lt;{\\rm n_1+n_2},\\sigma&#39;&#39;\\right&gt;} \\\\ \\\\ \\frac{x\\in\\text{Var} }{\\left&lt;x,\\sigma\\right&gt;\\rightarrow\\left&lt;\\sigma(x),\\sigma\\right&gt;} \\\\ \\\\ \\frac{\\text{Meaning}({\\rm n_1})+\\text{Meaning}({\\rm n_2})=\\text{Meaning}({\\rm n_3})}{\\left&lt;{\\rm n_1+n_2},\\sigma\\right&gt;\\rightarrow\\left&lt;{\\rm n_3},\\sigma\\right&gt;} \\end{aligned}\\] 注意这里\\(\\text{Meaning}\\)映射后的加法实际上是正整数的加法，而二元组中的\\(+\\)仅作为符号，具体含义由\\(\\text{Meaning}\\)决定 以求值为例，来展示一下小步语义的特点 给出的求值语义有一些比较好的性质： 可以看到上面给出的求值语义是确定的(Deterministic)，这并不是所有语言必备的 并且我们对代码、状态二元组的推导只能通过每次寻找当前语句的结构中，最左的可推导的句子，这也不是所有语言必备的； 到这里就可以发现一些情况了。所谓的形式语义无非是刻画某些东西性质的一类工具，具体只能作为解决问题的帮手，而本身只起到一种规范的作用，也就是Reference Manual了。并且这种语义是建立在数学符号上的，可以与具体的实现无关（当然也可以有关，但这并不是我们希望的），可以非常容易地进行推理 对于上面给出的例子，可以用写表达式求值辅助：对于给定的表达式二叉树（why 二叉？），我们规定必须先算左子树，再算右子树。如果遇到了变量，就替换为它在环境中的值；如果遇到了常量+常量，就把token转化为整数，计算之后再变回token。 到这里就又可以发现一些事情。操作语义几乎就是在模拟一个具体的算法、一个抽象计算机上编程语言的执行过程、一个解释执行的解释器。反过来，有了操作语义，我们也可以以此指导编译器、解释器的实现，规范具体细节。 这有什么用？用JYY的话说，我们就可以用一个甚至还没有被实现的语言来编写程序、模拟执行，并且预测这些程序的结果、证明它的细节了。这是多酷的一件事情！ While \\[\\begin{aligned} \\frac{[\\![exp]\\!]\\sigma=\\textbf{true} }{\\left&lt;{\\textbf{while} }\\;exp\\;\\textbf{do}\\;stmt,\\sigma\\right&gt;\\rightarrow\\left&lt;stmt\\textbf;\\;\\textbf{while}\\;exp\\;\\textbf{do}\\;stmt,\\sigma\\right&gt;} \\\\ \\\\ \\frac{[\\![exp]\\!]\\sigma=\\textbf{false} }{\\left&lt;{\\textbf{while} }\\;exp\\;\\textbf{do}\\;stmt,\\sigma\\right&gt;\\rightarrow\\left&lt;\\textbf{skip},\\sigma\\right&gt;} \\\\ \\\\ \\frac{\\left&lt;stmt_1,\\sigma\\right&gt;\\rightarrow \\left&lt;stmt_1&#39;,\\sigma&#39;\\right&gt;}{\\left&lt;stmt_1\\textbf;\\;stmt_2,\\sigma\\right&gt;\\rightarrow\\left&lt;stmt_1&#39;\\textbf;\\;stmt_2,\\sigma&#39;\\right&gt;} \\\\ \\\\ \\frac{}{\\left&lt;\\textbf{skip;}\\;stmt,\\sigma\\right&gt;\\rightarrow\\left&lt;stmt,\\sigma\\right&gt;} \\\\ \\\\ \\frac{}{\\left&lt;\\textbf{skip},\\sigma\\right&gt;\\rightarrow\\left&lt;\\textbf{skip},\\sigma\\right&gt;} \\end{aligned}\\] 这里主要想讲 \\([\\![exp]\\!]\\sigma\\) 这个符号的用法。这里表示在环境 \\(\\sigma\\) 下，表达式求出的值是什么。其具体含义与操作语义的求值相同，但是注意到操作语义在执行的过程中会丢失表达式的结构信息（我们在一步步化简），因此通常需要把求值和化简分开，这里表现为用单独的函数表示求值操作，以此简化\\(\\textbf{while}\\;\\ldots\\;\\textbf{do}\\) 的操作语义规则，否则我们得这么写： \\[\\begin{aligned} \\frac{}{\\left&lt;{\\textbf{while} }\\;exp\\;\\textbf{do}\\;stmt,\\sigma\\right&gt;\\rightarrow\\left&lt;\\textbf{if}\\;exp\\;\\textbf{then}\\;(stmt\\textbf;\\;\\textbf{while}\\;exp\\;\\textbf{do}\\;stmt)\\;\\textbf{else}\\;\\textbf{skip},\\sigma\\right&gt;} \\end{aligned}\\] Evaluation Context 人总喜欢偷懒，观察到上面的操作语义其实在做很重复的事情： 我们写一堆加法的rules，然后获得了加法操作 再写一堆乘法的rules 再写一堆减法的rules 再写一堆二元运算的rules.... 可以发现，既然这些都是二元运算，那么能不能给所有的二元运算定义一次求值顺序（e.g. 最左推导、确定性blabla），然后只需要根据不同情况带入具体算符就可以了？ 这就是所谓的Evaluation Context了 仍然以Evaluation为例，可以规定 \\[\\begin{aligned} {\\cal E}:=[\\;]{\\,\\Big |\\,}{\\cal E}+exp{\\,\\Big |\\,}{\\rm n}+{\\cal E}{\\,\\Big |\\,}{\\cal E}-exp{\\,\\Big |\\,}{\\rm n}-{\\cal E} \\end{aligned}\\] 这里我们称 \\(\\cal E\\) 为Evaluation Context，对于带入其中的项 \\(r\\) 称为Redex 那么我们只需要如下几条rules，就足够表达出带有加减法的表达式求值语法： \\[\\begin{aligned} \\frac{\\text{Meaning}({\\rm n_1})+\\text{Meaning}({\\rm n_2})=\\text{Meaning}({\\rm n_3})}{\\left&lt;{\\rm n_1+n_2},\\sigma\\right&gt;\\rightarrow\\left&lt;{\\rm n_3},\\sigma\\right&gt;} \\\\ \\\\ \\frac{\\text{Meaning}({\\rm n_1})-\\text{Meaning}({\\rm n_2})=\\text{Meaning}({\\rm n_3})}{\\left&lt;{\\rm n_1-n_2},\\sigma\\right&gt;\\rightarrow\\left&lt;{\\rm n_3},\\sigma\\right&gt;} \\\\ \\\\ \\frac{\\left&lt;s,\\sigma\\right&gt;\\rightarrow \\left&lt;t,\\sigma\\right&gt;}{\\left&lt;{\\cal E}[s],\\sigma\\right&gt;\\rightarrow\\left&lt;{\\cal E}[t],\\sigma&#39;\\right&gt;} \\end{aligned}\\]","tags":["Formal Semantics"]},{"title":"密码学04 PRG&PRF","path":"/2021/10/19/Crypto04-PRG-PRF/","content":"Pseudo Random Generator 真的随机性是要求很强的东西，上一章是对安全性的适当弱化，而这一章就是对随机性的适当弱化，使得我们可以得到一个“不那么随机但是可以当成随机数用的随机数” 也就是伪随机性 定义 这里的随机性指的是一个比特串的分布的随机性，通常用 \\(\\text{Dist}\\) 表示 为了使用，这个分布通常是计算出来的，即有一个多项式确定性算法 \\(G\\)，可以通过一个真随机种子 \\(s\\) 得到一个伪随机串 \\(G(s)\\) 通常我们要求 \\(G(s)\\) 的输出要比输入长（否则一个平凡的伪随机函数可以取 \\(G(s)=s\\)，这没有意义；），因为在前面我们知道OTP的一大缺陷就在于双方需要共享很长（与密文等长）的秘钥。假若我们可以通过 \\(G(\\cdot)\\) 和一个较短的真随机数 \\(s\\) 来获得一个较长的（和密文等长）的比特串，那么就可以部分解决OTP的问题了 我们称一个 \\(G(\\cdot)\\) 是伪随机函数，当且仅当对于任意PPT的算法 \\(D\\)，都有 \\(|Pr\\left[D(G(s))=1\\right]-Pr[D(r)=1]|\\leqslant\\text{negl}(n)\\)，这里 \\(r\\) 是一个真随机串，其中 \\(D\\) 输出 \\(1\\) 表示算法认为这是一个伪随机函数。 这个式子的含义是：如果任何高效的算法都无法以一个大于 \\(\\text{negl}\\) 的概率区分出我们的\\(G(\\cdot)\\)和真随机的比特串，那么我们就可以认为这个生成器有比较好的随机性——即伪随机性 注意到定义中我们没有对伪随机生成函数做出任何行为上的定义，即这样的函数其实是理想的模型，定义并不是构造的。通常我们可以认为目前已有的一些随机数生成器具有PRG的性质，即\"某样东西是PRG\"属于安全性证明前提中的假设部分 实现 具体的典型PRG实现一般通过一个有限状态自动机来完成，即我们的每一步构造其实都是确定性的，但是每一步都依赖于随机种子 \\(s\\) 一类比较经典的PRG其实可以生成任意长的比特串，然后再根据需要连续截取来获得不同的随机串 应用 如果把OTP的真随机变成这里的PRG，那么就可以得到一个Computationally Secure的加密策略。注意到与原本需要共享与明文等长的key不同，此处只需要双方共享PRG的key，而根据PRG的定义这个key是要比明文的长度短的。这样就通过牺牲一些安全性（多了一个\\(\\text{negl}\\)）解决了key太长的问题 Multiple Encryptions 前面的EAV-secure都是基于一个假设的：每次通讯只会传输一条密文。这个假设显然是不够强的——我们肯定会有多条消息传输的需求，在这种情况下，之前的某些安全加密策略就变得不安全了 同样先定义一个游戏 \\(Priv^\\text{mult}_{\\mathcal A,\\Pi}(n)\\) 如下： 首先生成一个key \\(\\mathcal A\\) 给出两条明文向量 \\(\\vec{m_0},\\vec{m_1}\\) Challenger随机一个比特 \\(b\\)，把 \\(\\vec{c_b}\\) 发回给 \\(\\mathcal A\\) \\(\\mathcal A\\) 来猜 \\(b\\) 如果 \\(Pr[Priv^\\text{mult}_{\\mathcal A,\\Pi}(n)=1]\\leqslant\\dfrac1 2+\\text{negl}(n)\\)，那么就称策略 \\(\\Pi\\) 是mult-secure的（这个词是我自己发明的....囧） 一个很重要的观察是，如果加密算法是确定性的，那么这个策略一定不是mult-secure的，攻击构造如下： \\(\\mathcal A\\) 只需要给出 \\(\\vec{m_0}=(M_1,M_1)\\)，\\(\\vec{m_1}=(M_1,M_2)\\)，其中 \\(M_1 eq M_2\\) 对于接收到的 \\(\\vec{c_b}=(C_1,C_2)\\)，若 \\(C_1=C_2\\) 就猜 \\(b=0\\)，否则猜 \\(b=1\\) 用这个 \\(\\mathcal A\\) 去玩上面的游戏正确率是\\(1\\)，因此这个策略不是mult-secure的。我们还可以发现前面讲过的弱化OTP也是确定性的，因此做不到mult-secure 这提醒我们要给加密算法引入随机性，引入随机性的方法就是PRF Pseudo random Functions 首先要引入functionality的概念，即一个概率的函数。对于函数 \\(\\hat F\\)，若对于某输入 \\(x\\) 而言，其结果 \\(\\hat F(x)\\) 满足某概率分布，那么我们就称其为一个 functionality。可以发现传统的definite function也是一类特殊的functionality 所谓PRF类似PRG，就是期望通过一类确定性的算法来模拟出随机函数的行为。因此概率的引入就全部在于密钥 \\(k\\) 了 也就是说我们有一个函数的集合(A family of functions) \\(Func\\)，我们可以通过一个密钥 \\(k\\) 来实例化出一个具体的函数 \\(F_k\\)，这是一个确定性的函数。 由于 \\(k\\) 是完全随机的，所以在Adversary 眼中 \\(F_k\\) 的行为也应该是完全随机的。但是因为我们的生成算法是多项式的，因此 \\(|Func|\\) 也是多项式的，即 \\(Func\\) 只能做到非常有限的采样，因此 \\(F_k\\) 又不可能是完全随机的（这一段要体会一下） 我们称一个函数集（族）是伪随机函数当且仅当对于任意PPT的算法 \\(D\\)，都有 \\(\\left|Pr\\left[D^{F_k(\\cdot)}(1^n)=1\\right]-Pr\\left[D^{f(\\cdot)}(1^n)=1\\right]\\right|\\leqslant\\text{negl}(n)\\) 成立。其中 \\(D\\) 输出 \\(1\\) 表示算法猜这是一个伪随机函数 这个式子的随机性引入来自两方面：1. \\(k\\) 的选取 2. \\(f\\) 是一个真随机函数 这里之所以把 \\(F\\) 写在右上角是习惯问题，意思是 \\(D\\) 不需要自己计算函数值，而是通过访问我们人为提供的oracle（谕示机，这名字好屌）来瞬间得到答案 这么做是因为如果我们把某个函数作为参数传入，则可能会需要处理指数级的信息，这与多项式时间复杂度是矛盾的。 实现 对PRG进行定长采样就可以得到一个PRF了 应用 可以构造如下加密策略： \\(\\text{Enc}(m)\\) 会随机生成一个 \\(r\\)，然后把 \\(\\left&lt;r,m\\oplus F_k(r)\\right&gt;\\) 作为密文输出 \\(\\text{Dec}(\\left&lt;r,c\\right&gt;)\\) 则通过计算 \\(c\\oplus F_k(r)=m\\oplus F_k(r)\\oplus F_k(r)=m\\) 就能得到明文 这个策略的安全性的保障在于，对于Adversary而言 \\(k\\) 是未知的，而 \\(F\\) 是PRF。要证明也很简单，只需要把 PRF换成真随机函数，这样就可以确保 \\(\\mathcal A\\) 只有 \\(\\dfrac12\\) 的成功率(why?)，而一个能有效攻击PRF策略的算法，必然也能用于有效区分PRF和真随机函数（证明这一点需要构造一下），这与PRF的定义矛盾，因此是不存在这样的策略的。 Weakly Pseudo random Function 密码学的一个套路就是先引入一个问题，然后通过实践或者需求来强化/弱化某些条件，再猜想/证明这些情况下的构造仍然具有一些良好的性质 所谓Weakly Pseudo random的意思就是，对于任意的PPT算法 \\(D\\)，都有 \\(\\left|Pr\\left[D^{F_k^\\$(\\cdot)}(1^n)=1\\right]-Pr\\left[D^{f^\\$(\\cdot)}(1^n)=1\\right]\\right|\\leqslant\\text{negl}(n)\\) 对任意的 \\(\\text{negl}(n)\\) 成立 和PRF的区别在于，这里我们给 \\(D\\) 的是一个概率oracle，即每次 \\(D\\) 向oracle查询的时候，会由oracle生成一个随机数 \\(r\\)，然后把 \\(\\left&lt;r,F(r)\\right&gt;\\) 给 \\(D\\) 意思是虽然 \\(D\\) 有能力知道一些 \\(F\\) 的取值，但是具体取到哪些 \\(x\\) 并不由 \\(D\\) 决定 有一个定理：一个PRF一定是WPRF，但反之不一定成立。证明比较简单，因为是课后习题所以不放具体证明了 一个经典的构造如下：如果我们有一个PRF \\(F\\)，那么我们就可以构造一个WPRF \\(F&#39;(x)=\\left\\{\\begin{aligned}F(y)&amp;,&amp;x=y||0\\\\F(y)&amp;,&amp;x=y||1\\end{aligned}\\right.\\) 根据这个例子可以发现，构造同样长度的PRF要比WPRF简单的多，因为我们只需要一个更短的PRF就可以弱化得到一个更长的WPRF了","tags":["Cryptography"]},{"title":"形式语义04 Types","path":"/2021/10/05/Semantics04-Types/","content":"Types 首先要说明什么是Type Types可以看成是对数据的分类、一种约定，即我们用一个界来描述一类数据构成的集合，用不同的界区分不同的数据种类。对于untyped的语言，我们则可以看成是只有唯一一种包罗万象的type 类型实际上有很多作用，可以进行针对性的优化、可以提供部分代码的信息、可以作为接口分离各模块的逻辑、可以保证程序的正确执行..... 如果写过Coq的话，还会知道类型可以与逻辑系统中的元素建立对应关系，从而可以利用类型推导工具来进行定理的证明（虽然我感觉这是绕了一个大弯，毕竟类型系统本来就应该看成一类特殊的逻辑系统），也就是Curry-Howard morphism 这一节讲的就是在\\(\\lambda\\)-calculus中引入type Errors 分为trapped errors和untrapped errors trapped意思是就是出现使得程序停止的错误（除0） untrapped意思是程序虽然继续运行，但是状态被破坏了（回想注入攻击） forbidden-error则指的是一个untrapped errors的超集 Safe &amp; Well-behaved 我们称一个程序是safe的，当且仅当它执行过程中不会出现untrapped error。如果一个语言的所有合法程序都是safe的，那么我们就称这个语言是safe的 我们称一个程序是well-behave的，当且仅当它在执行过程中不会出现forbidden-errors。 于是就可以定义type safe了。如果一个语言经过了type system检查的程序都是safe的，那么我们称这个语言就是type safe的 实践中通常不明确定义forbidden-errors、well-behave以及safe，一般混着来，不过不影响理解 Annotations 把变量 \\(x\\) 的类型为 \\(\\tau\\) 记作 \\(x:\\tau\\) 把\"以类型 \\(\\tau_1\\) 作为输入、\\(\\tau_2\\) 作为输出\"的函数 \\(f\\) 的类型记作 \\(f:\\tau_1\\rightarrow \\tau_2\\) 有些变量的类型由环境决定，例如表达式 \\(f\\;1\\)，在 \\(f:\\text{int}\\rightarrow\\text{int}\\) 和 \\(f:\\text{int}\\rightarrow\\text{double}\\) 两种情况下的类型就不一样，因此需要引入环境的概念，用函数 \\(\\Gamma:\\text{Types}^\\text{var}\\) 表示，意思是给free variable分配类型，也可以理解成假设 在Simply Typed \\(\\lambda\\)-calculus里，类型仅由基本类型以及通过函数对基本类型进行复合两种方法得到。很显然类型的数量是可数的 以及若干推导规则： \\(\\Gamma,x:\\tau_1\\vdash e:\\tau_2\\Rightarrow \\Gamma,x_1:\\tau_1\\vdash \\lambda x.e:\\tau_1\\rightarrow\\tau_2\\)，意思是函数的类型由参数类型和返回值类型确定。 $,x:x:$，意思是在相同环境下，同名变量有相同类型 \\(\\Gamma,e_1:\\tau_1\\rightarrow\\tau_2,e_2:\\tau_1\\vdash e_1\\;e_2:\\tau_2\\)，意思是给一个函数传入参数就可以得到返回值的类型 可以发现，一个类型系统类似一个逻辑系统，我们有若干公理（变量类型）、推导规则，并且我们希望所有的命题都可以通过推理得到，也都有相应的语义含义 Soundness &amp; Completeness 也有叫Consistency的 如果一段代码能给所有表达式标上类型：即每个表达式的类型都可以在类型系统中推导得到，那么就称类型系统accept了这段代码 我们称类型系统sound，当且仅当所有通过的代码都不会出错 我们称类型系统complete，当且仅当所有不会出错的代码都能通过检查 很快就能反应过来对于递归可枚举的图灵完备的语言而言，这个问题是不可判定的。因此通常的做法是牺牲completeness追求soundness，意思是也许你的做法是对的，但我们建议用其它可以通过检查的方法来写；并且所有通过的代码都必须是安全的 在Simply Typed \\(\\lambda\\)-calculus中，不出错就意味着： 如果 \\(\\vdash M:\\tau\\) 并且 \\(M\\overset *\\rightarrow M&#39;\\)，那么就有 \\(\\vdash M&#39;:\\tau\\) 并且要么 \\(M&#39;\\) 是一个value，要么 \\(M&#39;\\) 可以继续规约。这里value通常就定义为normal form 上面两条分别对应了下面的两个定理 一个证明上述type system不complete的例子如下： \\((\\lambda x.(x\\; (\\lambda y.y))(x\\;3))(\\lambda z.z)\\) 关键就在于对于同一个 \\(x\\)，它既作用于 \\((\\lambda y.y)\\)，又作用于 \\(3\\)，因此我们不能推导得到 \\(x\\) 的类型，因此也就无法通过类型系统的检查 但是稍微规约一下就可以得到 \\((\\lambda y.y)\\;3=3\\)，最终是可以停止并得到值的，意思是这段代码不会出错 Progress TH 如果 \\(\\vdash e:\\tau\\)，那么要么 \\(e\\) 是value，要么存在 \\(e\\rightarrow e&#39;\\) 证明这个只需要对推导次数进行归纳就可以了 引理1：如果一个拥有 \\(\\tau_1\\rightarrow\\tau_2\\) 类型的表达式 \\(e\\) 是value，那么它一定是 \\(\\lambda x.E\\) 的形式 引理1的证明：观察类型推导的规则就可以发现，能够给出 \\(\\tau_1\\rightarrow\\tau_2\\) 类型的规则只有一条。反证即可说明 Progress的证明： base case 是很简单的，这里就不写了 设当推导次数 \\(n=k\\) 时命题成立，分类讨论第 \\(k+1\\) 次推导时表达式 \\(e\\) 的结构： 常量，此时 \\(e\\) 是value，符合； \\(e=x\\)，此时环境为空，不可能有 \\(\\vdash x:\\tau\\) \\(e=\\lambda x.E\\)，此时 \\(e\\) 是value，符合； \\(e=e_1\\;e_2\\)，分类讨论 \\(e_1,e_2\\) \\(e_1\\) 不是value，那么由归纳假设，存在 \\(e_1&#39;\\) 使得 \\(e_1\\rightarrow e_1&#39;\\)，符合； \\(e_1\\) 是value，\\(e_2\\) 不是value，那么同理存在 \\(e_2\\rightarrow e_2&#39;\\)，符合； \\(e_1,e_2\\) 都是value，根据引理有 \\(e_1=\\lambda x.E\\)，于是有 \\(e_1\\;e_2\\rightarrow E[e_2/x]\\)，符合； 由数学归纳法得命题对任意自然数次的推理成立 关键在于为什么需要引理，以及何时使用引理 Preservation TH 意思是如果 \\(\\vdash e:\\tau\\) 且 \\(e\\rightarrow e&#39;\\)，那么有 \\(\\vdash e&#39;:\\tau\\) 引理2：若 \\(\\Gamma,x:\\sigma\\vdash E:\\tau\\)，且 \\(\\Gamma\\vdash e:\\sigma\\)，那么 \\(\\Gamma\\vdash E[e/x]:\\tau\\) 证明比较简单，只需要对 \\(e\\) 推导归纳再分类讨论就好了 仍然对推理次数进行归纳，同样省去base case 对 \\(e\\) 的结构分类讨论： \\(e\\) 是常量，那么不存在 \\(e&#39;\\)； \\(e=x\\)，那么不存在 \\(e&#39;\\) \\(e=\\lambda x.E\\)，那么不存在 \\(e&#39;\\) \\(e=e_1\\;e_2\\)，那么就有 \\(\\vdash e_1\\;e_2:\\tau\\)，并且 \\(\\vdash e_1:\\sigma\\rightarrow \\tau,e_2:\\sigma\\) 存在 \\(e_1\\rightarrow e_1&#39;\\)，那么就有 \\(e\\rightarrow e_1&#39;\\;e_2\\)。根据归纳假设有 \\(\\vdash e_1&#39;:\\sigma\\rightarrow\\tau\\)，又根据类型推导规则有 \\(\\vdash e_1&#39;\\;e_2:\\tau\\)，归纳步骤成立； 存在 \\(e_2\\rightarrow e_2&#39;\\)，那么就有 \\(e\\rightarrow e_1\\;e_2&#39;\\)。根据归纳假设有 \\(\\vdash e_2&#39;:\\sigma\\)，再根据类型推导规则有 \\(\\vdash e_1\\;e_2&#39;:\\tau\\) \\(e_1,e_2\\) 都是normal form，那么根据引理1有 \\(e_1=\\lambda x.E\\) 的形式，于是 \\(e=e_1\\;e_2=\\lambda x.E\\; e_2\\)。此时显然有 \\(e\\rightarrow E[e_2/x]\\)，根据引理2有 \\(\\vdash E[e_2/x]:\\tau\\) 于是就证完了","tags":["Formal Semantics"]},{"title":"密码学03 Computational","path":"/2021/09/27/Crypto03-Computational/","content":"Background 前面提到的perfect secrecy虽然好，但有着理论上的局限：key太长、key太多等等，用起来不是那么方便 一个idea就是，我们放弃部分安全性来换取更实用的密码。这里要回答几个问题：放弃哪些？放弃多少？放弃之后的安全性如何衡量？ 注意，接下来的讨论如无特殊说明，都以eavesdropper作为threat model Computational Security 首先回答\"放弃哪些\"，这部分直接产生了Computational Security的概念 perfect secrecy要求即使是有无穷算力的窃听者也不能得到任何关于明文的信息（概率分布不变），而Computational Security就妥协了加粗的两处： 仅考虑一个比较强的窃听者，意思是窃听者虽然有一定计算资源，但并不是无上限的。并且在给定足够时间的条件下，窃听者可以威胁到信息安全。但是如果我们能把破译所需的算力要求设定得很高，那么我们就可以保证在一段时间内明文的安全 窃听者能够以一个概率成功破译。如果我们能让这个概率很低，那么就可以让窃听者破译成为极小概率事件 Informally Define Security Levels 既然作出了妥协，那么最好能明确说明\"究竟要放弃多少\"，这部分主要有两个方法 Concrete Approach 这个很好理解，所有的concrete approach都可以理解为如下句式： “在窃听者破译花费时间不超过\\(T_0\\)时，他破译的成功率至多为\\(P_0\\)”，这样的密码也被称为\\((T_0,P_0)\\)-secure的密码 这么做的好处和坏处都是显然的。一句话就是，通常这个结果是不够普遍，但具体有用的 好处是很直观，把最直接应用的条件给出来了 坏处是这只是一个关于时间上界的一个概率上界，我们不能知道更多的信息（如果花费了更多时间概率是多少？如果只花费一半时间概率又会是多少？不知道）。当然还要明确各种技术进步的影响、计时的衡量等等。 Asymptotic Approach 意思是我们给每个密码策略赋予一个整数的security parameter，通常记作\\(n\\)。而一个密码策略被攻破的概率就可以写成\\(n\\)的函数\\(P(n)\\)，而一个窃听者的计算就可以用计算复杂度函数\\(T(n)\\)来表示 Negligible Function 如果一个函数\\(F(n)\\)有：\\(\\forall c,\\exists N\\in\\mathbb N^+\\) 使得当 \\(n&gt;N\\) 时恒有 \\(F(n)&lt;\\dfrac{1}{n^c}\\)，那么我们就称 \\(F(n)\\) 是negligible的 其实可以换个方式理解，如果\\(\\dfrac{1}{F(n)}\\)在渐进意义下比任何多项式函数都要大，那么\\(F(n)\\)就是可忽略的 negligible有很好的性质： \\(\\forall F(n)\\in\\text{neg}\\)，任取正多项式函数 \\(p(n)\\)，我们有 \\(p(n)\\cdot F(n)\\in\\text{neg}\\) 这个只需要证明\\(\\text{neg}\\)乘上常数、乘上\\(x^k\\)、任意加减仍然是\\(\\text{neg}\\)就好了 Probabilistic Polynomial Time(PPT) Polynomial意思是存在一个多项式\\(p(n)\\)作为bound，使得任意情况下这个算法的复杂度不超过\\(p(n)\\) Probabilistic意思是这个算法可以获得真随机数源，即它可以获得任意多的均匀0/1随机比特 有了如上介绍，给出Asymptotic Approach的定义： 对于给定的\\(n\\)而言，如果任意概率多项式复杂度的算法都不能以高于的negligible函数的概率破解该密码，那么我们就称这个密码是安全的 同样列一下pros&amp;cons 好处就是我们可以通过调整parameter来构造出针对不同需求的（即对应于concrete method）密码，并且可以很好评估一个密码的性能 好像没啥坏处，因为concrete method可以看成是这个的特殊情况。 事实上efficient adversary和概率的界定从某种程度上来说是任意的，这里选择PPT和negligible有几个原因 在计算理论中多项式复杂度有特殊地位，这类算法被认为是高效的，并且Church-Turing命题保证了不同计算模型上多项式算法的不同实现仍然至多有多项式级别的差距，意思是还是多项式的 多项式函数具有一定的闭包性质(closure)，多项式的复合、四则运算仍然得到多项式 negligible也有很好的性质，任意多项式个\\(\\text{neg}\\)得到的仍然是一个\\(\\text{neg}\\)，意味着如果单次破译的概率是\\(\\text{neg}\\)的话，在多项式次尝试后破译概率仍然是\\(\\text{neg}\\)的 Private-key Encryption Scheme 一个私钥加密由一个PPT算法上的三元组构成 \\((\\text{Gen, Enc, Dec})\\) \\(\\text{Gen}\\)会输入一个表示parameter的参数（通常用一个unary number \\(1^n\\)表示parameter为 \\(n\\)），然后生成一个key。通常我们会假设 \\(|k|\\geqslant n\\) \\(\\text{Enc}\\)会输入一个表示明文的串\\(m\\in\\left\\{0,1\\right\\}^*\\)和\\(k\\)，然后得到一个密文\\(c\\in\\left\\{0,1\\right\\}^*\\) \\(\\text{Dec}\\)会输入\\(c,k\\)，然后得到对应的明文\\(m\\)。注意\\(\\text{Dec}\\)是deterministic的函数，并且\\(\\text{Dec}_k(\\text{Enc}_k(m))=m\\)。如果输入的密文在对应key下不能得到任何明文，还要求返回一个错误值 如果密码对\\(m\\)的长度有固定要求\\(\\mathscr l(n)\\)，那么我们称这是固定长度\\(\\mathscr l(n)\\)的私钥加密 Security Goal Definition semantic security是最早用来形式化computational security的定义。因为比较复杂所以咕咕咕 这里用的实际上仍然是adversary game的定义方式 Distinguish Game \\(\\mathcal A\\) 选取两条等长消息 \\(m_0,m_1\\)，记 \\(n=|m_0|\\)，把消息发出去 我们获得一个随机比特\\(b\\)，设定parameter为\\(n\\)得到一个key，并加密 \\(m_b\\) 发给 \\(\\mathcal A\\) \\(\\mathcal A\\) 猜 \\(c\\) 对应哪条明文，输出一个结果。\\(\\mathcal A\\) 猜测的正确性用 \\(\\text{PrivK}^{\\text{eav} }_{\\mathcal A,\\Pi}(n)\\)，表示（\\(1\\)正确\\(0\\)错误） 记一个密码 \\(\\Pi=(\\text{Gen, Enc, Dec})\\) 若对于任意PPT的算法\\(\\mathcal A\\) 都有 \\(\\left|Pr[\\text{PrivK}^{\\text{eav} }_{\\mathcal A,\\prod}(n)=1]-\\dfrac 1 2\\right|&lt;\\text{neg}(n)\\)，那么我们就称 \\(\\Pi\\) 是EAV-secure的 这个定义很直观，意思是无论我用什么PPT的策略猜，在多项式时间内最多也只能够比随机猜要好出一个 \\(\\text{neg}\\) 注意到perfect secrecy的定义是这个的特例，对比可以发现妥协了恰好就是上面提到的两项 另一个等价定义： 定义 \\(\\text{out}_{\\mathcal A}(n,b)\\) 表示一次固定了 \\(b\\) 的 \\(\\mathcal A\\) 的输出，那么EAV-secure 等价于 \\(\\left|{Pr[\\text{out}_{\\mathcal A}(n,0)=1]-Pr[\\text{out}_{\\mathcal A}(n,1)=1]}\\right|&lt;\\text{neg}(n)\\) 意思是对于一个adversary，它在\\(b\\)不同的情况下表现几乎一致。注意到 \\(\\mathcal A\\) 的功能仅仅是输出一个bit，因此\"表现\"意味着输出结果是什么，而\"几乎一致\"则可以用仅仅相差 \\(\\text{neg}\\) 衡量 证明它们的等价性只需要注意到 \\(Pr[\\text{PrivK}^\\text{eav}_{\\mathcal A,\\Pi}(n)=1]=\\dfrac1 2Pr[\\text{out}_\\mathcal A(n,0)=0|b=0]+\\dfrac1 2Pr[\\text{out}_\\mathcal A(n,1)=1|b=1]\\) 以及 \\(Pr[\\text{PrivK}^\\text{eav}_{\\mathcal A,\\Pi}(n)=1]+Pr[\\text{PrivK}^\\text{eav}_{\\mathcal A,\\Pi}(n)=0]=1\\) 即可","tags":["Cryptography"]},{"title":"形式语义03 Lambda","path":"/2021/09/16/Semantics03-Lambda/","content":"\\(\\lambda\\)-calculus Background 首先这是一种编程语言，在1930s被Alonzo Church和Stephen Cole Kleene发明（两位都是听说过的明星人物） 还是一种计算模型，在1937年被Alan Turing证明其和图灵机的表达能力等价（这位更是重量级） \\(\\lambda\\) 演算是函数式编程的基础，同时它简单的特点也使得很适合用于研究PL的各个领域（回忆IDFS中的\\(\\lambda\\) 表达式作为transfer function） PL的东西有点乱，大家的说法和记号也都不一样，这里选择了可能算是比较好理解又简洁的一种 Syntax 定义 \\(\\mathcal V\\) 是无穷变量集合，\\(\\Sigma=\\mathcal V\\cup\\left\\{(,),\\lambda,.\\right\\}\\) 为字符集，那么定义一个term为\\(\\Sigma\\)上满足的某些约束的有穷串 \\(\\lambda\\) calculus term组成的集合 \\(\\Lambda\\subseteq \\Sigma^*\\) 定义为满足如下条件的最小集合： 若 \\(x\\in\\mathcal V\\)，那么 \\(x\\in\\Lambda\\) 若 \\(M,N\\in\\Lambda\\)，那么 \\((MN)\\in\\Lambda\\)，此时称 \\(N\\) 是parameter 若 \\(x\\in\\mathcal V\\) 且 \\(M\\in\\Lambda\\)，那么 \\((\\lambda x.M)\\in\\Lambda\\)，此时称 \\(M\\) 为这个term的body 上面三种形式分别叫做variable、application、\\(\\lambda\\) abstraction(function) 为了简略（这也太简略了），有一些约定俗成的优先级和结合律： 最外层括号可以省略不写，例如 \\((\\lambda x.x^2)=\\lambda x.x^2\\) Application形式是左结合的，即 \\(f\\;x\\;y\\;z=((f\\;x)\\;y)\\;z\\) 一个\\(\\lambda\\)的约束范围(scope)向右延伸，body的范围尽可能长。例如 \\(\\lambda x. x+y=\\lambda x.(x+y) eq(\\lambda x. x)+y\\) 多个\\(\\lambda\\) 可以写在一起。例如 \\(\\lambda x.\\lambda y.\\lambda z.=\\lambda xyz.\\) Semantics Bound &amp; Free Variables 对于一个形如 \\(\\lambda x.N\\) 的term，我们就说 \\(N\\) 中出现的 \\(x\\) 都被绑定(binding)到了 \\(\\lambda x.\\) 上，并把 \\(\\lambda x.\\) 叫做binder 注意绑定是可以嵌套的，例如 \\(\\lambda x. (\\lambda f.\\lambda x.f\\; x)+x\\)，最里面的 \\(x\\) 被绑定到了第二个 \\(\\lambda x.\\)，而最后一个 \\(x\\) 则被绑定到了第一个 \\(\\lambda x.\\) 这个规则和C的作用域是很相似的 如果一个变量没有被绑定，就被称为自由变量(free variables) 一个bound variable是可以被换名字而不改变term的含义的，类似于C function中形参的名字其实是任意的（但仍然有限制，后面会说）。而free variable则不能随意换名字。因此我们主要关注一个term \\(M\\) 中具体的free variables有哪些，用 \\(FV(M)\\) 这个记号来表示 容易有如下规则： \\(FV(x)=\\left\\{\\;x\\;\\right\\},\\; x\\in\\mathcal V\\) \\(FV(\\lambda x.M)=FV(M)-\\left\\{\\;x\\;\\right\\}\\) \\(FV(M N)=FV(M)\\cup FV(N)\\) 也就是结构归纳 Substitution 这里的substitution和直接替换是有区别的，注意我们substitute的必须是free variable 我们用 \\(M[x/y]\\) 或者 \\(M\\left\\{x/y\\right\\}\\)、\\(M[y:=x]\\) 来表示用 \\(x\\) 替换掉term \\(M\\) 中的 \\(y\\) 的结果，并把一次substitution记作 \\(M\\rightarrow M[x/y]\\) 考虑如下C代码 int x=1; int y=2; int foo(int z) &#123; return x + y + z; &#125; 这里第5行中，x,y是free variable，z被绑定到形参int z 前面说到对函数的形参可以任意换名但是有条件。考虑以下三种情况： 作替换\\([x/z]\\)，这时候第5行变为return x + y + x;，且这里的两个x都被绑定到形参int x上（形参也被替换了），含义和原文显然是不同的 作替换\\([u/z]\\)，这时候u从未出现过（称为fresh variable），这个替换就没有任何问题 作替换\\([z/x]\\)，这时候x从free variable变成了bound variable，含义也发生了变化 基于如上考虑，我们给出substitution的递归定义： \\(x[N/x]\\overset{\\text{def} } =N\\) \\(y[N/x]\\overset{\\text{def} }=y\\) \\((M N)[P/x]=(M[P/x])(N[P/x])\\) \\(\\lambda x.M[N/x]=\\lambda x.M\\) 若 \\(y ot\\in FV(N)\\)，那么 \\((\\lambda y.M)[N/x]=\\lambda y.(M[N/x])\\) 若 \\(y\\in FV(N)\\)，那么 \\((\\lambda y.M)[N/x]=\\lambda z.(M[z/y][N/x])\\) 1234都好理解，主要说说56 5的意思是如果 \\([N/x]\\) 不会引入新的 与当前形参重名的free variable，那么就直接换 6的意思是如果非要换，那么就得先把 \\(M\\) 中被绑定到当前 \\(\\lambda y.\\) 上的 \\(y\\) 都换成一个fresh variable \\(z\\)，此时就规约成5的情况了。最开始我以为也可以先换掉 \\(N\\) 中的 \\(y\\)，但是注意free variable的含义，如果换掉了的话语义就变了。 \\(\\alpha\\)-conversion 也叫\\(\\alpha\\)-renaming，意思是我们把形如 \\(\\lambda x. M\\) 这样的term的body中与binder同名的变量连同binder一起改名字（改成一个从未出现过的变量）得到的term和原来等价。这样的等价关系叫做\\(\\alpha\\)-equivalence、\\(\\alpha\\)-congruence。 \\(\\beta\\)-reduction 讲的是如何reduce一个term。递归定义如下： \\((\\lambda x. M)N\\rightarrow M[N/x]\\)，这就是基本的\\(\\beta\\)-reduction 如果 \\(M\\rightarrow M&#39;\\)，那么 \\(\\lambda x.M\\rightarrow\\lambda x.M&#39;\\) 如果 \\(M\\rightarrow M&#39;\\)，那么 \\(M N\\rightarrow M&#39; N\\)，\\(N M\\rightarrow N M&#39;\\) \\(\\beta\\)-reduction是满足上述条件的最小二元关系，显然是自反、传递的 再定义 \\(\\overset*\\rightarrow\\) 是 \\(\\beta\\)-reduction的自反传递闭包，这个在后面会用来刻画confluence性质 \\(\\beta\\)-redex 和 \\(\\beta\\)-normal form 引入这俩的意图是为了判断reduction何时停止 \\(\\beta\\)-redex=\\(\\beta\\)-reduction expression，意思是形如 \\((\\lambda x.M)N\\) 这样的term。这样的形式仍然可以根据\\(\\beta\\)-reduction rule来进一步化简 \\(\\beta\\)-normal form指的是不含\\(\\beta\\)-redex的项，注意到所有的reduction都是基于\\(\\beta\\)-reduction定义的，不含\\(\\beta\\)-redex意味着不能再reduce。如果我们把reduction看成映射，那么就可以认为是到达了一个reduction操作下的不动点 \\(\\eta\\)-reduction 这个主要是利用了函数的外延性等价。意思是说，如果两个函数对于相同的输入有相同的输出，那么它们就可以互相替换 一个例子也是从sicp中来的。假如我们要构造有理分数这一数据类型，并打算用pair来构造，那么可以写成如下形式： (define (make-rat a b) (cons a b)) 也可以这么写： (define make-rat cons) 具体到\\(\\lambda\\)-calculus就是： 若 \\(f=g\\)，那么 \\(\\lambda x. f\\; x=\\lambda x.g\\; x\\) Confluence Church-Rosser Confluence Theorem： 对于任意的term \\(M\\)，若存在两个不同的reduction序列使得 \\(M\\overset*\\rightarrow A\\)，\\(M\\overset*\\rightarrow B\\)，那么就必然存在一个term \\(N\\) 使得 \\(A\\overset*\\rightarrow N\\) 且 \\(B\\overset*\\rightarrow N\\)。注意\\(A\\)、\\(B\\)、\\(N\\)都有可能相等 推论：在\\(\\alpha\\)-equivalence下，每个term如果存在\\(\\beta\\)-normal form，那么这个形式是唯一的 反证即可，如果存在两个那么就违反了上述定理。 需要注意的是，这里并没有说“任意推导序列都能得到normal form”，只是说 如果从同一个term开始推导，那么两个推导序列中将会存在一个公共项（不一定是normal form） 如果一个term开始推导能得到一个normal form，那么这个形式在\\(\\alpha\\)-equivalence意义下唯一 Reduction Strategies 回忆scheme求值的正则序和应用序（即先代换形参还是先对实参求值）策略，正好对应了\\(\\lambda\\)-calculus的不同reduction\"路径\"，也就是策略。 实际上就是取 \\(\\rightarrow_\\beta\\) 这个二元关系的一个子集，在牺牲一些推导能力的前提下使每步推导确定下来 之所以会出现策略的不同，是因为一个term可能存在多个\\(\\beta\\)-redex，每一步的选取就造成了化简序列的差异。而虽然我们知道normal form唯一，但并不是所有的推导序列都能终止，也并不是所有的term都有normal form。 一个比较好玩的例子就是 \\((\\lambda x.x\\; x)(\\lambda x.x\\; x)\\)，这就是没有normal form的term 利用上面的例子可以构造出如下式子 \\((\\lambda u.\\lambda v.v)((\\lambda x.x\\; x)(\\lambda x.x\\; x))\\)，它就存在一个无法终止的推导序列 Full \\(\\beta\\)-reduction 每次从所有可以reduce的项中挑一个。很显然这是单步非确定性的。 Normal-order reduction 每次选择最左、最外的redex 有定理：Normal-order reduction一定能找到normal form（如果存在的话） Applicative-order reduction 每次选择最左、最内的redex Call by Name reduction 规则与Normal相同，唯一的额外要求是不能对function body化简 Call by Value reduction 一个abstraction term能被化简，当且仅当它apply到了一个value上。这意味着所有的function在apply之前都要先对argument求值。这是比较熟悉的经典做法。 这两种规约的效率谁更优是不一定的 Evaluation Strategy 好像搜了很多地方都没有找到类似的定义，可能这是村规 Evaluation和Reduction的区别在于 Evaluation只要求最后是一个特殊的形式(canonical form) Evaluation会尽可能避免对function body化简 Canonical Form 意思是形如 \\(\\lambda x. M\\) 这样的term 一个Closed Normal Form一定是Canonical Form（所有变量都bounded，且不能再规约，意味着最外面是\\(\\lambda x. M\\) 的形式），反之则不然（很显然一个CF仍然可能被规约简化） Normal Order Evaluation Rules 注意到，如果normal order reduction停止了，那么在规约过程中一定存在一个canonical form。因此提出normal order evaluation的求值策略，规则如下 \\(\\lambda x. M\\Rightarrow \\lambda x.M\\)，意思是在此处停止 如果\\(M\\Rightarrow\\lambda x. M&#39;\\)，且 \\(M&#39;[N/x]\\Rightarrow P\\)，那么\\(M\\; N\\Rightarrow P\\)，意思是先把function body规约成canonical form(也就是一个标准的function形式)，再带入parameter，最后整体化简 Eager Evaluation Rules 这个考虑的则是applicative order reduction的evaluation，即先把parameter化成CF，再进行形参代换 \\(\\lambda x. M\\Rightarrow \\lambda x. M\\) 若\\(M\\Rightarrow \\lambda x.M&#39;\\)，\\(N\\Rightarrow N&#39;\\)，\\(M&#39;[N&#39;/x]\\Rightarrow P\\)，那么 \\(M\\; N\\Rightarrow P\\)，注意观察与上面的不同 Fun Func 给几个好玩的例子吧，可以在lambda calculus interpreter里面玩一玩 Bool \\(True=\\lambda x.\\lambda y. x\\) $False=x.y. y $ 编码其实是任意的，类比函数等价定义的外延性（在相同输入下有相同输出），数据等价定义为它们在相同操作下有相同的行为 negate就比较取巧了 \\(negate = \\forall b. b\\;False\\;True\\) 然后可以写一个mux，可以方便后面的二元函数 即\\(if(A)then(B)else(C)=(A\\;B)\\;C\\) 于是就可以很容易地写出and和or和xor，实际上只需要写出三个中的一个就完备了 \\(and=\\lambda x.\\lambda y. if(x)then(y)else(False)\\) \\(or=\\lambda x.\\lambda y.if(x)then(True)else(y)\\) \\(xor=\\lambda x.\\lambda y.if(x)then(negate\\; y)else(y)\\) Natural 如何判断一个自然数是\\(0\\) \\(isZero=\\lambda n.(n\\;(\\lambda x.False)\\;True)\\) 注意到当\\(n=0\\)时得到的是单位函数，否则得到常函数\\(False\\) 其他部分都快写烂了，跳 Recursion 这个比较好玩，之前算是没怎么搞懂 还是那个玩烂了的例子，我们要算阶乘 很容易写出 \\(fact=\\lambda n.if (isZero\\;n)then(1)else(mult\\;n\\;(fact\\;(pred\\; n)))\\) 问题在于等式两侧都出现了\\(fact\\)，在具体语言中就表现为我们必须给递归函数一个名字才能调用递归 接下来就是很神奇的操作了 考虑这个函数 \\(Func=\\lambda f.\\lambda n.if (isZero\\;n)then(1)else(mult\\;n\\;(f\\;(pred\\; n)))\\)，它接受一个函数 \\(f\\)，返回一个\\(f\\)的递归调用。那么上面的定义就可以解释为\\(fact\\)是函数\\(Func\\)的不动点，即\\(Func\\;fact=fact\\) 也就是说，虽然我们不能给\\(fact\\)命名，但是我们可以通过一个不含\\(fact\\)的式子把它算出来 算不动点也是很神奇的操作 回忆之前的神奇表达式 \\(\\Omega=(\\lambda x.x\\; x)\\,(\\lambda x.x\\; x)\\)，可以构造（怎么想到的？！）一个新的函数 \\(Y=\\lambda F.(\\lambda x.F(x\\;x))\\,(\\lambda x.F(x\\;x))\\) 观察 \\(YF=(\\lambda x.F(x\\; x))\\,(\\lambda x.F(x\\;x))=F((\\lambda x.F(x\\;x))\\,(\\lambda x. F(x\\;x)))=F(YF)\\) 大概的idea就是我们希望每次apply完之后，前面多出一个\\(F\\)而后面保持不变，这样就可以得到不动点 那么阶乘就可以写成 \\(Y\\; Func\\)","tags":["Formal Semantics"]},{"title":"密码学02 Perfect","path":"/2021/09/15/Crypto02-Perfect/","content":"概率论前置技能 其实就三个公式 条件概率公式： \\(Pr[B|A]=\\dfrac{Pr[A\\wedge B]}{Pr[A]}\\)，这个是定义 贝叶斯公式 \\(Pr[B|A]=\\dfrac{Pr[A|B]\\cdot Pr[B]}{Pr[A]}\\)，这个只需要按照上面的展开就可以证明 全概率公式 \\(Pr[A]=Pr[A|B_1]\\cdot Pr[B_1]+Pr[A|B_2]\\cdot Pr[B_2]+\\cdots +Pr[A|B_n]\\cdot Pr[B_n]\\)，意思是把整个概率空间作划分，然后分别考虑这些划分内部事件 \\(A\\) 的那部分，最后合并在一起 获取随机数 注意到一套encryption scheme包括一个Gen()操作生成一个key，我们会需要一个随机生成数据的方法来产生密钥 首先我们需要收集若干high-entropy的数据，然后通过特殊的处理来生成nearly independent and unbiased bits high-entropy意味着高不确定性，而nearly independent and unbiased则对最终产生的数据有一些要求 一个例子就是扔硬币。假设每次扔硬币事件独立，正反面概率分别为 \\(p,1-p\\)，那么我们可以扔很多次，然后： 如果出现连续的\"正反\"，就写下一个\"1\"；如果出现连续的\"反正\"，就写下一个\"0\" 注意到它们的概率都是 \\(p(1-p)\\)，因此就得到了一个uniformly distributed output 通常用作cryptography的随机数算法有特殊的要求，因此不是所有的随机数生成器都可以用于特定的加密算法的，这一点需要注意（也即正确使用加密算法的注意事项） 一些定义 \\(\\mathcal {K,M,C}\\) 分别表示密钥、消息、密文空间（即集合），\\(\\text{Gen,Dec,Enc}\\) 分别表示密钥生成器、解密器、加密器三个算法/函数。通常我们规定 \\(|\\mathcal M|&gt;1\\)，这是因为如果你要发送的信息永远只有一种，那就没有破译的必要而只存在是否发送这一区别了....即信息是不确定性的度量，因此消息空间必须具有不确定性（每次可能传送的消息不止一种） \\(\\text{Gen}\\) 会随机产生一个key，而 \\(\\Im \\text{ Gen}=\\mathcal K\\) \\(\\text{Enc}\\) 会输入一个 \\(m\\in\\mathcal M\\)，并根据key \\(k\\) 产生一个密文 \\(c\\in\\mathcal C\\)。我们允许 \\(\\text{Enc}\\) 是probabilistic的，即每次都按照一定概率生成出不同的密文。通常写作 \\(c\\leftarrow \\text{Enc}_k(m)\\)。这是一个functionality \\(\\text{Dec}\\) 会将给定的密文 \\(c\\in\\mathcal C\\) 根据key \\(k\\) 产生对应的原文 \\(m\\)，我们要求即使 \\(\\text{Enc}\\) 并非是确定性的，也应该有 \\(\\text{Dec}(\\text{Enc}_k(m))=m\\) 成立 我们传统上认为一段原文应该是确定的，但事实并非如此。一个独特的视角是，原文 \\(m\\) 可以看成是对 \\(\\mathcal M\\) 上具有特定概率分布的随机变量 \\(M\\) 进行采样得到的结果，这是站在攻击者的视角考虑的。 \\(\\mathcal M\\) 上的概率分布与加密策略无关，而只由使用者决定。 我们通常假设 \\(K\\) 和 \\(M\\) 这两个随机变量是独立的，意思是发送消息的人的消息分布不应该受到他所使用的加密策略的影响。 如果我们确定了加密策略和发送消息的人（即 \\(\\mathcal M\\) 上的概率分布），那么就确定了 \\(\\mathcal C\\) 上的概率分布。 Perfect Secrecy 一些关于攻击者的假设： 攻击者可以窃听密文 攻击者可以知道所有可能发送消息的集合和消息空间的概率分布 同时这个攻击者也知道加密策略 但他仅仅只是不知道具体哪条消息被发送了而已 Perfect Secrecy的意思就是，攻击者无法通过密文来改变他对 \\(\\mathcal M\\) 上的概率分布的认识 形式化写出来就是： \\(\\forall m\\in\\mathcal M,c\\in\\mathcal C:Pr[M=m\\;|\\; C=c]=Pr[M=m]\\) 这里要求 \\(Pr[C=c]&gt;0\\) 用条件概率拆开就是 \\(Pr[M=m\\wedge C=c]=Pr[M=m]\\times Pr[C=c]\\) 另一种等价表述是这样的： \\(\\forall m,m&#39;\\in\\mathcal M, c\\in\\mathcal C:Pr[\\text{Enc}(m)=c]=Pr[\\text{Enc}(m&#39;)=c] \\text{ (1)}\\) 也就是说，对于给定的任意的密文 c，我们都无法区分是明文m加密成了它还是明文m'加密成了它 有引理：加密策略是perfectly secret的当且仅当它满足\\((1)\\) 式 \\(\\Rightarrow\\) 需要注意到这样一个等式：\\(Pr[C=c|M=m]=Pr[c=\\text{Enc}_k(m)]\\) 意思是说，如果我们已经知道了 \\(M=m\\) 的条件，那么 \\(C=c\\) 的概率就是 \\(c=\\text{Enc}_k(m)\\) 的概率。因为原文已经作为条件给出，因此我们可以这么做 于是就有 \\(Pr[c=\\text{Enc}_k(m)]=Pr[C=c|M=m]=\\frac{Pr[M=m|C=c]\\times Pr[C=c]}{Pr[M=m]}\\) 又根据perfectly secret的定义我们有 \\(Pr[M=m|C=c]=Pr[M=m]\\)，因此带入上式就有 \\(Pr[c=\\text{Enc}_k(m)]=Pr[C=c]\\) 注意到此处 \\(m\\) 是任意的，因此自然满足等式\\((1)\\)，证毕。 反方向类似，贝叶斯公式拆开再用那个观察到的等式和全概率就可以啦 Perfect adversarial indistinguishability 好长的名字... 第三个等价的，用一个game表述的perfect secrecy是这样的： 有一个假设拥有任意算力的攻击者 \\(\\mathcal A\\)，他可以选择两个明文 \\(m_0,m_1\\in\\mathcal M\\)，然后交给一个决策者 决策者会随机一个\\(0/1\\)比特 \\(b\\)，然后把 \\(c=\\text{Enc}_k(m_b)\\) 交给 \\(\\mathcal A\\)，由 \\(\\mathcal A\\) 判断这是哪个明文加密形成的 对于一个加密策略，我们用三元组 \\(\\Pi=(\\text{Gen, Enc, Dec} )\\) 表示，并用 \\(\\text{PrivK}^{eav}_{\\mathcal A,\\Pi} =1\\) 表示某一轮猜测的结果正确，用 \\(0\\) 表示猜测错误 一个很naive的策略就是一直猜\\(0\\)，那么这样正确率就是\\(\\frac{1}{2}\\)的，因为 \\(b\\) 是随机的。 若对于任意的 \\(\\mathcal A\\)，都有 \\(Pr[\\text{PrivK}^{eav}_{\\mathcal A,\\Pi}=1]=\\frac{1}{2}\\)，那么我们称这个加密策略是perfectly adversarial indistinguishable的 可以证明这个定义和perfect secrecy的定义是等价的。因为是作业内容所以就不放证明了 这里的 \\(\\mathcal A\\) 并不一定要是具体的人，当然也可以是某种算法、某段程序。 One Time Pad 这个策略的perfect secrecy是由大名鼎鼎的香农证明的。 给出构造如下： 对于一个固定的正整数 \\(l\\)，我们规定 \\(\\mathcal M=\\mathcal C=\\mathcal K=\\left\\{\\;0,1\\;\\right\\}^l\\) \\(\\text{Gen}\\) 等概率从 \\(\\mathcal K\\) 中选取一个串，\\(\\text{Enc}\\) 则输出key和m按位异或的结果，\\(\\text{Dec}=\\text{Enc}\\) 很显然这个策略满足我们对加密策略一般性的要求，也不难证明这是perfectly secret的 对于任意的 \\(c\\in\\mathcal C,m\\in\\mathcal M\\)，我们固定这样的 \\(c,m\\)，于是 \\(Pr[C=c|M=m]=Pr[\\text{Enc}_k(m)=c]=Pr[m\\oplus K=c]=Pr[K=c\\oplus m]\\) \\(K\\) 是key的随机变量，而 \\(c,m\\) 是任意定的串，于是这个式子就是从key space中取出一个特定串 \\(c\\oplus m\\)，概率即为 \\(\\frac{1}{2^l}\\) 又 \\(Pr[C=c]=\\sum\\limits_{m\\in\\mathcal M}Pr[C=c|M=m]\\times Pr[M=m]=\\frac{1}{2^l}\\sum\\limits_{m\\in\\mathcal M}Pr[M=m]=\\frac{1}{2^l}\\) 于是我们的老朋友贝叶斯公式又来了 \\(Pr[M=m|C=c]=\\frac{Pr[C=c|M=m]\\cdot Pr[M=m]}{Pr[C=c]}=Pr[M=m]\\)，就证明完了 这个加密策略确实很牛逼，但是存在一些问题使得现在很少用它。 Key太长了，实际上必须和m等长。如果可以安全运输这么长的key，为啥不直接运输m呢？ Key必须是One-time的。假设一个攻击者连续获取到了两次用同样key的信息\\(c_1,c_2\\)，那么根据\\(\\text{Enc}(x)=x\\oplus k\\)，很容易就能得到 \\(c_1\\oplus c_2=m_1\\oplus k\\oplus m_2\\oplus k=m_1\\oplus m_2\\)，这就泄露了部分信息。结合1使得这个方法变得非常昂贵 Limitations 有如下定理： 给定perfectly secret scheme \\(\\Pi=(\\text{Gen, Enc, Dec})\\)，则 \\(|\\mathcal K|\\geqslant |\\mathcal M|\\) 由反证法，假设 \\(|\\mathcal K|&lt;|\\mathcal M|\\)，那么取如下集合： \\(\\mathcal M(c)=\\left\\{\\;m\\;|\\;\\text{Dec}_k(c)=m\\text{ for some }k\\in\\mathcal K\\;\\right\\}\\) 注意到 \\(\\text{Dec}\\) 是一个函数，因此 \\(|\\mathcal M(c)|\\leqslant|\\mathcal K|&lt;|\\mathcal M|\\) 即 \\(\\mathcal M(c)\\subseteq\\mathcal M\\)。取 \\(m&#39;\\in\\mathcal M\\backslash\\mathcal M(c)\\)，那么就有 \\(Pr[M=m&#39;] eq0\\)，但是 \\(Pr[M=m&#39;|C=c]=0\\)，因为 \\(c\\) 不可能被解密为 \\(m&#39;\\) 这个结论很厉害，直接把这条路封死了。","tags":["Cryptography"]},{"title":"形式语义01 Intro","path":"/2021/09/09/Semantics01-Intro/","content":"感觉第一节课都差不多 推荐了Software Foundations，搜了一圈发现非常劝退....但还是磨磨蹭蹭看完了Lists 书上说不建议贴答案和题解，那就不贴了吧（ developing general abstractions, or building blocks, for solving problems, or classes of problems. Also considers software behavior in a rigorous and general way, to prove that programs enjoy properties 当我们提及程序的正确性时，我们在谈论什么？ How to describe meanings of programs? 含义 How to describe properties of programs? 性质 How to reason about programs? 推理 How to tell if two programs have the same behaviors or not? How to design a new language? How to build Bug-Free Software? 现在的软件也越来越复杂 Multi-core concurrency Embedded software, Limited resources Distributed and cloud computing, Network environment Ubiquitous computing and IoT Quantum Computing 测试的局限性 简单，容易自动化、流程化 无法保证一定没有bug 对并发(多核/网络)程序作用有限，发现的bug难以重现 Testing shows the presence, not the absence of bugs. Dijkstra 对于Crash-Proof工作，如何证明？ How to prove mathematically? How to define \"crash\"? How to prove a system is \"crash-free\"? 为什么要上这门课？ 软件可靠性是目前严重的问题 是别人没有的优势 可以提升代码能力 更好地理解和比较编程语言 这门课很有挑战 课程内容 对现有语言的研究 讲一些定义程序行为的方法 讲一些证明程序性质的方法 如何定义性质 如何证明 Coq 可以自动化证明命题 可以自动化验证证明 我们不再需要信任证明，只需要信任证明工具，然后检验一个证明 也就是说，一个程序可以自带一个安全性proof，而运行环境自带的证明器用以验证，以此来保证程序是正确的","tags":["Formal Semantics"]},{"title":"形式语义02 Math","path":"/2021/09/09/Semantics02-Math/","content":"Basic Set Theory 没啥好讲的 \\(\\bigcap S=\\left\\{\\;x\\;|\\; \\forall T\\in S,x\\in T \\;\\right\\}\\) 记 \\(R=\\bigcap\\emptyset\\)，则 \\(\\forall x. \\forall T\\in\\emptyset\\wedge x\\in T\\rightarrow x\\in R\\) 注意到命题的前件为假，因此 \\(R\\) 是\"a set of everything\" 根据幂集公理这是不成立的，因此我们规定它无意义。 Relations 没啥好讲的 Functions \\(f\\subseteq D(f)\\times R(f)\\)，且 \\(\\forall x,y,y&#39;\\)，\\((x,y)\\in f\\wedge (x,y&#39;)\\in f\\Rightarrow y=y&#39;\\) Total Functions \\(f\\) on \\(A\\times B\\) \\(A=D(f)\\) Partial Functions \\(f\\) on \\(A\\times B\\) \\(D(f)\\subseteq A\\) \\(\\lambda\\)-Expression \\(\\lambda x\\in D. f(x)\\) 和 \\(\\left\\{\\;(x,f(x))\\;|\\;x\\in D\\;\\right\\}\\) 是等价的 Function Variant 我们用 \\(f\\left\\{x\\leadsto y\\right\\}\\) 记号表示 \\(f\\cup (x,y)\\) 这个新的函数，此处的 \\(x\\in D(f)\\) 不一定成立 显然 \\(D(f\\left\\{x\\leadsto y\\right\\})=D(f)\\cup\\left\\{\\;x\\;\\right\\}\\) Function Type 我们用 \\(A\\rightarrow B\\) 表示 \\(A\\mapsto B\\) 的所有函数，且规定 \\(\\rightarrow\\) 右结合 即 \\(A\\rightarrow B\\rightarrow C=A\\rightarrow (B\\rightarrow C)\\) ，表明这是一个 \\(A\\times B\\mapsto C\\) 的函数，或者 \\(A\\mapsto (B\\mapsto C)\\) 的函数 这里给出的 Function Type 默认是对应集合的 Total Function. 很显然 \\(A_1\\mapsto A_2\\mapsto \\cdots\\mapsto A_n\\mapsto R\\) 的表达能力要强于 \\(A_1\\times A_2\\times\\cdots\\times A_n\\mapsto R\\)，即我们可以先后给出参数 表达能力的弱化无需额外做什么，而强化则需要currying操作 Tuples as Functions 二元组 \\((x,y),\\, x,y\\in D\\) 可以看成是 \\(\\left\\{\\;0,1\\;\\right\\}\\mapsto D\\) 的一个函数，即 \\(f(0)=x,f(1)=y\\)，\\(D\\times D\\) 就可以看成是 \\(\\left\\{\\;0,1\\;\\right\\}\\mapsto D\\) 的所有函数 形式化地写出来就是 \\(A\\times B=\\left\\{\\;f\\;|\\;D(f)=\\left\\{\\;0,1\\;\\right\\}\\wedge f(0)\\in A\\wedge f(1)\\in B\\;\\right\\}\\) 于是自然，\\(A_0\\times A_1\\times\\cdots\\times A_{n-1}\\) 就可以看成是 \\(\\left\\{\\;0,1,2\\ldots n-1\\;\\right\\}\\mapsto D\\) 的所有函数，其中 \\(D=\\bigcup\\limits_{i=0}^{n-1} A_i\\) 写出来就是 \\(\\prod\\limits_{i=0}^{n-1} A_i=\\left\\{\\;f\\;|\\;D(f)=[0..n-1]\\wedge\\forall i\\in[0..n-1].f(i)\\in A_i\\;\\right\\}\\) 再进一步就是 \\(\\prod\\limits_{i\\in I}S_i=\\left\\{\\;f\\;|\\;D(f)=I\\wedge\\forall i\\in I.f(i)\\in S_i\\;\\right\\}\\) Product of Functions 现在，我们令 \\(\\theta: \\alpha\\mapsto \\left\\{\\;S_\\alpha\\;\\right\\}\\)，即是以 \\(\\alpha\\) 为指标集的一个集合族，\\(\\theta\\) 将一个下标映射到集合族里对应下标的某个集合 我们同样可以定义 \\(\\theta\\) 上的乘积(product) 即 \\(\\sqcap\\theta=\\left\\{\\;f\\;|\\;D(f)=\\alpha\\wedge \\forall x\\in\\alpha.f(x)\\in\\theta(x)\\;\\right\\}\\)，其中 \\(\\alpha=D(\\theta),\\;\\theta(x)=S_x\\) 当 \\(\\theta\\) 是常函数时，\\(\\forall x. \\theta(x)=S\\)，那么 \\(\\sqcup\\theta=\\left\\{\\;f\\;|\\;D(f)=\\alpha\\wedge\\forall x\\in\\alpha. f(x)\\in S\\;\\right\\}=\\prod\\limits_{i\\in\\alpha}S=S^\\alpha=\\left\\{\\;f\\;|\\;f:\\alpha\\mapsto S\\;\\right\\}\\) 大概的意思是说，我们有一个定义域 \\(\\alpha\\) 到若干集合的映射 \\(\\theta\\)，现在把每个 \\(\\alpha\\) 中的元素 \\(x\\) 的像限制到 \\(\\theta(x)\\) 里的一个具体元素，就可以得到一个具体的 \\(D(\\theta)\\rightarrow \\bigcup R(\\theta)\\) 函数。如果我们取遍所有可能的像，那么就恰好遍历完了所有这样的函数。 具体到程序设计语言，就是若干个 Type 经过这样的操作，就可以得到一个 Product Type. 即我们把一个 Type 看成是一个单点函数的集合，那么就可以通过简单的 Product 操作复合得到新的 Type. Sum of Functions 对于任意集合 \\(A,B\\)，规定 \\(A+B=\\left\\{\\;0\\;\\right\\}\\times A\\cup\\left\\{\\;1\\;\\right\\}\\times B\\) 称 \\(A+B\\) 为 \\(A,B\\) 的不交并。 自然就有 \\(\\sum\\limits_{i\\in \\alpha} S(i)=\\left\\{\\;(i,x)\\;|\\;i\\in\\alpha\\wedge x\\in S(i)\\;\\right\\}\\) 同样的，如果我们把 \\(\\theta\\) 看作是 \\(\\alpha\\rightarrow \\bigcup\\limits_{i\\in\\alpha}S(i)\\)，那么就有 \\(\\Sigma\\theta=\\left\\{\\;(i,x)\\;|\\;i\\in\\alpha\\wedge x\\in S(i)\\;\\right\\}\\) 当 \\(\\theta\\) 是常函数 \\(\\theta(i)=S\\) 的时候，\\(\\Sigma\\theta=\\sum\\limits_{x\\in\\alpha}S=\\left\\{\\;(x,y)\\;|\\;x\\in\\alpha\\wedge y\\in S\\;\\right\\}=\\alpha\\times S=\\alpha\\rightarrow S\\) 这个的意思是，我们可以通过若干 Type 得到 Sum Type，新的 Sum Type 中的变量只能是组成它的若干 Type 中的某一个。 感想 写着写着就突然悟了，注意到一个编程语言中的类型可以看成是值域 \\(R\\) 构成的集合 而带类型的变量可以看成是单点函数的集合 \\(\\left\\{\\;f\\;|\\;f:\\left\\{\\;var\\;\\right\\}\\rightarrow R\\;\\right\\}\\)，变量的一个具体取值则对应一个具体的单点函数。需要说明的是，我们这里不妨规定变量两两不重名(否则可以很容易用自然数命名而不改变程序的行为) 那么上面说的 Sum 和 Product 实际上就是通过简单类型构造出复杂类型的两类方法 而我们知道，一个程序的所有状态由所有变量的取值组成，那么一个程序的特定状态就可以表达成一个具体的函数 \\(P:\\left\\{\\;\\;|\\;x\\text{ is variable}\\;\\right\\}\\)","tags":["Formal Semantics"]},{"title":"密码学01 Intro","path":"/2021/09/02/Crypto01-Intro/","content":"密码学关心的问题和应用 Secrecy，即信息是否泄露 Integrity，即信息是否被篡改 Oblivious Transfer(不经意传输) Zero Knowledge Proof(零知识证明) Secure Multi-party Computation(多方计算) Digital Currency(数字货币) 如何定义安全？CPA-secure、CCA-secure.... 观点一： Only I know the encryption algorithm and keys, so it's safe. 观点二： It would takes 100 years to break the system for an adversary with a currently most advanced computer using the best known method. 严格证明: Computationally Security Game-Based Proof Simulation-Based Proof 如何取随机数？什么是伪随机序列？如何衡量伪随机序列的随机性？ Oblivious Transfer 你可以找女孩A和B中其中一个女孩的号码，但是不能让另一个人知道 一些概念 现代加密方法包括如下几个部分： \\(\\mathcal M\\) 表示信息空间，即所有可以加密的信息的集合 \\(\\mathcal K\\) 表示密钥空间，即所有密钥组成的集合 \\(\\text{Enc,Dec}\\) 表示加密、解密方法 \\(\\text{Gen}\\) 表示密钥生成器 一个比较显然的要求是 \\(Dec(Enc(M))=M\\)，即加密后解密的信息要保真 Classical Cipher 给了几个例子 凯撒密码(Caesar Cipher) 定义函数 \\(next:\\Sigma\\mapsto\\Sigma\\) 为 按照某种顺序排列 \\(\\Sigma\\) 的内容，某字符的下一个字符是什么 然后构造映射 \\(f_k=next^k\\)，其中 \\(k\\) 是非 \\(0\\) 常正整数，那么 \\(f_k\\) 和 \\({f_k}^{-1}\\) 就是一对加密和解密算法了。 \\(k=3\\) 的情况就是经典凯撒密码 这个密码问题很大 首先这是一个固定的算法 其次 \\(k\\) 的值域很小，只需要做 \\(26\\) 次就可以得到所有情况 由此自然引出 Sufficient Key Space Principle(充分密钥空间原则) 任何加密算法必须有足够大的密钥空间，使得不会被暴力枚举破解。 一个原则是密钥要尽可能地长 但是足够大的密钥空间并不一定能保证安全，MAS加密是一个例子 ROT-13 取 \\(k=13\\) 的凯撒密码 这个密码的一个好处在于，\\(f_{13}={f_{13} }^{-1}\\)，即加密和解密是一样的 Mono-alphabetic Substitution Cipher(单字母替换密码) 构造一个permutation \\(\\pi:\\Sigma\\mapsto\\Sigma\\) 那么 \\(\\pi\\) 和 \\({\\pi}^{-1}\\) 就是加密和解密。这里permutation 的意义在于，如果不是双射的话，就无法解密了 凯撒密码可以看成是这个的特例。这类密码的一大特色就是要有一个对应小本本 容易计算，MAS密码的密钥空间就是排列数量 \\(\\left|\\Sigma\\right|!\\)。取小写字母表就是 \\(26!\\approx 2^{88}\\) MAS的最大缺点在于，它永远将相同字符加密成相同的字符(双射)，即它完全保存了原文的所有信息。这使得第三方可以基于统计学原理来进行攻击。例如说\"u常在q后，h很可能在t后\"之类的统计学规律，常见的模式会被保留，甚至说直接用字母频率的相对大小进行匹配。 Poly-alphabetic Substitution Cipher(多字母替换密码) 考虑构造这样构造的排列 \\(\\pi:\\Sigma\\times\\Sigma\\mapsto \\Sigma\\times\\Sigma\\) 也就是我们把每两位字符映射为两位字符。这样可以部分解决相同字母映射下的象永远相同的问题 缺点也很明显，我们需要更大的空间储存映射表 Vigenere Cipher 这个密码首先需要一个catch phrase，例如说 \\(phrase=cafe\\) 然后构造 \\(\\pi:\\Sigma\\mapsto\\Sigma\\)，规定 \\(\\pi(code_i)=f_{phrase_{i\\pmod |phrase|} }(code_i)\\) 可以发现这实际上就是一个特殊的PAS密码，即我们每 \\(|phrase|\\) 位造一个映射，并且每个映射都是相同的 VC的破解 分成几个步骤 求\\(phrase\\)的长度 \\(m\\) 把密文每 \\(m\\) 位分组，那么模 \\(m\\) 同余的位放在一起就是一个\\(k\\)未知的凯撒密码了 现在考虑怎么破译单次的凯撒密码 我们当然可以枚举\\(26\\)种可能的密钥，然后找到make sense的明文 但是由于判定明文是否make sense是不那么平凡的，因此书上给出了另一个可以自动化完成的做法 定义 \\(p_i\\) 表示第 \\(i\\) 个字符在英语中出现的频率，\\(q_i\\) 表示第 \\(i\\) 个字符在密文中出现的频率，假设密钥是 \\(k\\)，那么有 \\(\\sum {p_i}^2\\approx \\sum{q_{i+k \\text{ mod } 26} }^2\\) 于是我们可以计算不同key对应的 \\(\\sum p_iq_{i+k\\text{ mod } 26}\\)，然后按照和期望值的差来排序 那么就可以枚举 \\(m\\)，然后对模 \\(m\\) 相同的位做凯撒的破译，最后合并就好了 有一个小小的优化，它源自如下观察： 若两个位置 \\(i,j\\) 满足 \\(i\\equiv j\\pmod {m}\\)，且明文对应的第 \\(i,j\\) 位相同，那么密文的对应位置上的字符也相同 那么就可以寻找长度较短的、出现了多次的密文的子串，\\(m\\) 一定是用它们之间的距离的一个因数。还可以多次寻找重复的pattern，枚举这些距离的GCD的因数来求 \\(m\\) Modern Cipher 古典密码更像是艺术，缺乏科学的分析 现代密码学的目标在于：给定一个密码构造，可以严格证明它是安全的 需要证明，首先要： 定义什么是\"安全\" 需要给出一些假设(可以是未被证明的猜想) 安全性取决于安全目标(security goal)和攻击模型(threat model)，即不同的目标和不同水平的攻击，会使得安全性的定义发生变化。 security goal 这里用secure encryption举例 前几个要求相对而言比较基本，并且非常显然 攻击者无法通过密文获得密钥 攻击者无法通过密文获得明文 攻击者无法破译任意明文的任意字符 无论攻击者掌握了多少信息，密文不会泄露除此之外的任何信息 threat model Cipher text-only attack: 攻击者只知道密文 Known-plain text attack: 即已知明文攻击，攻击者在攻击前可以知道若干明文到密文的单向映射(比如说加密文件有固定的格式，有已知的片段) Chosen-plain text attack: 攻击者可以选择知道若干明文到密文的单向映射(比如说加密设施是公开的，那么攻击者就可以任意获得任意明文对应的密文，但反过来不行) Chosen-cipher text attack: 攻击者可以选择知道若干密文到明文的映射(比如说解密设施是公开的) assumptions 加密解密问题通常依赖于一些难以计算/不存在高效算法的问题 大数分解问题 SAT问题 选择假设的时候需要注意这几个点 选择虽未被证明，但是经过长时间检验的假设 选择更弱的假设。若存在两个假设A,B，其中A能推出B，那么选择A而不是B。这样在B被证伪后，A仍然坚挺。如果两个假设不可比较，那么选择被研究得更深入的假设 在假设被证伪后，需要针对性地研究它在证明一个加密方法时扮演的角色 provable security 基于我们的假设，针对特定的攻击者，达到了一些安全目标。这样的安全性被称为可证明的安全性 通常可证明的安全性是对现实中的安全性的一种建模，即仅针对重要部分形式化证明。 Kerckhoffs' Principle 古典密码的安全性很大程度上基于信息的不对称(即我用的什么加密方式，攻击者是不知道的) 柯克霍夫原则： The cipher method must not be required to be secret, and it must be able to fall into the hands of the enemy without inconvenience. 即现代密码的安全性仅依赖于密钥，而不是加密方式 这么要求的理由有以下几个： 相比起同时保密密钥和加密方法，只保存密钥要方便得多 相比起更改加密方法，定期更换密钥更容易保持安全 统一的加密方法更容易实现标准化 也就是说，尽量使用公开、经过验证的加密策略，配以定期更换的密钥，更能保护安全。所谓home-brewed算法很多其实经不起验证","tags":["Cryptography"]},{"title":"Compiler03 语法分析","path":"/2021/09/01/Compiler03-语法分析/","content":"写的时候复制到笔记2那里去了...晕，这个点还不睡就是不行啦 CFG 上下文无关语法(Context Free Grammar)，或者说BNF(Backus Naur Form)，是用于描述一类语言的法则，也即是语法 语法包括： 终结符号(terminal)集 \\(T\\) 非终结符号(nonterminal)集 \\(N\\) 推导规则(rule of inference) \\(R\\) 起始符号 \\(s_0\\) 任意规则有如下形式： \\(h\\rightarrow B\\)，其中 \\(h\\) 是非终结符，\\(B\\) 是 \\(T\\cup N\\) 上的串 对于给定的串 \\(s\\)，若存在推导规则 \\(r:h\\rightarrow B\\)，且 \\(h\\in s\\)，则用 \\(B\\) 替换 \\(s\\) 中的一个 \\(h\\) 得到新串 \\(s&#39;\\) 称为一次推导，记作 \\(s\\underset{r}\\Rightarrow s&#39;\\)，也可以简单记作 \\(s\\Rightarrow s&#39;\\) 若串 \\(s\\) 中出现多个 \\(h\\)，则我们称对最左侧的 \\(h\\) 的替换推导为最左推导(left-most)，同理有最右推导的概念 为了方便，规定 \\(a\\overset{*}\\Rightarrow b\\) 表示 \\(a\\) 经由零次或多次推导可以得到 \\(b\\)，而 \\(a\\overset{lm}\\Rightarrow b\\) 和 \\(a\\overset{rm}\\Rightarrow b\\) 分别表示一次最左和最右推导 从起始符号 \\(s_0\\) 开始，若存在串 \\(e\\) 使得 \\(s_0\\overset{*}\\Rightarrow e\\)，则我们称 \\(e\\) 是该语法的一个句型/句式。若 \\(\\forall x(x\\in e\\rightarrow x\\in T)\\)，则称 \\(e\\) 是一个句子 用 \\(L(T,N,R,s_0)\\) 表示该语法规定的语言，则容易知道所有的句子构成了语言本身(废话) 叫上下文无关，就是因为我们的推理规则要求推理的左侧只能是单独的非终结符，即替换可以发生在任何位置而与被替换的符号的上下文无关 表达能力 如何比较语法的表达能力？或者说，怎么判断一种新的语法和已知语法的表达能力强弱？ 直观地，若 \\(L(G_1)\\subseteq L(G_2)\\)，则我们称语法 \\(G_1\\) 表达能力不强于 \\(G_2\\)。 乔姆斯基把语法分成四类，其中两类便是CFG和RG(regular grammar)，可以证明他们的表达能力是存在差异的 命题：正则语法的表达能力严格弱于上下文无关语法 首先证明 \\(L(RG)\\subseteq L(CFG)\\)。根据正则表达式的递归定义，我们可以这么构造对应的CFG： \\(\\epsilon\\)，对应 \\(h(\\epsilon)\\rightarrow \\epsilon\\) \\(c,c\\in\\Sigma\\)，对应 \\(h(c)\\rightarrow c\\) \\(s|t\\)，其中 \\(s,t\\) 是正则表达式，对应 \\(h(s|t)\\rightarrow h(s)|h(t)\\) \\(st\\)，对应 \\(h(st)\\rightarrow h(s)h(t)\\) \\(s^*\\)，对应 \\(h(s^*)\\rightarrow h(s)h(s^*)|\\epsilon\\) \\((s)\\)，对应 \\(h((s))\\rightarrow h(s)\\) 注意到正则表达式长度有限，因此我们构造的非终结符有限，同理推导规则有限，因此这是一个CFG，并且可以归纳证明 \\(L(CFG)=L(RG)\\)，即任意正则表达式都存在一种上下文无关语言，使得他们的表达能力相等。这就说明了 \\(L(RG)\\subseteq L(CFG)\\) 再证明 \\(L(RG) eq L(CFG)\\)，即存在一种CFG能表达但RE无法表达的语言。这个构造很强，而且能说明很多问题，值得体会一会儿 考虑如下上下文无关语法： \\(S\\rightarrow aSb|\\epsilon\\) 它表示的是所有形如 \\(ab,aabb,aaabbb\\ldots\\) 的串的集合。下面证明正则表达式无法表示这种语言 注意到任意正则表达式可以化为等价的DFA，因此只需要证明不存在可以识别该语言的DFA即可 由反证法，假设存在这样一个DFA，不妨设其状态数为\\(n\\)，则根据抽屉原理在识别前\\(n\\)位的时候，必然存在 \\(i eq j\\) 使得 \\(tr(start,a^i)=tr(start,a^j)\\) 根据定义，DFA能识别串 \\(a^ib^i\\) 和 \\(a^jb^j\\)，再根据状态转移的等式有 \\(tr(start,a^ib^i)=tr(tr(start,a^i),b^i)=tr(tr(start,a^j),b^i)=tr(start,a^jb^i)\\in E\\)，即该DFA也能识别串 \\(a^jb^i\\)，这与假设矛盾。 也就是说，任意的 \\(n\\in \\mathbb N\\)，我们都能找到一个串使得其满足CFG规定的形式但是不能被DFA识别。于是证毕 关于正则/DFA的证明大多依赖于反证法和抽屉原理这两大保健，然后结合状态转移的结合性来说明某个状态的接受性，最后得出矛盾。同时这里的问题也直观说明了正则表达式没法表示同时向左和向右无限延伸的串，因为DFA一旦从某一位开始无限延伸，就意味着它前面必须是有限位。 (扯远点，也就是说CFL-Reachability不能简单地用DFA解决，而是要用一个PDA....一周前的我还是太naive了！) ST 从初始符号开始到任意句子的推导过程隐式地产生了语法树(Syntax Tree)的结构，即考虑任意单次推导 \\(a\\Rightarrow b\\)，如果我们把 \\(a\\) 视作根，\\(b\\) 中的所有字母看作儿子，那么树形结构就出来了。具体的证明只需要注意到每个字符的父亲唯一(上下文无关的定义)，结合句子的定义说明一下就好了。 有个比较显然的性质：任意语法树的叶子都是终结符，且从 \\(s_0\\) 到句子 \\(s\\) 的推导产生的语法树的 所有叶子的前序遍历序 恰好构成了 \\(s\\) 本身 最左推导 很显然一个推导序列唯一地构造了一棵语法树，然而一棵语法树并不唯一对应一个推导序列(考虑同一层出现了多个非终结符号，选择的次序就产生了不同的推导) 为了构造推导序列到语法树的双射，我们提出了最左推导的概念。即在最左推导下，语法树和推导序列是一一对应的。 二义性 注意到即使我们规定了最左推导，同一句子的推导仍然不唯一，构造的语法树也不唯一，因此自然引出二义性的问题 注意二义性是针对语法而言的而与串无关，即语法 \\(G\\) 有二义性当且仅当存在串 \\(str\\in L(G)\\)，使得存在两种从初始符号 \\(s_0\\) 开始的推导序列 \\(l_1,l_2\\) 满足 \\(s_0\\overset{*}{\\underset{l_1}\\Rightarrow}str\\) 且 \\(s_0\\overset{*}{\\underset{l_2}\\Rightarrow}str\\) 举个例子，考虑如下语法： \\(Expr\\rightarrow \\epsilon|Number|Expr+Expr|Expr*Expr\\) \\(Number\\rightarrow \\text{[1-9][0-9]*}\\) 这里的 \\(Number\\) 是用正则语言表达的 这个语法对于语句 \\(1+1*2\\) 就存在多种推导，但它们都是最左推导 例如 \\(Expr\\Rightarrow Expr+Expr\\Rightarrow 1+Expr\\Rightarrow 1+Expr*Expr\\Rightarrow 1+1*Expr\\Rightarrow 1+1*1\\) 和 \\(Expr\\Rightarrow Expr*Expr\\Rightarrow Expr+Expr*Expr\\Rightarrow 1+Expr*Expr\\Rightarrow 1+1*Expr\\Rightarrow 1+1*1\\) 具体表现出来就是+*优先级冲突的问题 二义性的消除 二义性的消除没有通用方法，需要根据语义和语法灵活处理(这不是说了等于没说吗喂) 虽然没有通用做法，但这一步还是必不可少的！ 小结一下就是 最左推导构造了语法树和推导序列的双射 二义性的消除保证了句子的最左推导序列唯一，即每个句子和语法树存在双射 左递归 若语法 \\(G\\) 存在 \\(A\\overset{*}\\Rightarrow A\\alpha\\)，其中 \\(A\\) 是非终结符(废话)，\\(\\alpha\\) 是 \\(N\\cup T\\) 上的非空串，则我们称 \\(G\\) 存在左递归(left recursion) 为了消除二义性我们需要规定一个推导顺序，而规定了推导顺序后自然引出终止的问题。容易发现存在左递归的文法 \\(G\\) 的推导过程不一定终止 这个很好玩，好玩的地方在于它有点像前面造DFA的时候用Arden's TH解正则式状态方程(事实上这是一个右线性文法，确实是正则语言...) 左递归的消除 考虑最简单的形式，即单个直接的左递归如何处理 \\(A\\Rightarrow A\\alpha|\\beta\\) 由 Arden's TH 可知 \\(A\\) 能产生的所有串用正则语言表达就是 \\(\\beta{\\alpha}^*\\)。注意到除了第一个字符外，剩下的部分都是若干片段的重复，因此考虑把两部分分开做。 不妨设 \\(A&#39;\\rightarrow \\alpha A&#39;|\\epsilon\\)，而 \\(A\\rightarrow \\beta A&#39;\\)，这样就用非左递归的方式表示出了原本存在左递归的语法 对于间接的左递归(即存在推导序列 \\(l\\) 使得 \\(A\\underset{l}\\Rightarrow A\\alpha\\))，我们这么考虑： 首先为所有的非终结符号编号，设共有 \\(n\\) 个，建图 对于一对非终结符 \\(A_i,A_j\\)，若存在推导序列 \\(A_i\\Rightarrow A_j\\alpha\\)，则连边 \\(i\\rightarrow j\\) 于是存在左递归\\(\\iff\\) 存在有向圈 问题变成了存在一些具有传递性的边(即 \\(i\\rightarrow j,j\\rightarrow k\\) 可得 \\(i\\rightarrow k\\))，我们已知怎么去掉一个点的自环(消除直接左递归)，问要如何消除所有的有向圈 一个做法就是规定编号为 \\(i\\) 的点只能连向编号大于 \\(i\\) 的点(容易归纳证明这样无环)，对于不满足的边我们用边的传递性使之越过若干中间节点。于是书上的算法就很好理解了 先写到这里，去睡觉/(ㄒoㄒ)/ Top-Down Parsing 再次回顾语法分析的目标：判断给定的串 \\(code\\) 是否是语法 \\(G\\) 的语言，如果是的话给出语法树结构 为了讨论的方便，下面提到的文法都是无二义性文法 考虑对起始符号 \\(s_0\\) 的一个最左推导，若 \\(s_0\\overset *{\\underset{lm}\\Rightarrow}code\\)，由文法无二义性可知推导序列唯一，且其作为最左推导唯一对应与一棵语法树。 Top-Down实际上就是在模拟这个过程。即我们每次从当前句型中取出最左的nonterminal \\(N\\)，根据某个 \\(N\\) 作为head的产生式替换得到新的句型 递归下降 没啥好讲的很sb的技术，也就是我们每次尝试所有可能的产生式，如果遇到了不能匹配情况就回溯.... LL(1) 文法 在认真研读龙书前，我对语法分析的理解停留在递归下降的层面，并好奇工业上究竟是怎么解决这么一个复杂的问题的.... 现实令人吃惊，通常面对一个很难的问题，我们不仅可以用精巧的做法解决它，还可以用更简单的case回避它！LL(1)文法就是这么出现的 准备工作 给定上下文无关文法 \\(G(N,T,s_0,R)\\) 首先定义两个函数 \\(FIRST(\\alpha)\\colon \\left(N\\cup T\\right)^+\\mapsto T\\cup\\left\\{\\;\\epsilon\\;\\right\\}\\) 和 \\(FOLLOW(A)\\colon N\\mapsto T\\) ，含义分别如下 \\(c\\in FIRST(\\alpha)\\) 当且仅当 \\(\\alpha\\overset *\\Rightarrow cy\\)，其中 \\(\\alpha\\) 是非终结符和终结符上的串， \\(c\\) 是某个终结符，\\(y\\) 是终结符上的串。若 \\(\\alpha\\overset*\\Rightarrow\\epsilon\\)，则规定 \\(\\epsilon\\in FIRST(\\alpha)\\) \\(c\\in FOLLOW(A)\\) 当且仅当 \\(\\exists yc\\in L(G)\\) 满足 \\(A\\overset*\\Rightarrow y\\)，其中 \\(A\\) 是某个非终结符，\\(c\\) 是某个终结符，\\(y\\) 是终结符上的串。特殊规定 \\(code\\) 的末端有一个终结符 \\(EOF\\)，且 \\(EOF\\) 只能出现在串的末尾 LL(1) 文法要求： 任意共享同一个head的两个产生式 \\(A\\rightarrow \\alpha\\) 和 \\(A\\rightarrow \\beta\\)，都满足 \\(FIRST(\\alpha) eq FIRST(\\beta)\\) 任意共享同一个head的两个产生式 \\(A\\rightarrow\\alpha\\) 和 \\(A\\rightarrow\\beta\\)，都满足至多有一个body的 \\(FIRST\\) 有 \\(\\epsilon\\) 任意共享同一个head的两个产生式 \\(A\\rightarrow\\alpha\\) 和 \\(A\\rightarrow \\beta\\)，若 \\(\\epsilon\\in FIRST(\\alpha)\\)，则必有 \\(FOLLOW(A) eq FIRST(\\beta)\\)，反之亦然。 琢磨一下这几条规则的含义需要首先搞懂两个函数在干什么 \\(FIRST(\\alpha)\\) 表明了句型 \\(\\alpha\\) 最终产生的所有句子的可能的第一个字符 \\(FOLLOW(A)\\) 表明了非终结符 \\(A\\) 产生的所有终结符序列(不一定是句子，因为 \\(A\\) 不一定是起始符号)在所有句子中后续紧邻着的字符 也就是说，LL(1)文法所有共享head的产生式都走向了不相交的分支，确定走哪一个只需要根据下一个非终结符的从属关系就能判断了 由此自然得到LL(1)文法的真正含义：从左往右读取(第一个L)，最左推导(第二个L)，推导非终结符时产生式的选择只需1个终结符即可判定 的文法 于是也就可以理解为什么上面要提到提取左公因子的做法了，只是为了把非LL(1)文法改造成LL(1)文法而已 求 \\(FIRST(),FOLLOW()\\) 先考虑怎么求FIRST 终结符的FIRST很好求 非终结符的FIRST只需要逐个判断产生式，再顺序判断产生式的每一项就好了 终结符和非终结符的串的FIRST可以看成是某个不存在的非终结符的body，那么就和上面一样了 再看看怎么求FOLLOW 起始符号的FOLLOW很好求，是\\(EOF\\) 对于每个非终结符，找到所有body包含它的产生式，往后根据FIRST规定集合的约束。有些来自FOLLOW，有些来自head的FOLLOW，这部分和FIRST是差不多的 制表 可以预处理出 \\(N\\times T\\) 的表，表示对于不同的状态(非终结符)和下一个输入(终结符)，我们应该采取哪一条产生式 由于任意两个产生式的FIRST不交，因此只需要枚举每个产生式和它body的FIRST，就可以得到表项了 对于表中空出的位置我们可以填入报错信息，也可以额外填入对应的错误处理方法 PDA 这个很有意思，但我还不是很懂，大概写一下定义就跑路( 下推自动机的形式化定义为一个七元组 \\((Q,\\Sigma,\\Gamma,\\delta,q_0,Z_0,F)\\) \\(Q\\)：状态的有穷集合 \\(\\Sigma\\)：符号的有穷集合 \\(\\Gamma\\)：一个叫堆栈(Stack)的数据结构，支持：1. 在顶端插入一个字符 2. 取出顶端的字符 3. 读取而不取出顶端的字符 \\(\\delta\\)：转移函数 \\(Q\\times \\Sigma\\times\\Sigma\\mapsto Q\\)，即转移状态由当前状态、输入和栈顶状态共同决定 \\(q_0\\)：初始状态 \\(Z_0\\)：初始符号，即初始栈中的元素 \\(F\\)：接受态的集合 Bottom-Up Parsing 自底向上语法分析期望做到这样一件事情：我们手上时刻保存着一个最右句型，并根据当前局面的情况作出如下操作：将连续若干符号收缩(用某条产生式的逆替换)成一个非终结符，且不改变最右句型这一性质。 换句话说，假设我们得到了起始符号 \\(s_0\\) 到最终句子 \\(c\\) 的一个最右推导序列 \\(l_{rm}\\)，那么我们需要希望通过对 \\(c\\) 的分析得到 \\(l_{rm}\\) 的逆 \\({l_{rm} }&#39;\\)。 具体的实现需要用到一个栈，保存当前最右句型仍未展开的部分；以及剩余全部已经展开为终结符的字符流。在栈顶和字符流的最左端，就是上一次发生推导的位置。注意到所有在当前栈顶的非终结符展开之后才展开的非终结符，必然都已经展开成了字符流(有点绕但一定要看明白)，否则违反最右句型的定义。因此每次面临的选择只有两个： 规约(Reduce)，将栈顶的若干符号根据某产生式的逆，替换为一个非终结符，记作一次规约 移入(Shift)，目前任意栈的前缀都无法匹配任何产生式的右端，表明所有栈顶位置之前的符号都已经规约完毕，继续规约需要用到栈顶位置往后的终结符，因此移入一个终结符到栈顶。 因此一个通用的LR Parser由四部分组成： token流的buffer 一个栈 一个GOTO表 一个ACTION表 与上面直观的分析不同，此处的栈储存的是状态 这里通用的真正含义在于：所有不同的LR方法的区别仅在于构造的GOTO和ACTION表不同，或者说构造算法存在差异。这就很适合自动生成了 实际上GOTO表、ACTION表、栈共同组成了一个下推自动机(Push Down Automata) GOTO和ACTION表 \\(GOTO[x,ch]=y\\) 表示在PDA的状态\\(x\\)，遇到非终结符 \\(ch\\) 时，会转移到状态 \\(y\\) \\(ACTION[x,tch]\\) 记录了在状态 \\(x\\)，遇到终结符 \\(tch\\) 时，应该进行什么操作(移入还是规约？移入什么状态？用哪条产生式的逆进行规约？)。这是不同LR策略之间的最本质区别 于是一个通用LR Parser的工作流程就可以描述为： 记栈顶为 \\(top\\)， 每次读入符号 \\(ch\\)，根据 \\(ACTION[top,ch]\\) 判断是否规约 如果规约，则从栈中弹出产生式的右端(对应的状态)。记产生式的左端为 \\(H\\)，那么当前到达的状态就是 \\(GOTO[top&#39;,H]\\)，其中 \\(top&#39;\\) 是弹出产生式右端后，得到的栈顶。 否则根据\\(ACTION\\)表，移入新的当前状态 在处理过程中可能出现下面的冲突情况： 移入-规约冲突，即当前可以规约，但是再移入几个可以规约成更大的非终结符 规约-规约冲突，即当前既可以规约栈的一个前缀\\(s\\)，又可以规约栈的另一个前缀\\(s&#39;\\) 一个好的LR策略，应当恰当地处理这些冲突(后面会看到SLR的做法就是不处理....) LR(0) 自动机 这是为后面的SLR技术作铺垫 其实是PDA的一个真子集，可以看成是一个特殊的DFA 首先对语法 \\(G\\)，我们构造等价的增广语法 \\(G&#39;\\)，唯一的区别在于增添一条产生式 \\({s_0}&#39;\\rightarrow s_0\\)，表示一个新的起始符号 项，项集 我们称一个项(term)为一个产生式加上一个点，例如 \\(A\\rightarrow ab\\cdot cde\\) 直观的含义就是，我们目前已经读到了这个产生式右端的某个前缀，于是在这个前缀的位置打上标记。特殊地 规定 \\(A\\rightarrow \\epsilon\\) 的项只有唯一的 \\(A\\rightarrow \\cdot\\) 不妨记所有项组成的集合为 \\(T\\)，一个粗略的估计为 \\(|T|=\\sum\\limits_{p\\text{ 是产生式，b是p的body} } {|b|+1}\\) 再造两个项上的操作 \\(cur:T\\mapsto \\mathbb N\\) 和 \\(next:T\\mapsto T\\) 规定 \\(cur(A\\rightarrow\\alpha\\cdot c\\beta)=c\\)，表示我们接下来需要一个终结符 \\(c\\) 规定 \\(next(A\\rightarrow\\alpha\\cdot c\\beta)=A\\rightarrow\\alpha c\\cdot\\beta\\)，表示我们又匹配上了一个字符 \\(c\\) 项集合的闭包操作 对于一个项组成的集合(a set of terms) \\(S\\)，我们构造地定义函数 \\(I:2^T\\mapsto 2^T\\) 如下： \\(S\\subseteq I(S)\\) \\(\\forall t\\in S\\)，若 \\(t\\) 中的点后是一个非终结符(即 \\(t\\) 的形式为 \\(H\\rightarrow \\alpha\\cdot A\\beta\\))，且存在产生式 \\(A\\rightarrow \\gamma\\)，则 \\(A\\rightarrow\\cdot\\gamma\\in I(S)\\) 感性的理解很直观，即如果我们已经读完了某个产生式右端的一个前缀，那么对于紧接着的一个非终结符 \\(A\\)，我们接下来可以读任何以 \\(A\\) 为head的产生式的右端的第一个符号。 项集作为状态的转移 现在有了一堆元素，自然考虑它们之间的关系以及它们和字符的相互作用。 对于一个项集 \\(S\\)，给定一个字符 \\(c\\)，则我们记 \\(trans(S,c)=T\\) 当且仅当： \\(\\forall t\\in T\\) 都有 \\(\\exists s\\in S\\) 使得 \\(next(s)=t\\) 且 \\(cur(s)=c\\) \\(\\forall s\\in S\\) 且 \\(cur(s)=c\\) 都有 \\(next(s)\\in T\\) SLR SLR(Simple LR Parsing)技术其实很简单，大概思路就是只分析最简单能分析得了的LR语法，其余一概不认 假设我们的项集为 \\(T=\\left\\{\\;I_0,I_1,I_2\\ldots\\;\\right\\}\\) 构造ACTION 假设我们要求 \\(ACTION[S,c]\\) 若 \\(A\\rightarrow\\alpha\\cdot\\in S\\) ，且 \\(c\\in FOLLOW(A)\\)，则用 \\(A\\rightarrow\\alpha\\) 规约 若 \\(A\\rightarrow\\alpha\\cdot c\\beta\\in S\\)，则规定移入 \\(trans(A,c)\\) 若 \\({s_0}&#39;\\rightarrow s_0\\in S\\)，则表明完成了parsing 最后在空的地方填上错误处理/报错 构造 GOTO 非常简单，规定 \\(GOTO[x,ch]=trans(I_x,ch)\\) 大概总结一下。LR分析最重要的就是解决\"何时规约\"的问题，因为如果不能规约就必须移入了 SLR给出的方案就是，如果我们走到了某个产生式的右端，且紧接着出现了一个可以立即出现在产生式左端之后的终结符，那么就立刻规约到这个产生式。其余的情况则是能移入就移入。其实也就是\"能规约就立刻规约，否则能移入就立刻移入\" 的意思...... SLR的缺点很明显，存在规约-规约冲突的时候不能做(因为这不是SLR语法的范围)，并且规约策略过于\"aggressive\"，导致可能得到的局部解走不到一个全局解。课程视频说LALR很好地平衡了LR(1)和SLR的优缺点，但是这部分我还没看。 错误恢复 很显然我们不可能每次编译只找到一个错就停下来。假设每次只报一个错，代码有一万个错的话..... 因此需要一种策略，在找到错误(无法匹配任何一种语法模式)时，迅速恢复到正常模式，对后续的(可能正常的)代码进行检测。 主要讲三种：panic mode和phrase level recovery，还有一种属于体贴型服务 xc原话\"由于程序猿写bug的创造力实在太强，甚至会搞爆编译器，因此要求编译器正确找出所有的错也是很困难的事情。\" error recovery的策略通常是根据经验得来的trick，也就是完全不具有普遍性。这也是为什么报错信息这么难读 Panic Mode 这一策略可以描述为：若栈顶的终结符 \\(A\\) 遇到了无法匹配的非终结符 \\(c\\)，则开始一直往后读输入的字符流，直到找到某个字符 \\(c&#39;\\in Syn(A)\\)，停止panic mode并弹出栈顶，然后往后继续分析。 这里的 \\(Syn(A)\\) 是一个字符集，它通常表示了一个语法单元的结束(例如一个分号，一个换行，一个右花括号)。意思是 \\(A\\) 已经识别不出来了，于是赶快找到(可能)包含 \\(A\\) 的最小(尽可能小)语法单元，把它和 \\(A\\) 一起丢弃。 举个生活中的例子就是，你一边听老师上课，一边记笔记。但是因为中间分神了，导致手上还在写第一页，老师已经讲到第五页了。于是你迅速往后跳，直到找到当前老师正在讲的位置，然后继续跟着记笔记(泪 Phrase Level Recovery 意思是我们可以对输入进行若干插入、删除、修改操作，使得它成为一段正确符合语法的串。 最直白的做法是最小编辑距离，但是这个东西在实际中效果并不好(书上原文 打表 对，预测程序猿可能犯什么错：行末没有分号，形参没有类型等等等等 属于是尽力在提升编译结果可读性，但是十分为难的sb做法。只能说确实有这个必要，但是这么做确实很无语....讲出来只是为了开开眼界(","tags":["Compiler"]},{"title":"Compiler02 词法分析","path":"/2021/09/01/Compiler02-词法分析/","content":"词法分析的作用 读取字符流，输出词法单元给语法分析器 在1的过程中去掉不必要的内容(空白符、注释)，查错报错 与符号表交互，插入符号的相关内容 虽然词法分析和语法分析是两个独立的部分，但它们通常在同一趟 为什么要独立词法分析 模块化 词法分析很简单，实现也很简单 PPT把1+2又说了一遍.... 词法分析和语法分析依赖的算法不同(有限状态自动机VS下推自动机) Token&amp;Pattern&amp;Lexeme Token是词法单元的抽象符号，可以理解成具体单词所属的类 Pattern是对一类词法单元形式的描述，它说明了这类词法单元“长啥样” Lexeme则是一个具体的词法单元(词素) 举例： Tokens:猫和狗 对应Pattern:会喵喵叫和会汪汪叫 对应Lexeme:我家的一只狸花和校门口的大黄 这也说明了为什么token需要额外的信息来标记，因为光凭token不能知道lexeme具体的内容 词法单元的形式规定(正则表达式) 串和语言 学一下形式化的定义.... 首先定义字符表(alphabet)为有限集\\(S\\)​​，那么字符表\\(S\\)​​上的串就定义为n元有序对\\(L=\\cup_{i\\in\\mathbb {N} }{S^i}\\)​​的集合中的元素 串\\(s\\)的长度定义为\\(|s|\\)，即\\(s\\)中符号的数量。我们称\\(L\\)为字符表\\(S\\)​上的语言。语言是可数的(可数个有限集的并仍然可数)，但不一定是有限的 对于特殊的长度为0的串记作\\(\\epsilon\\) 前缀：从串的尾部删除0个或多个字符得到的串 后缀：从串的前部删除0个或多个字符得到的串 子串：从串的前部和尾部删除0个或多个字符得到的串 子序列：从串中删除0个或多个字符得到的串 串操作 连接(concatenation)：把串y放在x后得到xy，注意这是不可交换的运算。串x自身的n次连接可以简记作xn。特殊地，我们规定x0=\\(\\epsilon\\) 语言操作 L和M的并：\\(L\\cup M=\\left\\{\\;s|s\\in L\\text{ or }s\\in M\\;\\right\\}\\) L和M的连接：\\(LM=\\left\\{\\;st|s\\in L\\text{ and }t\\in M\\;\\right\\}\\)，同样L自己的连接记作\\(L^k\\) L的Kleene闭包：\\(L^*=\\cup_{i=0}^{+\\infty}L^i\\)，即将L重复0次或多次​ L的正闭包：\\(L^+=\\cup_{i=1}^{+\\infty}L^i\\)，即将L重复至少一次 这样我们就有了构建复杂语言的工具了。 正则表达式 递归定义 \\(\\epsilon\\)是正则表达式，\\(L(\\epsilon)=\\varnothing\\) 若\\(a\\in S\\)，则\\(L(a)=\\left\\{\\;a\\;\\right\\}\\) 若\\(a,b\\)都是正则表达式，则\\((a)|(b)\\)也是正则表达式，且\\(L((a)|(b))=L(a)\\cup L(b)\\) 若\\(a,b\\)都是正则表达式，则\\((a)(b)\\)也是正则表达式，且\\(L((a)(b))=L(a)L(b)\\) 若\\(a\\)都是正则表达式，则\\((a)\\)也是正则表达式，\\(L((a))=L(a)\\) 可以用正则表达式定义的语言被称为正则集合 为了方便，可以给正则表达式命名。例如给表达式r命名为d，则写作d-&gt;r 还有一些其它的运算(语法糖) r+==rr* r?==e|r [a1a2a3]==a1|a2|a3 [a-z]==[abcdefg...wxyz] 词法单元的识别(状态转换图) 图的节点表示状态，边表示状态转换函数，边上的符号表示输入的符号与之相等时从这条边进入下一个状态。 必然有初始和接受状态 关键字的处理 关键字也会被识别成identifier，解决方案有两种 提前把关键字插入符号表 为关键字建立单独的状态转换图 词法分析器生成工具及设计 就讲了一波flex的用法....看文档！ 有限状态自动机 事实上有限状态自动机有两种，分别是确定状态自动机(Deterministic Finite Automata)和非确定状态自动机(Nondeterministic Finite Automata)，即DFA和NFA 自动机的形式化定义为一个五元组\\((\\Sigma,S,f,start,E)\\)​，分别表示字符集、状态集、转移函数、初始状态、接受状态 DFA 其中\\(f:S\\times\\Sigma\\mapsto S\\)​ 和 \\(E\\subseteq S\\)​​ 比较特别。 对于字符\\(c\\)​ 和一个状态 \\(s\\)​，我们称自动机 \\(D\\)​ 接受 \\(c\\)​ 后的状态为 \\(s&#39;\\)​ 当且仅当 \\(f((s,c))\\)​有定义，且\\(f((s,c))=s&#39;\\)​。对于DFA我们特殊规定 \\(f((s,\\epsilon))=s\\)，NFA后面再说 对于字符串\\(str=c_1c_2c_3\\ldots c_n\\) 和一个状态 \\(s\\)，我们称自动机\\(D\\) 接受\\(str\\) 后的状态为\\(s&#39;&#39;\\) 当且仅当 \\(f(f(\\ldots f(f(s,c_1),c_2)\\ldots,c_{n-1}),c_n)\\) 有定义且等于\\(s&#39;&#39;\\) 很显然，若字符串 \\(str=s_1s_2\\)，其中 \\(s_1s_2\\) 都是串，那么有 \\(f(f(state,s_1),s_2)=f(state,s_1s_2)=f(state,str)\\) 对于一个特定的DFA \\(D=(\\Sigma,S,f,start,E)\\)​，给定一个串 \\(str\\)​，我们称 \\(D\\)​ 接受串 \\(str\\)​ 当且仅当 \\(f^*(start,str)\\in E\\)​ 记 \\(L(D)=\\left\\{\\;str\\mid f^*(start,str)\\in E\\;\\right\\}\\)​，则\\(L(D)\\)​ 称为 \\(D\\)​ 的语言 NFA 与DFA的区别在于，\\(f:S\\times(\\Sigma\\cup\\left\\{\\;\\epsilon\\;\\right\\})\\mapsto 2^S\\) 转移函数的目标是若干状态组成的集合，表示这些状态都可以是当前状态加上某个字符后的后继状态 说人话就是造出来的图可以有\\(\\epsilon\\) 作为标号的边，并且一条标号的边可以连接多个后继状态。 有几个等价关系 自动机等价 直观地，对于自动机 \\(D_1,D_2\\)，我们称它们等价当且仅当 \\(L(D_1)=L(D_2)\\) 状态等价 对于状态 \\(s_1,s_2\\in S\\)，我们称它们等价当且仅当对任意的串 \\(str\\)，都有 \\(f^*(s_1,str)=f^*(s_2,str)\\) 于是自然要问：这两种不一样的自动机表达语言的能力相同吗？ 很显然DFA是一种特殊的NFA，因此NFA不会比DFA弱。 考虑用DFA对NFA进行模拟。 注意到不确定性的引入使得某一时刻NFA所处的状态可能有不止一个，因此可以用\\(2^n\\)​状压枚举所有可能的状态，并对字符集连边 构造如下： 给定NFA \\(N=(\\Sigma,S,f,start,E)\\) \\(\\epsilon\\text{-closure}\\) 首先规定一个函数 \\(c(S)=T\\)​​​​，满足 \\(S\\subseteq T\\)​​，且 \\(\\forall x\\in T\\)​​ 都 \\(\\exists y\\in T\\)​​ 使得 \\(f(y,\\epsilon)=x\\)​​。我们称 \\(c(S)\\)​​ 是 \\(S\\)​​ 的 \\(\\epsilon\\text{-closure}\\)​​。 函数\\(c\\)的存在性是可以构造证明的。首先令 \\(T_0=S\\)，对于自然数\\(k\\)，我们任意地选取 \\(T_k\\) 中的元素 \\(x\\)，若\\(f(x,\\epsilon) ot\\in T_k\\)，则令 \\(f(x,\\epsilon)\\in T_{k+1}\\) 再规定对任意的自然数\\(k\\)都有 \\(T_k\\subseteq T_{k+1}\\)，那么 \\(T_0\\subsetneq T_1\\subsetneq T_2\\ldots T_u\\)，由于总的状态集有限，且集合列的大小递增，故算法必然停止，这样就找到了\\(c(S)\\) 转移函数 对于任意状态\\(s\\in 2^S\\)​，我们规定\\(g(s,ch)=c\\left({\\cup_{x\\in s}{f\\left(x,ch\\right)} }\\right)\\)​，容易用反证法证明函数\\(g\\)​是well defined的 初始/接受状态 规定 \\(s\\in 2^S\\) 为接受状态当且仅当 \\(\\exists x\\in s\\) 使得 \\(x\\)是接受状态，初始状态则是\\(c(\\left\\{start\\right\\})\\) 下面证明这是一个DFA： 首先对于任意状态\\(s\\in 2^S\\)​，\\(\\epsilon\\text{-closure}\\)的过程确保了不存在\\(\\epsilon\\)的边，而 \\(g\\) 的良定义确保了不存在多条同标号的边连向不同的后继状态 然后证明其与\\(N\\)等价： 只需要证明二者的接受状态集等价，且两个状态等价当且仅当它们输入同一个字符后得到的两个后继状态仍然等价，然后归纳即可。 这样就证明了二者的表达能力是等价的....大概 容易发现，给出DFA则很容易判断某个串是否能被其接受，而NFA则不好直接模拟。因此根据上面的证明可以想到用一个DFA去模拟NFA 应用 说了这么多有啥用呢，就是我们可以通过将正则表达式转化为一个NFA，再将NFA转化为DFA的流程来实现用正则表达式识别字符流的目的，也就是写一个自己的lexer 注意到正则表达式实际上和一般的代数表达式没有本质区别，也是由一般字符(数字)和特殊字符(运算符)组成的，运算符又包括单目运算符(*号)、二元运算(|号，还有看不到的连接号) 因此可以用两个栈生成表达式的语法树，然后在语法树上根据递归构造的正则表达式规则来建立NFA，然后根据上面的构造把NFA变成DFA 我自己写了一下，写了大概一天一夜.....写得人都傻了但是停不下来 最开始打算用cpp写，后来发现OOP只会java的写法(什么居然没有instanceof要用)....更糟糕的是类之间的层次设计基本抓瞎，于是就转战c了。不过收货还是有的，不能说完全没学到东西。 尝试了一下程序的结构设计，分了几个struct和文件来保证这些部分可以单独改动，有些地方考虑得很粗暴，后面有时间再修修吧 生成NFA和DFA之后还会导出一个.gv文件用于graphviz绘图，这样就可以直观看自己的DFA和NFA长啥样啦！","tags":["Compiler"]},{"title":"DFA到等价正则表达式的转化","path":"/2021/08/20/DFA到等价正则表达式的转化/","content":"思考题的引入 首先看这样一道思考题： 如何用正则表达式识别所有是三的倍数的二进制串？ 考虑最暴力的做法。用一个变量rem表示一个串的前缀作为二进制对3的余数，对新进来的字符讨论： 进来一个0，则rem=(rem&lt;&lt;1)%3;，因为我们是从高位向低位读的 进来一个1，则rem=((rem&lt;&lt;1)+1)%3 那么只需要判断最终rem是否为0就好了 自动机的做法 在做这题之前，可以先想想这样的一个问题： 如何用自动机识别所有是三的倍数的二进制串？ 或者说 如何用自动机表示上述暴力做法？ 注意到rem的取值只能为0,1,2，因此可以建3个点，每个点两条出边表示对不同字符的处理转移，那么建出来的图如下 image 其中节点1,2,3分别表示rem对应为0,1,2的状态。 \"但是问题还没完啊，你不是要正则表达式吗\" 做到这一点需要一些前置姿势 正则表达式代数 没错！正则表达式也是有代数结构的！ 为了方便，我们规定连接运算(concatenation)用.符号表示 符号 | 性质 | ------------ | ------------ | | | 结合律，交换律，对.的分配率 | . | 结合律 | ^ | 幂等律 | 它们的优先级从上到下递增 那么自然应该想到，列出正则表达式的代数方程，也是可以解方程的 Arden's Theorem 定理的内容很简单，即对于形如 \\(x=A|xB\\) 的方程，\\(x\\) 的解都是 \\(AB^*\\) 的形式 对解的长度进行归纳。当 \\(n=1\\)， \\(x_1=A\\) 是原方程的一个解，满足 \\(x=AB^*\\) 的形式 设当 \\(n&lt;k\\) 时成立，则 \\(x_{n-1}=A\\overbrace{B\\ldots B}^{n-1\\text{个}B}\\)，带入方程右侧就有 \\(x_n=x_{n-1}B=A\\overbrace{B\\ldots B}^{n\\text{个}B}\\) 由数学归纳法可知原方程的解都是 \\(x=AB^*\\) 的形式，并且容易验证形如 \\(AB^*\\) 的串都是方程的解。 类似的也有对 \\(x=A|Bx\\) 的结论 自动机到正则表达式的转换 我们知道，自动机的每个状态都对应着一个接受串的集合(从初始状态到当前状态所有路径组成的串的并)，而不同状态之间存在转移关系 那么就可以设未知数列方程辣！ 对于最开始的那个DFA，我们可以设它的三个状态对应的接受串的正则表达式为\\(x_0,x_1,x_2\\)，那么有如下关系 \\[\\begin{aligned} \\begin{cases} x_0 &amp;=&amp; x_00|x_11 \\\\ x_1 &amp;=&amp; x_01|x_20 \\\\ x_2 &amp;=&amp; x_10|x_21 \\\\ \\end{cases} \\end{aligned}\\] 对式3用Arden's Theorem得到\\(x_2=x_101^*\\) 代入式2得到 \\(x_1=x_01|x_101^*0=x_01(01^*0)^*\\) 代入式1得到 \\(x_0=x_00|x_01(01^*0)^*=x_0(0|1(01^*0)^*)=(0|1(01^*0)^*)^*\\) 于是就得到了与该自动机等价的正则表达式 需要注意的是，在这个表达式中，我们认为可以有任意的前缀零，并且空串和任意长度的0串都是3的倍数 升华一下 如果你乐于思考，就会发现我们上述\"消元\"过程意味着什么——我们在化简自动机的状态！ 也就是说，假如我们要求得表示DFA从起点到终点e的串的集合的正则表达式，那么我们只需要合并掉除起点和e以外的所有状态即可。","tags":["Compiler"]},{"title":"软件分析10 Soundiness","path":"/2021/08/14/SPA10-Soundiness/","content":"Soundness Conservative approximation: captures all program behaviors, or the analysis result models all possible executions of the program 然鹅 Academia Virtually all published whole-program analyses are unsound when applied to real programming languages. Industries Virtually all realistic whole-program static analysis tools have to make unsound choices. Hard-to-analyze Language Features 不同的语言存在一些难以分析的特性，如果对这些特性采取过于保守的措施，那么我们将会得到正确但无用的分析 PPT给了几个例子，发现自己除了C的几个都不会....提上日程！ Java Reflection, native code, dynamic class loading JavaScript eval, document object model(DOM) C/C++ Pointer arithmetic, function pointers 这就导致了许多声称sound的做法只是保证了核心语法的sound，而对部分功能的sound有所取舍(说白了就是挑结果发论文，坠入生化环材的灌水之路)。 学术圈出现的乱象，需要正义的铁拳！(大雾，于是就有了一个Manifesto宣言，提出了Soundiness的概念 Soundiness 造词来自Truthiness Truthiness: a truthful or seemingly truthful quality that is claimed for something not because of supporting facts or evidence but because of a feeling that it is true or a desire for it to be true 也就是把原本的Soundness升华(?)了 A soundy analysis means that the analysis is mostly sound, with well-identified unsound treatments to hard/specific language features. 区分之后的三级分类 A sound analysis requires to capture all dynamic behaviors A soundy analysis aims to capture all dynamic behaviors with certain hard language features unsoundly handled within reason An unsound analysis deliberately ignores certain behaviors in its design for better efficiency, precision or accessibility. 也就是以后的论文必须要明确分析自己哪里准了，哪里还不准，以及原因。 然后讲了两个具体的例子来对reflection和native code分析 Reflection 难点在于class、method和field的具体对象是运行时确定的，并且是由一段字符串决定的，因此对于非静态决定的字符串难以求解上面的三个东西 动态的字符串可能来自 终端输入 配置文件 可能含有加密解密信息 网络 这些因素都使得反射机制难以静态分析 最早的做法是通过结合指针分析来对静态字符串进行分析，进而推断出一些目标方法。而另一个想法就是，我们不在求解定义时寻找对应的方法，而是在使用它的时候寻找。大意就是在调用方法的时候通过参数的类型和数量来推断可能的目标方法。 还有一些complete方法，例如说跑几个测试用例来找到一些必然真的目标方法。 Native Code Java在需要和OS交互的时候，需要调用一些C/C++代码，这样的代码称为Native Code Native Code难以分析比较好理解，因为语言不同，导致需要不同的分析策略。一种方法是根据函数的语义对常用的函数进行建模。PPT给的例子就是一个ArrCopy()可以建模成一整段的for+赋值","tags":["Static Analysis"]},{"title":"软件分析09 CFL-R&IFDS","path":"/2021/08/13/SPA09-CFL-R-IFDS/","content":"Feasible &amp; Realizable Paths Infeasible Paths: paths that do not correspond to actual executions Unrealizable Paths: paths whose \"returns\" are not matched with corresponding \"calls\" 定义的引入很直观，动态运行时并非所有路都会被执行，因此需要区分各类假边。IP是一类难以鉴别的路(why?)，而UP是我们定义出的一类较简单就能判定的路(用带标号的括号序即可)。很显然\\(UP\\subseteq IP\\)，即call和ret不匹配的路必然不会被动态执行。那么我们就可以针对UP进行优化来达到减少IP的目的。 于是我们就可以通过在边上加标号的方式来获得某path的标号序列，并通过这个序列来判断其Realizability 如果用上下文无关语言(CFL=Context Free Language)对括号序列(合法Call序列)进行识别，那么这样的可达性就叫做CFL-Reachability 注意到CFL可以用一个有限状态自动机(DFA)的接受集表示，因此只需要对CFL建出DFA，然后在传播状态的时候顺便传一下DFA的状态就可以无缝升级原来的做法了 IFDS IFDS = Interprocedural,Finite,Distributive,Subset Problem IFDS是一个分析框架，用于一类满足上述四个要求的分析问题 IFDS提供了MRP(Meet-over-all-Realizable-Paths)问题的解 作为对比，最早的朴素迭代算法提供了MOP(Meet-Over-all-Paths)问题的解 Overview 给定程序P，数据流分析问题Q，IFDS的流程如下 对P建supergraph \\(G^*\\)​ 并根据具体问题Q定义边上的flow function 对P建exploded supergraph(听起来就好中二) \\(G^{\\sharp}\\)​ 对Q的求解就转化为对\\(G^{\\sharp}\\)​应用tabulation algorithm求图可达性问题(MRP Solutions) 分配律的定义： \\(f(A\\sqcup B)=f(A)\\sqcup f(B)\\)​ 通常证明不符合只需要找范例就行了。尝试证明Constprop和Pointer Analysis的分配性质： 在Constprop中，令\\(A=\\left\\{\\;(x,1)\\;\\right\\}\\)，\\(B=\\left\\{\\;(y,2)\\;\\right\\}\\)，处理语句 z=x+y; 则\\(f(A)\\sqcup f(B)=\\left\\{\\;(x,1),(z,\\text{NAC})\\;\\right\\}\\sqcup\\left\\{\\;(y,2),(z,\\text{NAC})\\;\\right\\}=\\left\\{\\;(x,1),(y,2),(z,\\text{NAC})\\;\\right\\}\\)​​ 而\\(f(A\\sqcup B)=f(\\left\\{\\;(x,1),(y,2)\\;\\right\\})=\\left\\{\\;(x,1),(y,2),(z,3)\\;\\right\\}\\)​，不符合分配律 PPT给了一个简单的判断法则：若求一个结果需要考虑多个(大于一个)元素的值，则不是可分配的。 在Pointer Analysis中，对变量的转移函数需要实现对象的别名分析(即语句x=y;需要考虑变量y指向对象的所有别名)，这样就涉及到了多个变量，根据简单的判断法则是不可分配的。 看完Overview有一个猜测，大概的想法如下： 既然解决的是集合问题，那么就可以对每个点拆点。 supergraph的定义使得一条边能连接多个点，这样就可以表述集合之间的transfer关系 根据分配律可以把集合的映射变为单独元素的映射构成的集合，这样就可以对点连边了 这样CFG中一个点n是否包含元素x就变成了\\(G^{\\sharp}\\)中&lt;n,x&gt;点是否从起点可达 猜完继续看视频 Supergraph 首先定义一个单独过程p的flow graph \\(G_p\\)： \\(G_p\\) 是一个有向图，除basic blocks外，还包含一对唯一的起点 \\(start_p\\) 和终点 \\(end_p\\)，函数/方法调用以一对相邻节点 \\(call_m\\) 和 \\(ret_m\\)​​​ 表示 \\(G_p\\) 中的边除正常函数内的控制流连边外，还包含三类跨函数的边(\\(call_m\\rightarrow start_m\\)，\\(end_m\\rightarrow ret_m\\)，\\(call_m\\rightarrow ret_m\\)) \\(G^*\\) 由一系列图 \\(G_1,G_2\\ldots\\) 和跨函数间的边组成，即每个函数/过程都有自己的图 发现确实猜对了.....感觉就索然无味 需要注意的一个小细节就是拆点的时候空集需要特殊处理。为了方便可以加入特殊点\\(\\varnothing\\)表示空集 注意到如下事实： \\(A\\sqcup\\varnothing=A\\) \\(f(A\\sqcup\\varnothing)=f(A)\\sqcup f(\\varnothing)\\) \\(f(A)\\sqcup\\varnothing=f(A)\\) 因此可以认为任意作用在非空集合\\(A\\)​​上的函数\\(f(A)\\)​​实际上为\\(f(A\\sqcup\\varnothing)=f(A)\\sqcup f(\\varnothing)\\)​​，而\\(A\\)​中不可再分离出\\(\\varnothing\\)​ 这就表示我们需要连\\(\\varnothing\\rightarrow\\varnothing\\)的边，同时在其余的\\(f(A)\\)中去掉\\(f(\\varnothing)\\)​中的元素 有一些常数的小优化。比如说假设做完了方法p的方法内分析，那么对于所有调用了方法p的连通性其实都已经确定了","tags":["Static Analysis"]},{"title":"软件分析08 Datalog","path":"/2021/08/13/SPA08-Datalog/","content":"Datalog最早作为数据库的查询语言出现，是非图灵完备的编程语言 语法 和数理逻辑的命题逻辑、谓词逻辑基本是一致的，如果看的教材和我一样是Mathematical Logic in Computer Science的话甚至会发现很多描述的语句都是一样的 ,表示and ;表示or !表示not，这些运算可以用括号改变优先级 每条语句的最后要加.表示结束，这个老是忘.... Predicate 即谓词。谓词可以看成是一个函数，n元谓词就是一个\\(D^n\\mapsto \\left\\{\\;0,1\\;\\right\\}\\)的函数，其中\\(D\\)是命题变元(变量)与命题常元(常量)构成的集合 在Datalog中，我们讨论的都是有限论域，因此使谓词P为真的命题有限。记\\(F(P)\\)为所有在谓词P下为真的n元组，则我们称这些元素为Facts。这样就把一个函数变成判断元素是否属于某集合的问题了。 谓词分为两类，算术谓词x &gt;= y和关系谓词Friends(x, y)，算术谓词是Unbounded的 Inference Rules 即推理规则。用PPT的例子就是Datalog的推理规则形如 Adult(x) &lt;- Person(x, age), age &gt;= 18. &lt;-可以看成是向左的箭头(好形象！)，箭头左侧的称为Head，右侧是Body。语句的意思是\"若Body为真，则Head为真\" 这句话的意思是\"若x是age岁的人 且 age&gt;=18 那么x是成年人\" 考虑形式化命题会怎么说 $((x)(Person(x)Age(x)&gt;=18))Adult(x) $ 观察区别即可发现，如果不使用函数的概念(形式化命题里的\\(Age()\\))，我们就需要维护某个元素所有相关的信息，即用元组 求解 就是列真值表，\\(\\prod\\limits_{B\\in Body} |B|\\)枚举所有可能的值求解\\(F(P)\\) Safe Rules 既然涉及到枚举，就要看看是不是可穷尽的 考虑如下两组Rules A(x) &lt;- B(y), x &gt;= y. A(x) &lt;- B(y), !C(x, y). 对于1. 如果我们讨论的是正整数域，且\\(F(B)\\)非空，则必然存在无穷多的x满足要求... 对于2. 由于\\(F(C)\\)有限，因此\\(F(!C)\\)有无穷多元素，因此也存在无穷多的x满足要求... 基于上述情况，Datalog对Rules作出了规定： Head中的变元必须在Body中的某个非否定谓词中出现过(即必须是Bounded Variable) 同样，再考虑如下Rule A(x) &lt;- !A(x). 很显然就不是合理的Rule，毕竟都不是重言式...因此设定Rules的时候也要考虑是否永真的问题(不能有矛盾) IDB &amp; EDB IDB = Intensional DataBase 内涵谓词 EDB = Extensional DataBase 外延谓词 大概理解就是在初始阶段会规定一些谓词，这些就是EDB 而在推理过程中出现的谓词则是IDB。Head一定是IDB，而Body可以IDB、EDB 通常EDB是不可变的(常元)，而IDB可以随着程序的进行而修改，例如如果我想表示\"喜欢猫 或 喜欢狗 的人是好人\"，那么我可以写 Good(x) &lt;- Likes(x, &quot;Dog&quot;). Good(x) &lt;- Likes(x, &quot;Cat&quot;). 当然也可以写 Good(x) &lt;- Likes(x, &quot;Dog&quot;); Likes(x, &quot;Cat&quot;). 递归 Datalog的强大之处就在于递归 考虑给定等价关系集合的传递闭包，就可以这么写 Eq(x, y) &lt;- Rel(x, y) Eq(x, y) &lt;- Eq(x, z), Rel(z, y) 其中Rel(x, y)是EDB中提前定义好的常元 在指针分析中的应用 应用Datalog就能很容易翻译和实现前面讲到的Flow Insensitive &amp; Context Insensitive分析 首先规定下面几个谓词 New(x, o) Assign(x, y) Store(x, f, y) Load(x, y, f) VCall(l, x, k) #l: x.k(a1,a2...an) Dispatch(o, k, m) ThisVar(m, this) Argument(l, i, ai) #ith argument at l: is ai Parameter(m, i, pi) #ith parameter at method m is pi MethodReturn(m, ret) #return var at method m is ret CallReturn(l, r) #r receives return value at l: 推导规则就是翻译PPT VarPointsTo(x, o) &lt;- New(x, o). VarPointsTo(x, o) &lt;- VarPointsTo(y, o), Assign(x, y). FieldPointsTo(o1, f, o2) &lt;- VarPointsTo(x, o1), VarPointsTo(y, o2), Store(x, f, y). VarPointsTo(x, o2) &lt;- VarPointsTo(y, o1), FieldPointsTo(o1, f, o2), Load(x, y, f). CallGraph(l, m), Reachable(m), VarPointsTo(this, o) &lt;- VCall(l, x, k), VarPointsTo(x, o), Dispatch(o, k, m), ThisVar(m, this). VarPointsTo(pi, o) &lt;- CallGraph(l, m), Argument(l, i, ai), Parameter(m, i, pi), VarPointsTo(ai, o). VarPointsTo(r, o) &lt;- MethodReturn(m, ret), CallGraph(l, m), CallReturn(l, r), VarPointsTo(ret, o). 全程序分析还要加上一个默认的入口方法，没了","tags":["Static Analysis"]},{"title":"软件分析07 Security","path":"/2021/08/08/SPA07-Security/","content":"","tags":["Static Analysis"]},{"title":"软件分析06 CSA","path":"/2021/08/08/SPA06-CSA/","content":"不敏感的分析认为所有语句的执行顺序无法区分，也就是没有上下文的概念 考虑如下代码片段 class A; A a = new A(); // o1 A b = new A(); // o2 A pa = func(a); A pb = func(b); A func(A x) &#123; return x; &#125; CI静态分析pa,pb的指针集，容易发现pts(pa)=pts(pb)={o1,o2}，这与实际情况不符 出现这种情况的原因在于 动态执行的程序的函数每次执行都有自己的栈空间(上下文)，即同一方法的不同调用产生的数据流不同 CI中的建模认为同一函数的所有实体共享信息(指针集) 因此一个直观的想法就是区分出不同的函数实体，一个实现类似allocation site abstraction 即定义函数func的上下文为caller site的上下文+func的caller site，这样也是对调用栈的直观模拟 此外，Java是OO语言，需要频繁对堆进行操作，这样的语言也叫Heap intensive语言。所以在进行load和store的处理时，不同函数实体中产生的对象也应该加以区分，这就是所谓的CS Heap 处理的规则大同小异，区别在于处理方法调用的时候要多一步产生方法内的上下文的步骤，同时PFG点集变成\\((C\\times V)\\cup(C\\times V\\times F)\\)，别的就没了 提问：引入上下文的本质是区分了运行时的函数实体，那么对于递归/循环调用的情况如何处理？答案就是k-CFA的k的含义 几类CS变种 call-site sensitive，也叫call-string或k-CFA(k-Control Flow Analysis)，k的意思是取最后k位 object sensitive type sensitive 获得方法调用的方法上下文的选择函数select(c,l,c':o,m)= c+l c'+o c'+type(o)，这里的type指的是包含o的allocation site的类， 其中1是在对call stack建模，2是在对对象交互建模，3可以看成是1和2的结合和简化 这里一定要注意2,3的select函数，因为我们关注的是Object调用链，而每个方法的receiver object的context恰好就包含了前面所有的调用链信息 两位老师的论文对三种方法进行了比较(还没看过，惭愧....)，在k比较小且同一个类内的方法调用较多的时候，2更有优势(即存在调用路径的后缀完全相同且调用链较长，但调用者为两个不同的对象的情况) 但是面对同一个对象在多处调用同一方法的情况，1就要优于2(这个的原因比较显然....) 在实际情况中精度2&gt;3&gt;1，效率3&gt;2&gt;1","tags":["Static Analysis"]},{"title":"软件分析05 PA","path":"/2021/08/04/SPA05-PA/","content":"","tags":["Static Analysis"]},{"title":"软件分析04 CGC","path":"/2021/08/04/SPA04-CGC/","content":"Motivation 如果只做method内的分析，则任何包含function call的语句都需要保守分析(例如说默认不是常数) 这样是不利于进一步做优化的,因此引入Call Graph图对CFG作拓展 java call invokestatic: call static methods, 对应的方法唯一且在编译时确定 invokespecial: call constructor, superclass methods, private methods, 对应的方法唯一且在编译时确定 invokevirtual: instance methods(virtual dispatch)，后面这俩是运行时决定的,对应的方法可以不止一个(polymorphism) invokeinterface: same as virtual but cannot optimize and checks interface implementation() invokedynamic: JVM在若干版本之后支持各种语言,这东西让dynamic language方便地在JVM上跑起来 一对尖括号&lt;&gt;内的内容叫做method的signature(签名), 包含 class name, ret type, 方法名字(可选), param type 格式形如 InstanceName.&lt;ClassName: ReturnType MethodName(Param1Type, ...)&gt;(params, ...) 可以发现前两种call都是唯一确定的,因此问题的难点在于如何对virtual call建图 在运行时,virtual(的目标方法)由两点决定: receiver object的类型 method signature 考虑到Java的继承/方法覆写特性,我们可以用一个递归函数来求某个类中的方法的主体究竟是什么 dispatch(c,m)表示求c类中的方法m的主体,那么分别讨论m是否在c中,然后看情况往父类爬就好了 Class Hierarchy Algorithm 我们需要解决的问题就是求解dispatch,那么做这个的经典方法是CHA 做CHA有一个保守的假设,即任意指针指向的对象可以是他所有子类的对象(即子树中的一个点) 这时需要计算一个函数resolve(a.method).这里a是class A的引用变量,那么根据java的多态a可以指向所有A的子类的对象. 根据上面的保守假设,我们需要找到a所有可能指向的对象的method调用的dispatch值.这就是resolve的定义 Interprocedural Control Flow Graph 有了上面的resolve,我们就可以很方便地通过一个method call找到其所有可能调用的方法(的位置),然后连方法之间的边 方法间的边的transfer function实际上就是处理传参的过程. 需要注意的是,我们并不需要干掉caller位置的边,而是利用这条边来引导本地变量(数据流),没必要流进整个callee method 在caller位置的边的transfer function要干掉LHS变量,这是为了防止method call前后LHS变量的值发生变化使得分析精度下降 于是乎这样建出来的图就包含过程间的信息了,直接用和之前一样的方法做就好了 Reachability 方法间的调用形成了以main方法为起点的有向图，因此我们只需要分析可达的方法。在CGC的过程中可达性逐渐明确，因此可达范围也是不断增加的。","tags":["Static Analysis"]},{"title":"大一下存活纪实","path":"/2021/08/02/大一下存活纪实/","content":"大一下享受生活&amp;课程存活经验 下一届的直系小朋友们终于来了，希望自己能够真正帮到他们，而不是向去年的我一样自己摸索（虽然也没有摸索地太痛苦，但是从反响来看大部分的同学的求索之路还是很痛苦的.... 混了一年也学会了各种划水摸鱼，算是成了老油条了。不过享受生活和学习不冲突，因此这次不完全是存活经验了（毕竟难度太大，俺也不知道怎么存活） 最近混了几天的新生群，感觉问题还是在于自己讲得太多，希望自己可以沉得住气，多多倾听，不要说起话来得意忘形了 废话不多说，开始废话环节吧！ 三月 这部分的记忆已经模糊了，姑且看着照片说一说。 二月底到的学校，实际上开学的第一周仍然是网课。因为上课的地点就在床下因此十分舒适，最后两天甚至直接在床上用手机听。 因为一月的时候收了显示器，因此宿舍的学习/看剧体验有极大的提升，也不那么愿意出门了。 这学期的课程非常多，抛开体育就已经塞满了五天的早八，一学期学分30+，十分的难顶。 第一周还要抢体育课，于是就抱着选认识的老师的想法选了一个气排球（事实证明不要幻想在不会的球类运动中获得高分和良好体验） 三月份的南京仍然很冷，下雨的时候也有刺骨的寒意（其实是看到了自己穿着羽绒服的自拍才想起来的）。不过三月中旬的时候就会有花开，系楼前的景色非常美妙。 10号开始可以去鸡鸣寺樱花大道（没错就叫这个名字）看樱花，玄武湖也是不错的选择。有条件最好选择工作日放晴的下午，这样人少也舒服。 第一次形策就放出了上学期各课程的均分和成绩分布，看了一波发现大家分数差的都不多，俺好像也没有特别大的优势 月底的时候听了SJTU的OS叫兽的报告，觉得自己可能对这类能看到成果的问题比较感兴趣，不过还是不知道自己究竟想干啥...于是就激情下单了SJTU的OS教材！ 四月 清明的时候jjk从常州来了南京，算上达哥，三个东莞人在南京相聚也是独特了 jjk说是在追小姐姐，于是陪他去了鸡鸣寺求姻缘（后续是达哥脱单了....没啥好说的，这波是准歪了）因为我的原因没有约到南博和总统府，不过临时去的六朝博物馆还不错，学生证免费，有三层可以逛，布置很有意思，暖气也很足，好评 和同学聊了两天，发现也没什么事情可以说的了，不过默契还在，也就谈谈学了什么，吐槽一下老师课程和学校政务 四月份的抽代学得痛不欲生，四处找书都没有很好啃下来，题目也离谱，要么不会要么一眼秒.....图论就还好。实际上很多题目都在考基础数论知识和套路，可惜我完全不懂这方面。想补也有点晚，刷题也莫得什么用处，于是四月份算是过得比较痛苦的一个月了.... 中途和同学去雨花台，也算比较融洽，只要不谈学习都可以做好朋友（ 五月 五月初入手了通宝的十孔布鲁斯，非常好吹！低音6容易压，气息也很灵敏，算是入门的好选择了，于是就开始了双修之旅（并不） 五月中考完大雾就和队友去了水站银川，中间的过程可以参考银川的游记。回来才发现这个银川站是各种坑，泪累 顺便软院的EL也打完了，虽然最终排名没有预想的那么精准，不过也算是拿到了一小笔钱（怎么还没到账啊喂），顺便吃了一顿 也是在五月听了第一场歌剧，不得不说舞台非常棒，演员的水平很高！因为懒得排队所以花了50r买的纪念品票，还有个帆布袋，也算是不错了 五月底的时候尝试了人生第一次（半）女装，之所以说是半女装是因为只有上半身（脸部）。真厉害啊，美颜相机 感觉还是不擅长处理和异性的事情....先不管了 六月&amp;七月 这俩月都没啥可讲的，就硬考试，顺便继续思考如何与异性相处的问题....结果还是没有得到答案 得知要参加科研实践，于是开始四处翻主页当海王，不过最后还是拉上奶人上船了。发现自己的java水平根本不够看，于是开始疯狂看hfj+问舍友，赶了差不多一周终于可以看了。宣讲的时候感觉大部分都在炼丹，就没什么意思。看了一波网课发现所谓的worklist算法就是一个spfa....感觉难点主要在实现吧，那就把自己的动手能力搞起来！ PS:作业的部分到现在还鸽着...我尽量暑假前搞定！ 给分 有的科还没出，再等等看吧，或许会陆陆续续更新（真的会有人看吗喂 数分 5学分 4.25 淑芬B就是计算，除了一开始的分析后面都是算....级数也是硬算，反正展开就都能做....微分方程的部分也没有讲到，考核就水水过去了 感觉不是很难，主要是练得少，而且和物理高度相关（毕竟都是场论）。不过积分这种东西，练多了意义何在啊，反正算不过wolfram alpha（ 给分感觉还行，似乎是完全没有调分。这个分数也是自己烤出来的，没啥好说的.... 高代 4学分 4.7 期中挂了差不多1/3的人，于是期末就各种放水。上课内容也很水，和线性空间结构有关的内容基本都快速过了，直接退化成矩阵应用和计算课程.... 专门的矩阵论内容还得自己看，指望这门课显然是不靠谱的。酉空间的内容也没怎么讲，对这门课彻底无语了。 习题我觉得都没什么做的必要，反正都是计算（摔。要不是不点名课都没必要去了（暴论） 离散 5学分 4.8 这门课很难，但是考试远没有你想象中那么难（迫真 仲老师真的很喜欢数论，算是看出来了（习题也太离谱了 图论就很舒服，基本上学会几个套路就比较容易。当然也没有讲太现代的东西，毕竟啥基础没有也不能大跃进 最后考试是大概70pts的送分选择和经典四选三，不过这次的四道题大概有三道可以做，如果准备充足（指习题课全程做笔记）也许可以满分 信科实践 3学分 4.8 这门课就是各种坑.... 所谓的实践就是程设基础+算法+数据结构，这个就不是那么有手就行了。 最后是一场考试、三次小作业和期末的两个代码作业，基本就是大模拟/数据结构实现，随便卷卷就好了。 期末大概就是几个BST和RBT的小题，几个手写链表快排DP平衡树旋转的大题，这不就考背诵吗（摔 这门课的PPT都是祖传的，讲得也一般，笑话和梗都很老了，建议自学数据结构/算法看B站网课(我现在发现BYR和TJUPT也有MIT和CMU的DS课程，好评)，就酱 大物 4学分 3.6 喜提人生最低分(这之前是气排球) 没啥好说的，大物这学期基本没怎么看，考前对着程守洙的普物速成，最后确实靠着考前的自学写出了两道大题(还算是可喜可贺？) 四舍五入就是花费GPA-0.1的代价体验了一波矿院水准卢老爷子的教材和矿院水准王老师的课程，不亏了。 据上一届AI壬说王老师是捞人高手，现在想想大概是网课+王老师第一届的双重功劳吧，不过说真的，有这分我已经戴恩戴德了.... 话说学这有啥用啊，有空学物理换成ICS不行吗 听说读写 2学分+2学分 4.6 听说老师很好人！上课气氛很棒，大家互动很多。而且这个班很多人文的小姐姐，上课简直是天堂~平时就是听听力看视频，有一次quiz、一个小配音和一个剪视频分组大作业 读写就非常坑，每周起码要一个晚上做作业，每节课quiz，每三周轮到你一次pre，最后还有三次大作业（问卷调查和分析/录视频/essay），还有期末的一次exam....大家都躺平，最后甚至老师都佛了也无所谓了....这课设计就迷.不过中途有三次网课(也就是在宿舍躺着听)，还算比较新奇 期末就是四级难度，听力和四级持平，阅读和写作要比四级简单一些，剩下的就没啥好说的了。 马原 3学分 4.55 曹老师的课很有意思，每次都要装模作样打开PPT然后自由发挥，讲的明明是马克思主义原理结果半学期过去了才讲完诞生背景，然后话锋一转开始聊怎么进行资本的原始积累（发财致富之路）。算是这学期很有意思的课，值得不翘 这课只有一次点名，不过来听课很好玩也不亏，最后两周会集中画一下重点，也可以看他第一周就发出来的重点集萃（那个要比考试范围大一些） 考前四处参考了一下大家的笔记综合了一份，背了大概三天，最后分数感觉算是不错 军理 2学分 这课就大离谱 杨博士喜欢点名，更喜欢点点过的名。一学期下来大概有了五六次，更是有三次缺席重修的豪言壮语，不得不说是老顽固了 除了点名，这门课还是早八，因此每周都在挣扎着起床与不起床之间摇摆，就算决定了要去也是啃着面包灰溜溜坐第一排——后面的位置是留给早起人抢的。 由于大家都醒不来，且杨博士喜欢下课前点名，因此就有了人越来越多以至于下课前要站着听课喊到的场景，实在是大开眼界 期末更是离谱，所谓的考察细心就是答题卡的选项数量挖坑，开卷考何必呢... 气排球 1学分 3.7 2k4没有退步，不过也没有进步就是了 气排球的考核要考发球和对传，也就是很考验队友的水平以及配合能力....队友比较佛，我也就佛了，最后80都没有 不过体育嘛....才一学分 一些通识课 这个学期痛定思痛，下决心选了几门网课。仅有的一门线下也是不点名的那种，于是后期就逐渐摸了 网课好在1. 没有固定时间 2. 考核简单，考试约等于开卷 3. 给分好，基本上就是95+，找一找可以刷上99..... 本来还带着点愧疚感认真看网课的，不过看着看着发现还挺有意思，于是就认真看完了，体验可以说是很好了。 这里仅供参考，网课就不点评了（ 科学之光的走进天文学好评，平时不点名，期末交论文。即使是听故事也很有趣，教室是装修过的标杆多媒体教室（每个位置带插座！）前几节课的老师比较有意思，后面就很催眠了，目前还没出分","tags":["Eureka Moments"]},{"title":"集训补题合集","path":"/2021/07/17/集训补题合集/","content":"好多题都不会啊，这可咋整 先写着吧....大概会割掉.... cf490 求带点权树上的简单路径构成的点权序列的最长上升子序列的长度的最大值 一个比较容易想到的做法就是dp，设f[x,i]表示以x为根的子树中，以i为结尾的最长上升子序列的长度，g[x,i]就是下降。这里我们规定只能选取深度递降的顺序 那么每次合并子树就好了，这样直接做是n^2logn的。还可以用线段树记录这个结尾的状态，那么两个dp状态合并就是线段树合并了，这样就少一个n 牛客多校1C 给一棵树要求删掉一个节点，使得剩余每棵树中的cf490的答案最大值最小 这个做法就很强。首先可以找原树的最长路径所在的链c，那么要删的点一定在这条链上（否则最大值不变） 不妨假设删掉了一个点x，那么这条最长链被分成两部分c1和c2，此时再按照cf490的方法求一次最长链得到c'，则分类讨论 c和c'相交，这个时候删掉它们的交点会更优秀（不仅让原本的最长变短了，还让变短后的最长也变短了） c和c'相离，这个时候删掉c上的任意一点都一样 于是就可以发现如果我们不断这么做下去，最终将得到若干条链的交，且这个交在不同层次的最长链上，删掉交里的点一定是最优秀的 每次对当前的交二分（取中点），判断新的交在哪一侧，这样就只是再多了一个log 牛客多校1H 定义函数\\(f_h(x)=x\\text{ mod } h\\)，求最小的\\(h\\)使得\\(f_h\\)是给定有限正整数集的perfect hash 不是perfect当且仅当存在\\(a eq b\\)，却\\(f(a)=f(b)\\)，即 \\(a_i-a_j\\equiv 0\\pmod h\\)，其中\\(i eq j\\) 也就是说如果我们能算出任意两对数的差值，就可以方便地枚举答案了 直接开两个桶bct[i]和bct[INF-i]，做卷积就好了.... 牛客多校2I 没啥好说的，纯粹是菜 一开始竟然整了一个迭代加深的dfs，事实上直接用四维bfs就好了.... 牛客多校2L 欧老师：一般无向图没啥好的性质，考虑均摊或者度数分块就行了 考虑暴力怎么做。每次更新一个点之后暴力枚举它的邻居更新邻居的状态（是否为冠军） 为了不那么暴力对度数分块，记度数&gt;sqrt(n)的点为大点，否则为小点，那么大点的数量是有一个上界A的，而小点的度数有一个上界B。 若修改点是小点，那么就直接暴力，这部分是O(nB)的 若修改点是大点，那么就分邻居的情况讨论： 邻居是大点，那么这样的 大-大 点对不会超过A个 邻居是小点，那么我们对每个大点x维护一个集合S，按照点权顺序存放x的既是小点又是冠军的邻居。由于当前点的权值不减，因此当前大点的冠军小点邻居不增 hdu01G n个人传1个球，初始球在1号人手上，每次拿球的人随机传给别人。现在已知球回到1号人的方案数为x，求这是传了几轮得到的方案数 设f[i]表示i轮球回到1的方案数，则考虑1所在圈的大小： 恰好为2，就是f[i-2] * (n-1)，即1和另一个人配对组成长度为2的圈 至少为3，就是f[i-1] * (n-2)，即在前一个圈中的最后一轮传球中插入一个除1和x外的第三人。 于是就可以用特征根求通项，解一个BSGS就好了 hdu01J 给定序列\\(\\left\\{a_n\\right\\}\\) Q次询问求下标l到r，值域a到b之间出现了多少不同的数字 已经不会套路了.... 记last[i]为第i个位置的数上一次出现的位置，那么统计区间内不同的数字，就是统计区间内last[x]&lt; l的x的数量 于是就变成了一个三维偏序问题。经典的离线莫队分块平衡复杂度的操作可以看HH的项链(实在愧对队友，这题俺还写过...) hdu02J 给定\\(x,p\\)，\\(p\\)为质数。规定\\(a_n=nx\\text{ mod } p\\)，求数列\\(\\left\\{a_1,a_2,\\ldots,a_{p-1}\\right\\}\\)逆序对的奇偶性 这题就是Zolotarev定理...\\({\\mathbb {Z}_p}^*\\)中的元素\\(a\\)的勒让德符号\\(\\left(\\frac{a}{p}\\right)\\)和排列\\(p_i=ai\\text{ mod } p\\)的奇偶性(逆序对的奇偶性)是相关的。 上课讲的方法比较神奇，这里其实可以直接算 排列\\(\\left\\{p_i\\right\\}\\)的逆序对可以这么算\\({\\dfrac{\\prod\\limits_{i&lt;j}{\\left(p_i-p_j\\right)} }{\\prod\\limits_{i&lt;j}{\\left(i-j\\right)} } }=\\prod\\limits_{i&lt;j}{\\dfrac{p_i-p_j}{i-j} }=\\prod\\limits_{i&lt;j}{\\dfrac{xi-xj}{i-j} }=x^{\\frac{p-1}{2} }\\) 牛客多校3B 这场好难，不过做的都1A了...算是可喜可贺吧 给n*m的棋盘要求染色。每次染色代价为对因位置的权值，并且规定若某矩形的三个顶点染色了，则第四个点可以免费染。求染黑整个棋盘的最小代价。 容易发现答案至多染n+m-1个点，否则必然会出现某个四角都染色的矩形。 把行和列看成n+m个点，染色则相当于连一条i-j带权边，那么免费染色的含义就是把长度为3的边连成一个4元环。由于免费操作不改变图的连通性，因此最终全部染黑就要求初始的选择联通了所有n+m个点，这就是求一个最小生成树。","tags":["XCPC"]},{"title":"图论04 平面图与可平面图","path":"/2021/07/03/Graph04-平面图与可平面图/","content":"本来应该(被)科普一些拓扑的姿势的，但是目前好像也不太用得上，就先咕了吧。 本文假设读者有一定的图结构知识，比较新的概念俺会努力解释的 这里的内容都比较入门，大佬轻喷( 平面图(Plane Graph) 我们称具有如下性质的图 \\(G\\) 为平面图： \\(V(G)\\subseteq \\mathbb R^2\\)，\\(E(G)\\subseteq \\mathbb R^2\\) 毕竟是\"平面\"图 \\(\\forall e_1,e_2\\in E(G)\\), \\(st(e_1) eq st(e_2)\\text{ and } ed(e_1) eq ed(e_2)\\) \\(\\forall e_1,e_2\\in E(G)\\), \\(e_1\\cap e_2\\in \\left(V(G)\\cup\\varnothing\\right)\\) \\(\\forall e\\in E(G)\\), \\(e=(x,y)\\) 是连通 \\(x,\\;y\\) 的一段弧(arc)。 在一般图中，我们认为 \\(E(G)\\) 中的元素是若干二元组。在平面图中，我们认为图的边 \\(e=(x,y)\\) 是连通 \\(x\\) 和 \\(y\\) 的一段弧(arc)，即一个点集。此时 \\(G\\) 就可以代表由所有顶点以及边上的点构成的点集。 考虑点集 \\(S=\\mathbb R^2\\backslash G\\)，\\(S\\) 中存在着若干不相交的区域(region)。我们记 \\(F(G)\\) 为 \\(S\\) 中的所有区域，将这些区域称为图 \\(G\\) 的面(face)。若 \\(f\\in F(G)\\) 是无界的(unbounded)，则记为外面，否则记为内面。 平面图的欧拉定理 对于平面图 \\(G=(V,E)\\)，\\(F=F(G)\\)，则有 \\(|V|-|E|+|F|=2\\) 考虑对 \\(|E|\\) 归纳。 当 \\(|E|=|V|-1\\) 且 \\(G\\) 连通时，\\(G\\) 是树，此时 \\(|V|=|E|+1\\)，\\(|F|=1\\)，带入成立。 设当 \\(|E|&lt;k\\) 时成立，则对于 \\(||G||=k\\)，必然存在一个圈 \\(C\\)。取 \\(e\\in E(C)\\)，考虑 \\(G&#39;=G-e\\)。则必然存在两个面 \\(f_1,f_2\\)，满足 \\(f_1 eq f_2\\) 且 \\(e\\subseteq(\\partial f_1\\cap\\partial f_2)\\)。记 \\(f=f_1\\cup f_2\\cup (\\partial f_1\\cap\\partial f_2)\\)，则 \\(F(G&#39;)=F(G)-f_1-f_2+f\\)。于是可以由 \\(|V(G&#39;)|-|E(G&#39;)|+|F(G&#39;)|=2\\) 得到 \\(|V(G)|-|E(G)|+|F(G)|=|V(G&#39;)|-|E(G&#39;)|-1+|F(G&#39;)|+1=2\\) 于是对于任意有限的平面图，平面图欧拉定理成立。 三角剖分定理 若对于平面图 \\(G\\) 中的每个面 \\(f\\)，\\(\\partial f\\) 上都只有三个点，则 \\(G\\) 是一个三角剖分图(triangulation graph) 极大平面图(maximal plane graph)定义为 \\(\\forall x,y\\in V(G)\\)，\\(G+(x,y)\\) 都不是平面图。 我们有：平面图 \\(G\\) 是极大平面的当且仅当它是一个三角剖分 \\(\\Rightarrow\\)： 取 \\(f\\in F(G)\\)，则 \\(\\partial f\\) 是一个圈 \\(C\\)。观察到必然有 \\(|C|\\leqslant 3\\) (否则可以选取 \\(C\\) 上两个不相邻的顶点连边使得仍然是平面图，这与极大平面矛盾) 且由平面图的定义可知 \\(|C|&gt;2\\) (否则存在重边)，因此 \\(|C|=3\\)。由 \\(f\\) 的任意性可知 \\(G\\) 是一个三角剖分。 \\(\\Leftarrow\\)： 由反证法，假设存在 \\(x,y\\in V(G)\\) 使得 \\(xy\\) 的内部在某一区域 \\(f\\) 内，那么必然有 \\(x,y\\in\\partial f\\)。而由 \\(G\\) 是三角剖分可知 \\(|V(\\partial f)|=3\\)，故 \\(x,y\\) 必相邻，这与 \\(G\\) 不含重边矛盾。故命题成立。 平面图的必要条件 若图 \\(G=(V,E)\\) 是平面图，则 \\(|E|\\leqslant 3|V|-6\\) 对三角剖分的边和面计数，则有 \\(\\frac{3|F|}{2}=|E|\\) (每个面的边界上有三条边，每条边的两侧恰好为两个面)，带入平面图欧拉公式就有 \\(|E|=3|V|-6\\)。 若不含三角形的(triangle-free graph)的图 \\(G=(V,E)\\) 为平面图，则 \\(|E|\\leqslant2|V|-4\\) 证明和上面类似，每个面的边界有四条边 子式和拓扑子式 拓扑子式 考虑一个固定的图 \\(X\\)，我们用若干不相交的路替换掉 \\(X\\) 的边得到新的图 \\(X&#39;\\)，则我们称 \\(X&#39;\\) 是 \\(X\\) 的一个细分，也记作 \\(X&#39;=TX\\) 我们把 \\(V(X)\\cap V(TX)\\) 称作 \\(X\\) 的分支顶点，把 \\(V(TX)\\backslash V(X)\\) 称作 \\(X\\) 的细分顶点。很显然细分顶点度数都是 \\(2\\) 若 \\(TX\\subseteq G\\)，则我们称 \\(X\\) 是 \\(G\\) 的拓扑子式 子式 考虑一个固定的图 \\(X\\)，我们用若干不相交的连通图 \\(G_x\\) 替换掉 \\(X\\) 中的顶点，对于 \\(xy\\in E(X)\\) 则用 \\(G_x-G_y\\) 路替换掉，这样得到的图记作 \\(X&#39;\\)，那么我们记作 \\(X&#39;=IX\\) 若 \\(IX\\subseteq G\\)，则我们称 \\(X\\) 是 \\(G\\) 的收缩子式，记作 \\(X\\preceq G\\) Kuratowski定理 这个定理很强，但是证明非常麻烦....这里打算摸了只给出结论，具体证明可以参考任意一本找得到这个定理的图论教材~ 对于图 \\(G\\)，下列叙述等价： \\(G\\) 可平面 \\(G\\) 不包含 \\(K_5\\) 或 \\(K_{3,3}\\) 作为子式 \\(G\\) 不包含 \\(K_5\\) 或 \\(K_{3,3}\\) 作为拓扑子式 极大可平面图是三连通图 这是作业里需要证明的一个小引理，不过很有用，也放上来吧 要用到三连通图的收缩列 引理1：\\(\\exists uv\\in E(T_n)\\) 使得 \\(\\left|{N(u)\\cap N(v)}\\right|=2\\) 首先 \\(\\forall x\\in V(T_n)\\)，都有 \\(deg(x)\\geqslant 3\\)。任取边 \\(uv\\in E(T_n)\\)，\\(uv\\) 恰好在两个面 \\(f_1,f_2\\) 的边界上。而由三角剖分的定义可知 \\(f_1,f_2\\) 的边界是两个三角形。因此 \\(\\forall uv\\in E(T_n)\\) 都有 \\(|N(u)\\cap N(V)|\\geqslant 2\\)。 由反证法，不妨假设 \\(\\forall uv\\in E(T_n)\\) 都有 \\(|N(u)\\cap N(v)|\\geqslant 3\\)，则易知 \\(|N(u)|\\geqslant 4\\) 且 \\(|N(v)|\\geqslant 4\\) 不妨记 \\(N(u)=\\left\\{\\;x_1,x_2,x_3,x_4\\;\\right\\}\\)，则由 \\(|N(u)\\cap N(x_1)|\\geqslant 3\\) 可得 \\(x_1x_2,x_1x_3,x_1x_4\\in E(T_n)\\) 同理有 \\(x_2x_3,x_2x_4,x_3x_4\\in E(T_n)\\)，故 \\(u\\cup N(u)\\) 的导出子图是 \\(K_5\\)，这与 \\(T_n\\) 可平面矛盾。 因此 \\(\\exists uv\\in E(T_n)\\) 使得 \\(|N(u)\\cap N(v)|=2\\)。证明对图的阶没有要求，因此引理对 \\(T_n\\)，\\(n\\geqslant 4\\) 成立。 极大可平面图是3-连通图 对极大可平面图 \\(T_n\\) 的阶作数学归纳法。 当 \\(n=4\\) 时，\\(T_4=K_4\\) 是三连通图； 设当 \\(n=k\\) 时 \\(T_k\\) 是三连通图，则取 \\(k+1\\) 阶极大可平面图 \\(T_{k+1}\\)，由引理1可知 \\(\\exists uv\\in E(T_{k+1})\\) 使得 \\(|N(u)\\cap N(v)|=2\\)。 作图的收缩，记 \\(G=T_{k+1}\\circ uv\\)，容易发现 \\(||G||=||T_{k+1}||-|\\left\\{\\;uv\\;\\right\\}|-|N(u)\\cap N(v)|\\) 又因为 \\(T_{k+1}\\) 是三角剖分，所以 \\(||G||=3(k+1)-6-1-2=3k-6=3|G|-6\\) 又因为 \\(G\\) 仍然是平面图，所以 \\(G\\) 也是极大可平面图。由归纳假设，\\(G\\) 是三连通图，存在一个保持三连通性的收缩。因此 \\(T_{k+1}\\) 也是三连通图。","tags":["Graph Theory"]},{"title":"软件分析03 DFA","path":"/2021/06/11/SPA03-DFA/","content":"Data Flow Analysis 这一部分是比较重的重点...要好好看 或许需要补一补lattice(事实证明这一部分非常简单...) Data Flow Analysis 实际是要通过 How Application-Specific Data Flows through the Nodes &amp; Edges on CFG 这句话中三个加粗部分理解 ASD 这里的ASD可以看作是根据研究性质抽象(Abstraction)出来的变量值域 Flows Approximation 建CFG的时候实际上是考虑了所有可能的的控制流,这是一种over-approximation,即考虑尽可能全面 这种以over-approximation为手段的分析也叫may analysis(finds out what may be true)事实上大部分的静态分析都属于这一类 同理就会有must analysis(finds out what must be true),这时候的手段就叫under-approximation 这两者分别对应lec1的soundness和completeness,在实际应用中都有必要 在不引入歧义的情况下，两者可统称为safe-approximation,即safe的定义对于特定问题来讲是不同的 Nodes &amp; Edges 注意到3AC的性质，对Node的分析就是定义一套在ASD上的运算法则 对Edge的分析实际上就是定义一套分支合并的法则 通常来说，不同的静态分析算法，各自上述三个部分有不同的设计 In &amp; Out States 实际上就是一条IR的执行会导致程序状态的变化,我们把语句前后的状态分别称为这个节点的In&amp;Out States 在分支和并处需要合并不同分支的Out状态来生成当前位置的In状态,这里引入了\\(\\wedge\\)符号(读作meet)来表示将两个状态合并(可以是并交差补...) 为了研究的方便，我们在不同语句之间插入特殊节点(称为program states),这样In/Out States就都有了载体.具体的理解可以想想GDB的用法和操作步骤,这个是一样的 在每一个DFA应用中，我们要给每个program states赋一个抽象的状态表示(例如:用5进制vector来表示+-0bt) 此时再引入domain(定义域)的概念，即我们针对感兴趣的问题抽象出的数据范围D,那么总共含有n个变量的抽象的状态表示就可以用一个D^n向量表示 也就是说DFA的另一精确定义为:在若干经过safe-approximation后的限制条件下找到每个program state的解.这里的限制包括: 定义操作(transfer functions) 控制流(control flow merging) transfer function 的一些概念 forward analysis,就是符合直觉的顺序分析,我们将指令s看成函数fs，并且立即得到fs(in[s])=out[s] 结合block的特点和性质,fb就是block b中所有f的复合(注意顺序),in[b]就是b中第一条的in,out[b]就是最后一条的out,对于分支处理和上面单条语句的处理是一致的 类似的还有backward analysis,就是反向分析,容易发现这里取fs'=fs^{-1}就好了.或者反向建图做forward也是一样的 Reaching Definition 这是一个分析的例子，即把definition作为ASD来分析 a definition of v: a statement that assigns a value to v,通常用statement的位置表示(这是我的理解) 我们说一个位于p的变量v的definition d reaches q 当且仅当存在一条p到q的path且path上不存在别的v的definition.实际上有点像SSA的变量作用域 一个简单的应用就是在st节点给每个变量来个definition,然后所有变量第一次reach到的点如果出现了对变量的使用说明可能出现了\"used before definition\"错 基于以上事实，很自然就需要求出各个点能被哪些definition给reach.课程给出的寻找算法非常平凡,直接状压就好了... 课程中间给了一个例子理解: D: v = x op y 这个语句kill掉了前面关于v的definition,同时reach到所有的后续状态,也就是多了一个(当前的变量的reach)同时少了一个(kill掉前面的definition) ppt的例子有点怪，因为正式的定义是block B需要kill掉所有其中变量在所有其余地方的definition,即使这些definition没有在当前的状态集中 看到后面就可以敏锐地发现所谓迭代法就是在跑一个bellman-ford,我的第一反应其实是用队列更新,那么这个就是在跑SPFA了...这也太憨了 算法步骤的有限性可以通过观察集合size的单调性(单调不减)轻松得出 Live Variables Analysis 其实就是变量v是否能在某个节点p的后继(不一定相邻)被使用 一个应用场景就是在生成汇编的时候需要做寄存器分配,这种时候就需要保留live的变量而尽量使用dead变量的寄存器 此处的ASD就是用状压表示live的变量,并且容易想到反向建图维护的操作 感觉没啥好说的 Available Expressions Analysis 一个表达式x op y在点p被称为是available的当且仅当: 所有起点到p的路径都经过这个表达式(支配点) 在表达式到p之间不存在redefinition idea是非常直观的，即我们可以提前计算好这个值来优化 ASD同样是状压,区别在于merge的时候要取&amp;操作(考虑available的条件2) 感觉这一部分也没啥好说的 Lattice &amp; Partial Order 偏序的部分上学期已经讲得很多了...这里主要普及一下(较为简单的)Lattice.之所以这么说是因为离散课程中提到的Lattice是Minkowski Lattice,和这个不太一样 有了这些就可以形式化地验证所谓迭代算法的正确性了 对于有n个点的CFG,每个点都有一个ASD的元素作为函数值,那么这个图的状态就可以写成ASD^n中的一个元素 那么每次迭代可以看成是从ASDn到ASDn的函数 事实上只需要定义ASD^n上的包含关系为每个元素对应包含即可得到一个lattice (我在写上面这句话的时候还没有看到后面的product lattice,事实上这个东西也很直观...没啥好说的) 也就是说，严格的DFA算法可以用三元组(D,L,F)描述，即分别为direction, lattice, function 分别表示分析方向,值域(通常是格),格上的状态转换函数.而通过证明这个转换函数是单调的即可说明我们的迭代算法必然存在且能找到解 Precision Analysis Meet All Paths Solution(MOP) 一条path的transfer function就是路径上function的复合 MOP实际上就是枚举到达当前点的所有Path,然后合并这些path的函数值 MOP存在几个问题(事实上也是图论中的经典问题) 有些path实际上不会被执行,即我们的分析并不精确.这是从保守分析的出发点得到的.例如condition和path互斥的collision path 路径长度不确定(存在圈),路的数量不可枚举 但是经过比较迭代法和MOP,我们可以发现MOP要更精确.实际上就是F(xy)&lt;=F(x)F(y)的关系 当transfer function存在分配律时,等号成立.容易验证前面提到的三个分析都是可分配的. Constant Propagation 所谓常数替换优化，这个是第一次的作业 我们需要判断某个变量v在某一处p是否值一定为常量 类似的仍然采用状压，即用变量集合bitvector \\(V\\) 和值域集合bitvector \\(D\\) 得到笛卡尔积 \\(V\\times D\\)，那么这个就可以作为单个点的ASD了。考虑到实现的问题，作业中给出的框架代码用的是map 做起来也不是很难，对于合并变量的情况只需要分类讨论，合并flow的时候遍历Keys，求值的时候需要分类讨论递归求某个表达式的值就好了","tags":["Static Analysis"]},{"title":"软件分析02 IR","path":"/2021/06/11/SPA02-IR/","content":"","tags":["Static Analysis"]},{"title":"软件分析01 Intro","path":"/2021/06/11/SPA01-Intro/","content":"真是绝了，我连淑芬都没有专门的笔记 回想上学期的考试周开了CSAPP的大锅，这次考试周开了软件分析的大锅，考试周真是除了考试都好玩 术语 PLT = Programming language theory 编程语言理论 Static Analysis 静态分析 computable function 可计算函数 property \\(\\text{P is trivial}\\iff (\\forall xP(x))\\text{ or } (\\forall x eg P(x))\\) r.e. = recurrence enumerate 递归可枚举的 什么是PLT Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science. It has become a well-recognized branch of computer science, and an active research area, with results published in numerous journals dedicated to PLT, as well as in general computer science and engineering publications. 对语言及其特性的设计、实现、分析、描述和分类的科学 课程给出了如下分类： 理论：设计语言 design, type system, semantics and logics... 环境：让语言跑起来 compilers, runtime system... 应用：验证语言 analysis, verification, synthesis(综合)... 看了一波感觉理论和验证比较好玩 什么是静态分析(Static Analysis) 在程序运行之前判断其行为和是否满足某些性质 也就是用程序分析程序(老爹语气) 静态分析的背景 语言的三大分类： 命令式(imperative) 函数式(functional) 逻辑式(logical)/声明式(declarative) 静态分析的背景: 新的语言很多, 但它们的内核差异很小 软件变得复杂, 需要有效分析 静态分析的应用场景 reliability: 检测空指针解引用... security: 预防注入攻击... optimization: 编译后端的优化... development: IDE的补全, 提示, 类型提醒... Soundness &amp; Completeness, 取 &amp; 舍 There is no such approach to determine whther P satisfies such non-trivial properties, i.e., giving exact answer: Yes or No. Any non-trivial property of the behavior of programs in a r.e. language is undecidable. 也即: PERFECT static analysis 不存在 由此自然引出sound和complete的概念, 其实就是充分性和必要性的讨论... 三者的关系是 \\(\\text{complete}\\subsetneq \\text{truth}\\subsetneq\\text{sound}\\) 于是又自然引出了sound和complete的取舍/妥协问题 PPT的红绿颜色是反的...看了两回才意识到 根据定义有点怪，这里不讲妥协换一种追求的说法 追求soundness会引入truth之外的报错，这一部分称为false positive(假阳性) 追求completeness会遗漏truth内的报错，这一部分称为false negative(假阴性) (考虑比较\"宁可错杀一千也不放过一个\"和\"宁可放过一千也不错杀一个\") 事实上绝大部分的分析器追求 sound but not fully-precise anlysis,即要保证任意情况下程序的正确性 类似的还有precision和efficiency的取舍问题,于是给出static anlysis的进一步定义:ensure soundness while making trade-offs between precision and speed. PPT在这里给了一个栗子，实际上就是在做霍尔逻辑(\\(P{S}Q\\))的判别 Abstraction &amp; Over-approximation 与运行时分析不同，静态分析通常将数据作同态映射到一个小集合(例:PPT里的{+-0U}),集合的构造与我们在乎的性质相关 好像这句话就足够把abstract的部分讲完了 over-approximation依赖于control flows的分析，在分支合并的地方需要合并数据映射 结合两种方法，我们实际上构造了程序P到一个近似程序P'的同态映射，因此只需要研究P'的性质就可以得到P的一些性质了.针对不同的感兴趣的性质，则可以构造不同的映射","tags":["Static Analysis"]},{"title":"图论03 染色","path":"/2021/06/04/Graph03-染色/","content":"图染色(Coloring) 染色数(Coloring Index) 图的染色分为点染色(vertex coloring)和边染色(edge coloring) 点染色指的是构造映射 \\(f_k\\colon V(G)\\mapsto \\left\\{\\;1,2,\\ldots k\\;\\right\\}\\)，一个合法的染色(proper coloring) 则要求映射满足 \\(\\forall xy\\in E(G)\\Rightarrow f_k(x) eq f_k(y)\\)，记 \\(k\\) 为这个染色方案的染色数(chromatic number)。通常我们只关心最小的染色数，记作 \\(\\chi(G)\\) 下面讨论的染色都指的是proper coloring 边染色则是类似地构造 \\(f_k\\colon E(G)\\mapsto \\left\\{\\;1,2,\\ldots k\\;\\right\\}\\)，proper的则要求满足 \\(\\forall e_1,e_2\\in E(G),\\; e_1\\cap e_2 eq\\varnothing\\Rightarrow f_k(e_1) eq f_k(e_2)\\) 类似地定义 \\(\\chi\\prime (G)\\) 为最小的边染色数 关于染色最经典的问题就是著名的“四色定理”的证明。定理的证明非常长.....有兴趣也不会去看的 团(clique)和独立集(independent set) 团的定义为 \\(G\\) 的完全子图，即 \\(H\\subseteq G\\) 且 \\(2||H||=|H|(|H|-1)\\) 定义clique number为最大团的大小，记为 \\(\\omega(G)=\\max \\left\\{\\;H \\text{ is a clique}\\mid H\\subseteq G\\;\\right\\}\\) 定义反团(co-clique)为 \\(\\overline G\\) 的团，即 \\(G\\) 的一个独立集。同样定义 co-clique number 为最大独立集的大小，记为 \\(\\alpha(G)\\) 容易有如下数量关系： \\(\\alpha(G)\\cdot \\chi(G)\\geqslant |G|\\) \\(n-\\alpha(G)\\geqslant \\chi(G)-1\\) \\(\\chi(G)\\geqslant \\omega(G)\\) \\(\\binom{\\chi(G)}{2}\\geqslant ||G||\\) 1直接由每个独立集染上同一种颜色得到 2的意思是给最大的独立集染上一种颜色，剩下的点用 \\(\\chi(G)-1\\) 种颜色染 3的意思是团内部的颜色互不相同 4的意思是同色点之间不能连边，因此至多连 \\(\\binom{\\chi(G)}{2}\\) 条边 图的色多项式 对图 \\(G\\) 用 \\(k\\) 种颜色染色(我们认为每个节点都不一样)的方案数是关于 \\(G\\) 和 \\(k\\) 的函数，我们不妨记作 \\(F(G,k)\\) 对 \\(|G|\\) 进行归纳。当 \\(|G|=1\\) 的时候 \\(F(G,k)=k\\) 设当 \\(|G|&lt;n\\) 时成立，则 \\(|G|=n\\) 的时候，考虑 \\(xy\\in E(G)\\) 只有两种情况： \\(x,y\\) 同色，此时可以把 \\(x,y\\) 收缩成一个点而不改变方案数 \\(x,y\\) 异色，此时就是 \\(F(G,k)\\) 于是 \\(F(G,k)+F(G\\circ xy,k)=F(G-xy,k)\\) 很容易看出这是一个多项式，并且首项系数和次项系数与 \\(|G|\\) 和 \\(||G||\\)有关 染色的贪心算法 关于染色的证明通常通过构造给出一个最小染色数的上界。构造出染色方案的一种常见算法是所谓“贪心算法”，用如下步骤描述： 维护已经染色的点集 \\(V&#39;\\) 和未染色的点集 \\({V}&#39;&#39;\\) 任取 \\(x\\in V&#39;&#39;\\)，给 \\(x\\) 染上 \\(N(x)\\cap V&#39;\\) 中未出现、且最小的颜色 把 \\(x\\) 从 \\(V&#39;&#39;\\) 中删去，再加入 \\(V&#39;\\) 中 若 \\(V&#39;&#39;=\\varnothing\\) 则结束算法，否则回到步骤2 容易发现这个算法并不总能给出较好的染色数的界，但仍然给出了一个结果，并且算法的结果与给节点染色的顺序十分相关，因此我们要结合以下引理来改进以下这个算法的效果。 引理一 取图 \\(G\\) 中的任意点 \\(s\\)，都存在 \\(V(G)\\) 的一个排列 \\(v_1,v_2,\\ldots v_{n-1}, s\\) 使得 \\(\\forall 1\\leqslant i&lt; n\\) 都有 \\(\\exists j&gt;i,\\;v_j\\in N(v_i)\\)。 为了好记我把这个叫排列引理 只需要构造出这样的序列即可。我们取 \\(G\\) 中以 \\(s\\) 为根的生成树 \\(T\\)，每次从 \\(T\\) 中取出叶子，这样就能保证每个点在被放进序列时都有至少一个邻居在它的后面，且根是一定放在最后的。 这样结合贪心染色可以得到 \\(\\chi(G)\\leqslant \\max\\left\\{\\;\\text{deg}(s)+1,\\Delta(G-s)\\;\\right\\}\\) 引理二 若非完全图 \\(G\\) 满足 \\(\\delta(G)\\geqslant 3\\) 且 \\(\\kappa(G)\\geqslant 2\\)，那么 \\(\\exists x,y\\in V(G)\\) 使得 \\(\\exists v\\in V(G),\\; xv,yv\\in E(G)\\text{ and } xy ot\\in E(G)\\) 并且有 \\(G-\\left\\{\\;x,y\\;\\right\\}\\) 仍然连通。 取 \\(G\\) 的一个极小分隔集 \\(T\\)，则 \\(|T|=\\kappa(G)\\geqslant 2\\)。记 \\(C\\) 为 \\(G-T\\) 形成的连通分支的集合，那么有 \\(\\forall x\\in T\\)，\\(\\forall c_i\\in C,\\; N_G(x)\\cap c_i eq\\varnothing\\) 于是取 \\(v\\in T\\)，令 \\(x,y\\) 分别取自不同的分支，那么必然有 \\(xy ot\\in E(G)\\text{ and } xv,yv\\in E(G)\\)。 并且删去 \\(x,y\\) 后它们所属的分支仍然连通(\\(\\kappa(c_x)\\geqslant 2\\text { and }\\kappa(c_y)\\geqslant 2\\))，\\(\\left\\{\\;c_x,c_y,x\\;\\right\\}\\) 仍然连通(\\(\\text{deg}(x)\\geqslant 3\\))，得到一个矛盾 染色数的平凡上界 任意图 \\(G\\) 都有 \\(\\chi(G)\\leqslant \\Delta (G)+1\\)。 这个上界直接由贪心算法得到。 Brooks Theorem \\(G\\) 是连通图，那么 \\(\\chi(G)=\\Delta(G) + [G \\text{ is complete or an odd cycle}]\\)，其中 \\([x]=1\\) 当且仅当表达式 \\(x\\) 为真。 首先 \\(G\\) 是完全图的情况很显然，奇数圈的情况也很简单，反证法就可以说明不存在方案了。 然后 \\(\\Delta(G)\\leqslant 2\\) 的情况也很好讨论，就是一个圈，因此下面讨论的都是 \\(\\Delta(G)\\geqslant3\\) 的图。 对于 \\(G\\) 不是完全图也不是奇数圈的情况我们对 \\(|G|\\) 归纳证明： 当 \\(|G|=3\\) 时，\\(G\\) 不是完全图也不是圈，因此 \\(G\\) 只能是链，染色就很显然了； 设当 \\(|G|&lt; k\\) 时命题成立，则取 \\(|G|=k\\) 的图，分如下几种情况讨论： \\(\\kappa(G)=1\\)，即 \\(G\\) 存在一个割点 \\(x\\)，则 \\(G=G_1\\cup G_2\\)，其中 \\(G_1\\cap G_2=\\left\\{\\;x\\;\\right\\}\\)。那么我们对 \\(G_1\\)、\\(G_2\\) 分别染色，由归纳假设得到他们方案的并就是 \\(G\\) 的一个合法染色方案，因此 \\(\\chi(G)=\\max\\left\\{\\;\\chi(G_1),\\chi(G_2)\\;\\right\\}\\leqslant\\max\\left\\{\\;\\Delta(G_1),\\Delta(G_2)\\;\\right\\}\\leqslant \\Delta(G)\\) 得证。 \\(\\kappa(G) \\geqslant 2\\)，且存在 \\(x\\in V(G)\\) 使得 \\(\\text{deg}(x)&lt;\\Delta(G)\\)，则根据引理一构造 \\(x\\) 在最后的序列并贪心染色，这样就有 \\(\\chi(G)\\leqslant\\Delta(G)\\) \\(\\kappa(G)\\geqslant 2\\)，且 \\(\\forall x\\in V(G)\\) 都有 \\(\\text{deg}(x)=\\Delta(G)=\\delta(G)\\)，则根据引理二存在 \\(x,y,v\\in V(G)\\) 使得 \\(xv,yv\\in E(G)\\) 且 \\(xy ot\\in E(G)\\)。我们把 \\(x,y\\) 染上同种颜色，根据引理一把 \\(v\\) 放在序列末尾，这样就可以贪心地染出 \\(\\chi(G)\\leqslant \\Delta(G)\\) 了。 图的定向(orientation) 图定向的严格定义是构造映射 \\(f\\colon E(G)\\mapsto V(G)\\times V(G)\\)，简单地说就是给无向边定方向 我们称有向无环图(Directed Acyclic Graph) 为DAG，DAG有许多优秀的性质 DAG的染色算法 对于给定的有向无环图 \\(G\\)，我们给出如下染色算法步骤： 我们需要维护一个映射 \\(g\\colon V(G)\\mapsto \\mathbb N^+\\)，\\(g(x)\\) 表示以节点 \\(x\\) 为终点的最长路长度。 找到 \\(x\\in V(G)\\) 使得 \\(x\\) 的入度为0，在 \\(N_G(x)\\) 中找到已经走过的点中 \\(g(v)\\) 的最大值，令 \\(g(x)=g(v)+1\\) 删去 \\(x\\)，标记 \\(x\\) 已经走过了。若还有未经过的点则返回步骤2 细心的朋友们很快就可以发现这是一个拓扑排序上的计数。很显然 \\(g\\) 下任意相邻节点的函数值都不相等。于是我们caim：找到的映射 \\(g\\) 就是一个染色方案。 不妨记 \\(p(G)\\) 表示DAG图 \\(G\\) 中最长路的长度，那么有 \\(\\chi(G)\\leqslant p(G)\\) 复杂度是可以做到 \\(\\Theta(n+m)\\) 的 利用图定向给出染色数的紧界 不妨设 \\(H\\) 是图 \\(G\\) 的任意一个定向，\\(K\\) 是 \\(H\\) 极大的不含有向圈的子图，那么有： \\(\\chi(G)\\leqslant p(K)\\)，并且存在一个定向使得它们恰好相等 这个结论是很强的。不等号的部分在DAG的染色中已经给出，我们只需要考虑 \\(E(G)\\backslash E(K)\\) 中的边加入后会不会产生相同颜色的相邻节点就好了。由 \\(K\\) 的定义可知 \\(\\forall uv\\in (E(G)\\backslash E(K))\\)，有 \\(K+uv\\) 会产生一个有向圈，即 \\(K\\) 中存在一条 \\(v-u\\) 有向路，这保证了 \\(g(v) eq g(u)\\) 下面证明存在一个定向的极大无圈子图 \\(K\\) 使得 \\(p(K)=\\chi(G)\\)。我们只需要证明 \\(p(K)\\leqslant\\chi(G)\\) 首先用 \\(\\chi(G)\\) 给 \\(G\\) 染色，然后对 \\(G\\) 定向：若 \\(uv\\in E(G)\\) 有 \\(c(u)&lt;c(v)\\)，则构造定向 \\(f(uv)=(u,v)\\)，否则 \\(f(uv)=(v,u)\\) 即我们规定边只能从小颜色连向大颜色。这样 \\(K\\) 中路的长度至多为 \\(\\chi(G)\\)，也就是 \\(p(K)\\leqslant \\chi(G)\\) 这个定向实际上是在枚举所有贪心算法的操作序列，也就是说存在至少一种操作的顺序使得我们trivial的贪心算法能够摸到 \\(\\chi(G)\\) 的门槛。 平面图的五色定理 四色定理太难勒，这里有一个比较好玩的弱化版本——任意平面图(plane graph)/可平面图(planar graph)是5-可着色(colorable)的 引理一 极大可平面图有等式 \\(||G||=3|G|-6\\) 成立 推论1：平均度为 \\(\\frac{2||G||}{|G|}=\\frac{6|G|-12}{|G|}&lt;6\\)，因此 \\(\\exists x\\in V(G)\\) 使得 \\(\\text{deg}(x)\\leqslant 5\\) 推论2：极大可平面图删去任意点仍然是可平面图，因此 \\(\\forall x\\in V(G)\\) 都有 \\(\\text{deg}(x)\\geqslant 3\\) 引理二 极大可平面图中任意点的邻居导出一个圈 只需要找到一个平面画法，删去这个点，观察这个点所在的区域的边界即可。 可平面图不好直接做，因此我们向 \\(G\\) 加边直至 \\(G\\) 成为极大可平面。只需证明新图 \\(G&#39;\\) 仍然是 5-可染色即可。 我们对极大可平面图的大小归纳。当 \\(|G|=1\\) 是显然的。 设当 \\(|G|&lt;n\\) 时成立，则 \\(|G|=n\\) 时取 \\(x\\in V(G)\\) 讨论： \\(\\text{deg}(x)&lt;5\\)，则由贪心算法可知加上 \\(x\\) 也不需要超过5种颜色。 \\(\\text{deg}(x)=5\\)，则 \\(x\\) 有恰好 \\(5\\) 个邻居 若5个邻居中存在两个颜色相同，则染上 \\(x\\) 也只需要至多5种颜色 若5个邻居互不相同，则需要特殊讨论。 现在来看2.2的情况。根据引理二我们得到5个点共圈，不妨按顺序记为 \\(v_1, v_2,\\ldots v_5\\)，其颜色分别为 \\(c_1,c_2\\ldots c_5\\) 那么我们做如下操作： 把 \\(v_1\\) 的颜色换成 \\(c_3\\) 把与 \\(v_1\\) 距离为1的点中，颜色为 \\(c_3\\) 的点的颜色换成 \\(c_1\\) 把距离为2的点做同样操作.... 直到不存在可以更改颜色的点剩下。 若流程终止了，则我们通过switch得到了一个染色方案，而 \\(c(v_1) eq c(v_3)\\)，即5个邻居只用了4种颜色，那么 \\(G\\) 就是5-可染色的了。 若最后一直换到了 \\(v_3\\)，即存在一条 \\(v_1-v_3\\) 路，使得路上的节点颜色交替为 \\(c_1,c_3,c_1,c_3\\ldots c_3\\)，那么此次交换无效(没有达到预期的目的) 再类似地考虑 \\(v_2,v_4\\)，若成功则证明完毕，否则存在一条 \\(v_2-v_4\\) 颜色交替路 注意到 \\(G\\) 是平面图，因此不存在这样两次都失败的情况(why？)，即 \\(v_2-v_4\\) 和 \\(v_1-v_3\\) 必然相交。因此证毕。 二分图的染色 这个非常sb，二分图嘛，黑白染色黑白染色，\\(\\chi(G)=2\\) 还有边染色的部分，先去吃饭... 回来填坑了 图的边染色 具体定义见上面 首先给出一个简单的关于边染色的界的结论： \\(\\forall G\\), \\(\\chi&#39;(G)\\geqslant \\Delta(G)\\) 这个界的证明非常简单，只需要找到度数最大的节点，给它的边染上颜色就好了 二分图的边染色 若 \\(G\\) 是二分图，则 \\(\\chi&#39;(G)=\\Delta(G)\\) 首先有 \\(\\chi&#39;(G)\\geqslant\\Delta(G)\\)，因此只需要证明 \\(\\chi&#39;(G)\\leqslant\\Delta(G)\\) 即可 我们对 \\(||G||\\) 归纳，设当 \\(||G||&lt;n\\) 时命题成立，则取 \\(xy\\in E(G)\\)，\\(\\chi&#39;(G-xy)\\leqslant\\Delta(G-xy)\\leqslant\\Delta(G)\\) 考虑加入 \\(xy\\) 这条边，那么\\(\\text{deg}_{G-xy}(x)\\leqslant\\Delta(G),\\text{deg}_{G-xy}(y)\\leqslant\\Delta(G)\\)。不妨记 \\(M(x)\\) 为 \\(x\\) 相邻的边中没被用过的颜色，则分两种情况讨论： \\(M(x)\\cap M(y) eq\\varnothing\\)，则给 \\(xy\\) 染上 \\(M(x)\\cap M(y)\\) 中的任意一种颜色，\\(\\chi&#39;(G)\\leqslant\\Delta(G)\\) \\(M(x)\\cap M(y)=\\varnothing\\)，则 \\(\\exists a\\in M(x), b\\in M(y)\\) 使得 \\(a ot\\in M(y),b ot\\in M(x)\\)。类比点的染色，我们尝试通过交换来让出一种颜色。即令 \\(x\\) 邻边中染上 \\(b\\) 颜色的边换成 \\(a\\) 颜色，并沿着这条边走向一个邻居；再令这个邻居染上 \\(a\\) 颜色的邻边换成 \\(b\\) 颜色..... 直至走到一个不用换颜色的节点，则停止 我们claim这样的走法一定能换成功，即使走回了起点。原因在于这是一个二分图，所有的圈都是偶圈，而我们交替地走着 \\(a,b,a,b,\\ldots\\) 的边，因此最后必然可以走出一条路(这样就直接交换颜色)或一个圈(这样就相当于轮换了一圈的颜色) 于是就证完了 一个任意图的更强的界 事实上 \\(\\forall G\\) 都有 \\(\\Delta(G)\\leqslant\\chi&#39;(G)\\leqslant\\Delta(G)+1\\) 这个证明有点麻烦，咕了吧（","tags":["Graph Theory"]},{"title":"图论01 基本概念&定义","path":"/2021/06/01/Graph01-基本概念-定义/","content":"还是记一下吧，方便看博客的人(真的会有人看吗喂！) 图论基本概念 好多啊,还是英文,记不住..... 这里的图默认是有限图 点(vertex/vertices),边(edge) 边\\(\\left\\{x,y\\right\\}\\)可简记为\\(xy\\) 基本符号 \\(\\left[n\\right]=\\left\\{1,\\dots,n\\right\\},n\\in\\mathbb N\\) \\(\\binom{n}{k}=\\left\\{ {x\\subseteq \\left[n\\right]|\\left|x\\right|=k}\\right\\}\\) \\(\\binom{V}{k}=\\left\\{x\\subseteq V|\\left|x\\right|=k\\right\\}\\) 图(graphs) 图的严格定义由有序二元组表示,\\(G=(V,E)\\)表示以\\(V\\)为点集,\\(E\\)为边集的图,这里\\(E\\subseteq \\binom{V}{2}\\) 这里的图又叫无向简单图,不含重边(multi edge)和自环(loops) 无向图中的边用集合定义，连接 \\(x,y\\) 的边实际上是 \\(\\left\\{\\;x,\\;y\\;\\right\\}\\)，简记为 \\(xy\\) 其中用\\(V(G)\\)和\\(E(G)\\)分别表示图\\(G\\)的点集，边集 有向图(directed graphs) \\(G=(V,E)\\) 其中 \\(E\\subseteq V^2\\) 有向图中的边用有序二元组定义 多重图(multi graph) \\(G=(V,E)\\),其中\\(E\\)是一个多重集且\\(\\forall x\\in E\\)都有\\(x\\in \\binom{V}{2}\\cup\\binom{V}{1}\\) 也就是可以有重边和自环 超图(hypergraph) \\(G=(V,E)\\),其中\\(E\\subseteq 2^V-\\varnothing\\) 也就是一条边可以连接任意多个点,这个可以用建立辅助点来理解 下面默认讨论的是简单图 相邻(adjacent/neighbors) 两个点\\(x,y\\)相邻定义为\\(\\left\\{x,y\\right\\}\\in E\\) 两条边\\(e_1,e_2\\)相邻定义为\\(e_1\\cap e_2 eq\\varnothing\\) 完全图/团(complete graphs/cliques) 对于\\(G=(V,E)\\)若\\(\\forall x,y\\in V\\)都有\\(\\left\\{x,y\\right\\}\\in E\\)那么称\\(G\\)是一个完全图/团 我们记含有\\(n\\)个点的完全图为\\(K_n\\)或\\(K^n\\).特别地,\\(K_3\\)叫做三角形(triangle) 独立(independent) 不相邻的点对被称为是独立(independent)的 对于图\\(G=(V,E)\\)若\\(\\exists V&#39;\\subseteq V\\)使得\\(\\forall x,y\\in V&#39;\\)都有\\(\\left\\{x,y\\right\\} otin E\\),那么我们称点集\\(V&#39;\\)是独立集(independent set) 独立集的性质也被称为stable(不会翻,也可能是读错了) 同态(homomorphism)和同构(isomorphism) 考虑两个图\\(G=(V,E)\\)和\\(G&#39;=(V&#39;,E&#39;)\\),若存在映射\\(f:V\\mapsto V&#39;\\)使得\\(\\forall x,y\\in V\\)都有\\(\\left\\{x,y\\right\\}\\in E\\Rightarrow \\left\\{f(x),f(y)\\right\\}\\in E&#39;\\),那么我们称\\(G\\)和\\(G&#39;\\)同态 若同态映射同时是一个双射(bijection),那么就得到了一个\\(G\\)和\\(G&#39;\\)的同构,写作\\(G\\simeq G&#39;\\) 很显然\\(G\\)与\\(G&#39;\\)同构\\(\\iff \\begin{aligned} G\\)与\\(G&#39;\\)同态\\(\\and G&#39;\\)与\\(G\\)同态,且易得同构是一个等价关系(equivalence relation) 在不关注点和边的标号时,我们认为同构的图相等,此时记作\\(G=G&#39;\\) 图的运算 一下记 \\(G=(V,E)\\),\\(G&#39;=(V&#39;,E&#39;)\\) 图的并交补 定义\\(G\\cup G&#39;=(V\\cup V&#39;,E\\cup E&#39;)\\),交同理 若\\(G\\cap G&#39;=\\varnothing\\)则称它们不相交(disjoint) 定义\\(\\overline G=(V,\\binom{V}{2}-E)\\)为图\\(G\\)的补图(complement) 子图(subgraph) 若\\(V\\subseteq V&#39;\\and E\\subseteq E&#39;\\),则说\\(G\\)是\\(G&#39;\\)的子图,记作\\(G\\subseteq G&#39;\\) 导出子图(induced subgraph) 若\\(G\\subseteq G&#39;\\and \\forall x,y\\in V(xy\\in E\\iff xy\\in E&#39;)\\),则称\\(G\\)是\\(G&#39;\\)的导出子图,记作\\(G=G&#39;[V]=G&#39;[V(G)]\\) 生成图/支撑子图 若\\(G=G&#39;[V(G&#39;)]\\),则称\\(G\\)是\\(G&#39;\\)的一个生成图/支撑子图 连通分支/分量(component) 极大的连通子图被称为一个连通分支/连通分量 图的特性(property) 若\\(G&#39;\\subseteq G\\and G&#39;\\simeq H\\),则记\\(P(G,H)=1\\),表示图\\(G\\)具有特性\\(H\\);否则为\\(0\\),表示不具有该特性 极大/极小图(maximal/minimal) 我们称一个\\(G&#39;\\)的子图\\(G\\)是具有某类特性的极大子图意味着: \\(\\forall G&#39;&#39;\\subseteq G\\),都有\\(P(G&#39;,H)=1\\and P(G&#39;&#39;,H)=0\\) 极小同理.类似的定义还可以用在边的数量上/图的size上等等 图的乘法 若\\(G\\)和\\(G&#39;\\)不交,则定义\\(G*G&#39;=(V\\cup V&#39;,\\left\\{xy|xy\\in E\\or xy\\in E&#39;\\or (x\\in V\\and y\\in V&#39;)\\right\\})\\) 比如说\\(K_2*K_3=K_5\\) line graph(不会翻) 对于图\\(G=(V,E)\\),构造图\\(G&#39;=(E,E&#39;)\\),其中\\(xy\\in E&#39;\\iff\\) 边\\(x\\)和边\\(y\\)在\\(G\\)中相邻 邻点(set of neighbours) 对于图\\(G\\)中的点\\(v\\),定义\\(N_G(v)=\\left\\{x|xy\\in E\\right\\}\\),把这个集合称为\\(v\\)的邻点集 度数(degree/valency) 定义图\\(G\\)上点\\(v\\)的度数为\\(d_G(v)=|E(v)|\\),其中\\(E(v)=\\left\\{xv|xv\\in E\\right\\}\\) 度数为\\(0\\)的点被称为孤立点(isolated vertex) 记\\(\\delta(G)=\\max\\left\\{d_G(v)|v\\in V\\right\\}\\) 记\\(\\Delta(G)=\\max\\left\\{d_G(v)|v\\in V\\right\\}\\) 正则图(regular graph) 我们称所有点度数相等的图为正则图.所有点度数为\\(k\\)的图称为\\(k-\\)正则图 显然有\\(G\\)是正则图\\(\\iff\\delta(G)=\\Delta(G)\\) 特殊地,\\(3-\\)正则图也叫做cubic 距离(distance) \\(x,y\\) 之间的距离定义为 \\(\\min{xPy}\\)，\\(P\\) 为一条 \\(x-y\\) 路。距离记作 \\(\\text{dist}(x,y)\\) 直径(diameter) 图 \\(G\\) 的直径定义为 \\(\\max{(\\text{dist}(x,y))}\\)，记作 \\(\\text{diam} \\;G\\) 半径(radius) 定义半径为 \\(\\min\\limits_{x\\in V(G)}\\max\\limits_{y\\in V(G)}{\\text{dist}(x,y)}\\)，记作 \\(\\text{rad}\\; G\\) 取到半径的端点记作中心点(central vertex) 森林(forest) 森林是无圈图 树(tree) 树是连通的森林 代数基础 考虑集合 \\(\\left\\{\\;0,1\\;\\right\\}\\) 和其上模2的加法、乘法运算，容易验证这是一个数域，记作 \\(F_2\\)。 考虑 \\(V={F_2}^{|G|}\\)，\\(V\\) 中的元素都是长度为 \\(|G|\\) 的01向量，很显然 \\(V\\) 是 \\(F_2\\) 上的线性空间，我们称之为点空间 类似的考虑 \\(E={F_2}^{||G||}\\)，这是边空间 不难发现 \\(V\\) 中的每个向量对应着一个顶点的集合(特征函数)，\\(E\\) 有类似的结论。 \\(E\\) 有一个特殊的子空间 \\(C\\)，\\(C\\) 中是所有圈组成的线性空间。","tags":["Graph Theory"]},{"title":"数据结构01 搜索树","path":"/2021/05/29/DS01-搜索树/","content":"","tags":["数据结构"]},{"title":"ICPC 2021 银川划水记","path":"/2021/05/17/ICPC-2021-银川划水记/","content":"一个周末发生了很多事情，值得记录一下 day -1 前夜被告知航班取消了，匆忙换成了从合肥出发的灰机。早上匆忙赶到南京南的时候因为太着急甚至走错了路。在列车即将晚点的时候又被告知退掉高铁票改去徐州，疫情！ 人生第一次坐到了高铁一等座，座椅确实宽敞。到机场前短暂地体验了一波徐州的氛围，感觉和东莞差不太多，除了他们有自己的飞机场（囧 终于见到了碧海潮生和labelray本人，值了。两点半的飞机直接晚点到七点，期间一直在和简叶打雀，顺便看了一点少女漫，四处找人聊天。 到银川的时候已经十点了，沿途观察了一波地形，真是平坦啊。到酒店之后叫了一点外卖，很快就到了。躺上床就合了眼 day 0 早上爬起来才想起要去打板子，俩人四处晃悠问了三家店，看价格深感学校的好。最后货比三家还是爬回了第一家。出于某些原因没有打到目录（伏笔*1） 下午打车去NXIST，司机把能压的线都压全了，甚至没有系安全带，后怕（ 在学校里转了30 min才找到场地，问了三个志愿者分别指向了卖土特产的、饭堂和体育馆，迷惑 找场地的时候绕了一会儿，旁边是熟悉的广式口音，类目。CSHwang 在隔壁的隔壁的隔壁，被抓到了。看了一波参赛手册，觉得这波能Ag就是成功（flag，毕竟划了一整年的水。 热身赛是四个sb题，没啥好说的，唯一的难点在于古早gnome和编辑器。本来看到CLion还很高兴，直到打开它告诉我没有License，这么敷衍的吗（摔） 找半天找不到firefox，最后机智地firefox &amp;，当然直接点开cpp reference也可以，好评 评测姬中途直接停了，美其名曰“stress test”。开场1min全场都在交，然后就听到后面的摄影大叔：哇塞，一开始就过了这么多题目啊 出来之后听说本机栈是unlimited，死循环会直接卡死（伏笔*2） 有人在群里问要不要预定（伏笔*3）恰羊腿，非常心动就和GeWugu约了 拿着15r饭票的时候还在想够不够吃的问题，直到我看到了NXIST的感人物价：10r三个菜，3r一杯的饮料，甚至饭票没花完。九乡河人落泪.jpg 晚上睡了1h左右，醒来刚好看到简叶在写EL，于是我也就上了。接了一个E的锅，意识到是个水题就写了，中间因为LL和模太多挂了两发（伏笔*4），所幸还是过了。打完算了一波分，大概下盘不翻车就有rk2了。恰钱真难.jpg 大概十点出门吃宵夜，被告知羊腿要预约（预定），于是爽快决定去烧烤。找了一家店手抖点了五份考得很好的金(Au)针菇，气氛还是很不戳的。回到酒店开始赶当晚11:59的ddl，幸好带了笔记本 晚上睡得比较晚，水了很久的群。 day 1 貌似这波只有day 1..... 早上出门吃杂酱面，那边面食特别香。大概45分才让进门，差点以为要坐地上看题了。 发的题面甚至有封条，好评；密码条不是等宽字体，分不清0和O，差评； 看了一波昨天试机的配置还在，只是文件都没了，开始后悔没有设置snippet，血亏； 桌面上的cpp reference无了，oj地址也无了，于是开始发呆。 密码一发就过了，感觉很OK。开场我开了K，简叶开D，wky开A。事实上这个开法还算可以。K的题意有点绕，我跳过了第一段，直接导致题意看歪了。再加上第一发写错了下标，dfs直接就把系统都整挂了。万般震惊之下举手示意志愿者，小鸽鸽竟然表示这是正常现象。于是直接重启打印下场让wky做A。A写了一波就过了，再就是简叶写D，我看一波榜决定再开J和E。 看了一波E感觉是不太难的讨论，就把锅分给了wky然后去看J，意识到J是个sb模拟，就觉得这波五题应该稳了，怎么也得冲个Au吧（flag） 然后开始等简叶的AC，然后就看他写了30 min。感觉到不对劲就自己看了一发题面，于是发现按照他的做法过不得，这厮算错复杂度了。此时Au线是三题，我就开始着急了。上去抢键盘写掉了J，经历了行末空格就过了。wky讨论了一波E，也过了。于是开始回头看K，读了三次题才发现理解错了，不过改改就能A。此时痛失K的一血。看一波榜发现此时四题刚好Au线，心里开始鸡冻。 然后就开始了漫长的讨论-灵感-没有气球三连。中间经历了wky猜M的结论，简叶写B的假做法，我干D的LCT做法，我写G的假做法。三个人手上捏着四道题，但是两个不会写两个调不出。大概是吃午饭的时候决定跟榜全力搞G，然后就封榜了。这个时候瞄一眼，你南四支队都是四题（绝活），我们在100+，鸽呜咕在疯狂交B。直到认真看G才发现是nqlog刚好被卡的范围，于是十分自信地卡了30 min的常数。最后还剩3 min的时候突然想到只需要倒着做就可以用链表维护相邻关系了，这不就是sb题吗（摔 于是和队友说了想法，他们纷纷表示是正解，然后就躺了。最后一分钟认真看了看周围，原来xjj还是很多的，也许真的有人能在ACPC收货爱情吧（雾）。 离开之前疯狂刷新，最后还是苟在了Ag线上，个无辜刚好差三名，这也太极限了（。 在体育馆外面想着拍张合影，毕竟是第一次线下赛。于是三个人就拍了一张图上全是脸，脸上全是口罩的阴间照片（脸太大了） 一队貌似心态有点崩，不过已经打完了，像我这种容易满足的人总是没什么好不平的，也算好事吧。 飞机不出意料地晚点了，到学校已经是十一点。想想我出门都没刷卡，也没请假，只是和辅导员说了一声，还是有点后怕的。 总结一波 我和队友都算划水的那一类，平时课业也多，所以练得很少（或者说几乎没有，呃，碰头）。这次出门看到了强队准备的模板和词典，看到了强队明确的分工和各有所长，得知了强队每周训练频率，看来勤奋在哪里都是硬通货。有追求、有梦想当然就有热情持续投入，自然会比原来的自己要强一点点，这也是算法竞赛美妙的地方——你总要不断超越自己。不过我不算太肝这方面的人，参加也只是抱着旅游的心态，因此能目睹如此青春的奋斗盛会也算是很值了。当然这三天里和队友干饭旅游吹水的感觉还是很不错的，感谢算法竞赛给我带来的宝贵友谊。这一轮打下来，可以看到强队失意，可以看到弱队翻盘，既有结束前的绝杀，又有开局稳定遥遥领先。在这样百态的人生之中，我普普通通地过活，勉勉强强地学习，也算是活出了我自己的样子吧。 暑假，一定，补题，加练（flag）","tags":["XCPC"]},{"title":"图论02 匹配","path":"/2021/04/30/Graph02-匹配/","content":"","tags":["Graph Theory"]},{"title":"CSAPP实验06 : shlab","path":"/2021/04/21/CSAPP实验06-shlab/","content":"","tags":["CSAPP"]},{"title":"CSAPP实验05: cachelab","path":"/2021/03/07/CSAPP实验05-cachelab/","content":"","tags":["CSAPP"]},{"title":"大一上存活经验","path":"/2021/02/19/大一上存活经验/","content":"大一上存活经验 希望能给下一届的小朋友留点存活经验，毕竟俺也没有真正意义上的直系学长....真是残念 九月 开学前几天就到了南京，热得一言难尽。晚上骑车去了鼓楼新街口，宇宙中心.jpg 宿舍分配很迷，18是填表配对、19是生日配对、20是生源地配对，导致我带去的土特产无人稀罕。感觉怎么想都是18科学啊（摔 军训据说水分单调递增，时间甚至只有两周，唯一的困难在于早起。期待了很久的打靶只用上了电子枪，军训的话人均85，神枪手和优秀营员会有加分 中途还下了几天的雨，就呆在宿舍里睡了 买了支x宝的单车月卡，感觉还比较划算。 在军训途中溜去了二次选拔，不得不说zs老湿气氛能力一流。前几天就被软院群里浩瀚的往年题和答案吓到了，因此看了一圈还是去了信计，怎么都不会比软院更坑吧.jpg 拔尖和羟基是分开考的，一轮下来数学物理语文英语都来了。想了想物理肯定干不过物竞的，最后确实录了一堆物理人，迷惑。 面试的时候问了一堆奇怪的问题（当时还没有意识到我前面坐着的是frr）：“你是广东的啊，会说粤语吗”“有啥爱好？还挺少见”“你是软院的学生，能说说两边培养的区别吗” 瞎扯了几句就溜了，回去的时候慢慢悠悠反正请了半天假。 最后当然还是录上了。早知道这样为啥一开始不来南京面试呢（疑惑 20年的英语分级考试是线上考的。考试的设置让人摸不着头脑：听力进度条可以随便拖，但是听力部分的总时长有限制。我对着进度条来回对答案，还剩一题的时候突然就变成阅读了。这导致最后20min对着屏幕右下角的人脸发呆，然后意识到那张脸原来是我的orz 分级的话能去一层次就去吧，不然还是交白卷去三层次摸鱼好。这种课比较看老湿，但是二层老师榜上一片黑就离谱QAQ 在招新群里混了一个社团，晚上直接去了根据地面基。感觉良好，还挺喜欢这个氛围的 剩下的不太记得了.... 十月 国庆和网友去了一趟鼓楼拍照打卡，软院的学生吐槽软院院楼.jpg，NTR人狂喜 国庆晚上会有操场电影放映，约人草地游也非常不错，当然如果想认认真真看电影还是算了，比较考验单身狗在情侣面前的脸皮厚度 二号的时候响应网友号召去了一趟漫展，结果场子很小，人也很少。不过年轻人非常多（废话），看pljj还是不错的 还有定向越野之类的活动，顺便也去玄武湖转了转。 五号的时候新电脑到了，还专门在工作室来了个开箱、换硬盘。不得不说还是15寸适合俺 七号被安排了参观校史馆。只能说圣遗物比较有看头，大家争相合影留念。这个也要看讲解老湿，哪里人多往哪里靠就好了~ 随便跟znr、wky组了个队去打ACM，三个人场上日常懵逼状态，通常是要么机子没人碰，要么三个人抢键盘..... 大概看了看自己的水准觉得需要开始打CF了，结果半夜开了一把给我睡着了...第二天还是早八，醒来整个人都是懵的 就再没打过即时比赛 社团招新的时候已经被当成牛马使用了，奇观：用剪网线吸引小情侣来展位。从此接网线代替钓鱼成为你社招牌活动 十月下旬的时候接了人生第一个维修单。本来以为是尸检结果机子给复活了，中途出现了一堆奇奇怪怪的责任问题，最后还是机主爽快重买解决了问题。还是不够成熟... 十一月 双十一买了一把带靠背的椅子，神器！还买了一点衣服，希望可以学会穿搭吧... 小姿势：理发一般近一点就在不靠谱的阿玛尼，远一点的话可以去靠谱的四组团。剪前阿玛尼剪后尼玛阿 不是说着玩的 罗森的饭团很方便，周四满课的中午就靠这个活下去了 一般这个时候枫叶开始红，可以约着出去公园玩，虽然俺没有被约过.... 期中 期中只考了数分、高代、英语 数分就没什么好说的，老范出题毕竟还是很温和。往年题也有，拿来做做再把教材的习题写完就差不多了，难度大概就作业级别吧 高代改分就很迷，所有人都出现了估分差，助教甚至在归纳法的严格写法上设置了扣分点 英语提前一周开始背单词做往年题就行，如果追求90+就背背课文，看着哪个是课本单词选就完事了.... 十二月 中旬的时候下雪了，还专门出门感受了一下。操场上会有堆雪人，可以约着出去玩 冬天必备羽绒，一定要带个帽子那种。手套也是神器，买薄一点就行，骑单车的时候就知道有多舒服了。广东人瑟瑟发抖.jpg 冬天洗澡非常麻烦，那些赤膊下楼洗澡的都是抗冻猛士，我只敢打两桶热水用洗澡精灵。 从yjy那里嫖来一张迎新晚会的门票，确实水平要比高中晚会高到不知道哪里去了。然而坑爹的地方在于参加晚会需要1. 转发集赞 2. 排队拿票 3. 抽时间去看 通常第一步就把我劝退了，排队更是磨人....以后再尽量找时间去吧。当然如果有关系和内部人员PY也是十分OK的 十二月的最后一周都在干饭，和不同的人在不同的地方干饭，总之很快乐就是了。痛苦都是钱包的 跨年那天点了炸鸡外卖在工作室喝啤酒用投影仪看B站晚会，鉴于晚会的效果其实当天气氛有点尴尬 一月 所谓考试周就是在课程结束后专门用来考试的一段时间。在考试周理论上你是自由的。 考试周前就一直在看导论，在图书馆借了rosen的书、cantor的书、各种各样的书....顺便把课文和单词给背了 告诫后人：单词一定不要拖到最后一周来背，除非你的舍友也这么做（指可以陪着你一起背.... 9号的时候去了一次阅读的线下课，在南博看花鸟画。个人觉得很有意思。当然得有老湿在一边解释俺才能看出妙处来，不然就有点困 考完了还和ltygg、gcxgg出去吃烤肉打羽毛球，感觉这才是大学生活啊kora 在15号晚收了一台2k显示器，有点爽的 期末 你南的GPA有点独特，俺还没有摸清楚各种GPA的用处，等暑假再填坑（一定 数分 4.6 5学分，太哈人了.jpg 数分应该是作业只要交了就給满平时分的，上课也不点名 期末就是原题大作战，难点在于前面的定义做法。压轴的甚至是危机昏的往年题，凭记忆写出来就完事了 期中给了94，感觉是期末哪里写挂了，但是又不知道具体哪里写挂了....不管了 数分的话好好看书，推推定理就差不多。网课和辅助教材推荐陈纪修，老范上课用的是梅加强的数分教材（毕竟是数学系的），如果觉得不够可以去看那本（ 你专业的数学甚至只是通修课，离谱 高代 4.5 期末特别离谱，frr直接丢来一道IMO原题强行正态分布 更离谱的是提前交卷人，jjppp被开除人籍.jpg 高代还是教材不太行，但是丘维声的那本又有点厚....我觉得可以两边对着看，反正内容都在小本里，证明就看大本的就好了 习题看着做吧，反正frr留的都是计算，有啥用啊（摔 导论 4.4 这门课很难，但是考试远没有你想象中那么难（迫真 我觉得前半学期可以说是难度顶天了，后半学期的递归方程和渐进复杂度都还好说 难点主要在理解和应用吧（废话，主要是从“序”的引入开始就变得鬼畜了，中间\\(\\omega\\)的部分还可以理解，到了transfinite induction就已经完全掉线了.... 没啥好说的，这个可以看看其他学校数学系公理集合论的的讲义（md题目甚至可以从拓扑学实分析课本里出出来，我不做人啦.jpg），有一本小绿色的俄国人写的书上的习题还挺有意思的，康拓的书也可以看看（不过只能拿来形象理解了...），习题课嘛.....别睡着就好了 考试的时候放掉了一道水题，悔恨的泪是咸的（掀桌 实验 4.8 twnb！ 所谓的试验就是程设基础+stl大杂烩，这个有手就行。当然有一些关于内存的小姿势要自己看看，这个看CSAPP就差不多了，寒假再看也行（吧 最后是没有考试、平时课后有&lt;=3题的小作业、期末有一个造小游戏的大作业，随便卷卷就好了。唯一的槽点在于助教，数据实在太水了.... 听说读写 4.5 听说没啥好说的，全凭老湿选的好。读写坑比较多 读写一般会有课后作业，事情多的会两三周就一篇文章啥的。上课还有尴尬的点名提问，在回答了几次之后还会在冷场时求助你的救场.... 期末听说比较难，主要是听不清的问题（囧，这个莫得办法，最好还是对着原文过一遍课本的听力 读写的难点在于那个B级词汇，我反正是弃疗任选，毕竟只背了A级词汇.... 思修 4.2 lbj思修画重点还行，重点特别多（有好多划出来迷惑人却不考的重点... 实践是群友临时组了个队，最后一周1w字的报告1人1k，找个人上台，找个人做问卷就刚好12人的工作量了。也没有特别上心，还挺划算的吧... 考前一周都在背重点，俺还简略地抄了一些在本子上，然鹅还是出现了不入脑的情况，不管了.... 平心而论感觉这个分有点低，问了一圈貌似大家都不高，可能是今年压分了（我瞎说的 羽毛球 4.5 这学期突然说要跑2k4就离谱，还要计入期末成绩，这是向某体校看齐吗 最后看着表压线跑完了13'30''的及格成绩，没啥好说的 羽毛球期末还算友好，发发球、对打一下就过了，认真练练买点好球也能高分过 体测就不说了，都是泪=_=! 一些通识课 通识课需要1. 看脸 2. 手速。本非酋通识课第一轮只中了几乎是送的软工导学，离谱。后面学到了把笔记本屏幕竖起来抢课的姿势，这样就不用划到底才能确认了（惨 开放抢通识的时候没有太在意，想起来去看的时候才发现已经莫得选了。建议早点蹲蹲抢点网课，在宿舍看视频他不香吗。 俺的通识都是乱选的，也没有特意看过榜，这里也仅供参考 软工导学好评，平时不点名，期末交论文。即使是听故事也很有趣，人多也方便写作业啥的，还可以抱周围的大腿（毕竟软工认识的人比较多，最后4.25 地理信息系统好评，期末交论文，只有最后一节课点名，平时课堂很催眠，报告厅的椅子很好睡觉。认真听也可以得到一点小知识，还可以体验一波转码农的洗脑传教（雾，最后也是4.25 科学之光的微结构光子。现工院开的课，平时的课程比较有意思但是讲课很尴尬，可能这些大佬都不怎么给本科生上课了。这课第三周公布考核方式，第八周增加作业论文数量，结课前一周宣布增加开卷考试环节，鼓励提问加分、论文数量加分、多劳多得，也就是内卷加分。最后开卷的题目是写一写对课程的理解和建议，我直接把论文又抄了一遍....何必呢。不过给分还行，最后是4.55","tags":["Eureka Moments"]},{"title":"CSAPP实验04: archlab","path":"/2021/02/17/CSAPP实验04-archlab/","content":"","tags":["CSAPP"]},{"title":"半音阶口琴谱合辑","path":"/2021/02/14/半音阶口琴谱合辑/","content":"","tags":["Eureka Moments"]},{"title":"CSAPP实验03 : attacklab","path":"/2021/02/04/CSAPP实验03-attacklab/","content":"","tags":["CSAPP"]},{"title":"CSAPP实验02 : bomblab","path":"/2021/01/24/CSAPP实验02-bomblab/","content":"","tags":["CSAPP"]},{"title":"CSAPP实验01 : datalab","path":"/2021/01/14/CSAPP实验01-datalab/","content":"考试周除了学习什么都好玩，偶然发现了B站上的“精翻”视频，就冲了 第一章的视频还没看完(太长了quq),这里也只是写了整形的lab,写了大概有一整天 明天烤完高代就滚回来填这个lab、课程笔记、导论4、集合论习题的坑...好像有点多,不管了 这些只在本地btest过,不保证能对...如果有错或者有更好的做法欢迎指正! upd:做完了，爽耶 tricks \\([a=b]\\iff [(a \\otimes b)=0]\\iff [(a-b)=0]\\) 这个视能否使用\"-\"和\"^\"来选择,相当于不用if做出了判断是否相等 \\((111\\dots 11)_2=(-1)_{10}\\) 这个...没啥好说的 \\(f(flag,x)=\\left\\{ {\\begin{aligned}0,flag=0\\\\x,flag=1\\end{aligned} }\\right.\\iff x\\&amp;(-flag)\\),结合2就可以理解,结合4很有用 \\((-x)=( (\\sim x) + 1)\\),这个实际上就是电路中减法的做法,这里可以看出反码在简化运算中的作用 bitXor 根据集合论/数理逻辑的知识可以很快想到异或的\"对称差\"定义 //1 /* * bitXor - x^y using only ~ and &amp; * Example: bitXor(4, 5) = 1 * Legal ops: ~ &amp; * Max ops: 14 * Rating: 1 */ int bitXor(int x, int y) &#123; int fx = ~x, fy = ~y; int tx = fx &amp; y, ty = fy &amp; x; return ~((~tx) &amp; (~ty)); &#125; tmin 这里的最小指的是补码对应数值最小...这个直接符号位填1就好了 /* * tmin - return minimum two&#39;s complement integer * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 4 * Rating: 1 */ int tmin(void) &#123; return (1 &lt;&lt; 31); &#125; isTmax tmax的特点是除了符号位都是1,那么加上1就得到了tmin,取反仍然是tmax 但是除了tmax还有别的数有这个性质:-1，排除掉就好了 //2 /* * isTmax - returns 1 if x is the maximum, two&#39;s complement number, * and 0 otherwise * Legal ops: ! ~ &amp; ^ | + * Max ops: 10 * Rating: 1 */ int isTmax(int x) &#123; int t = x + 1; return !( ( (~t) ^ x) | (!t)); &#125; allOddBits lab有要求不能使用超过255的常量,那么一个想法就是把32bits分成4*8bits, 我们造一个(10101010)来复制4份就可以到奇数位全为1的二进制数，然后就很简单了 /* * allOddBits - return 1 if all odd-numbered bits in word set to 1 * where bits are numbered from 0 (least significant) to 31 (most significant) * Examples allOddBits(0xFFFFFFFD) = 0, allOddBits(0xAAAAAAAA) = 1 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 2 */ int allOddBits(int x) &#123; int t = 170 | (170 &lt;&lt; 8); t = t | (t &lt;&lt; 16); return !((t &amp; x) ^ t); &#125; negate 看trick4 /* * negate - return -x * Example: negate(1) = -1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 5 * Rating: 2 */ int negate(int x) &#123; return (~x) + 1; &#125; isAsciiDigit 这道题就比较灵性... 观察一下asciiDigit的特点,最后6位都形如\\((11xxxx)_2\\),而最后4位恰好是\\((0000)_2\\sim (1001)_2\\) 我最早的做法是把后6位抠出来,用倒数第4位判掉0~8的情况,再判掉最后2位的情况 事实上做了后面的isLessOrEqual就可以发现这里的另一种做法了...不是很懂这个顺序啊 //3 /* * isAsciiDigit - return 1 if 0x30 &lt;= x &lt;= 0x39 (ASCII codes for characters &#39;0&#39; to &#39;9&#39;) * Example: isAsciiDigit(0x35) = 1. * isAsciiDigit(0x3a) = 0. * isAsciiDigit(0x05) = 0. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 15 * Rating: 3 */ int isAsciiDigit(int x) &#123; int A = !( (x &gt;&gt; 4) ^ 3); int B = !(x &amp; 8); int C = !(x &amp; 6); return A &amp; ( B | C ); &#125; conditional 利用trick3就可以做了，构造\\(f(flag,x)\\otimes f(!flag,y)\\)就好了 我在写到这里的时候没有意识到trick4可以用,所以写的比较繁琐 /* * conditional - same as x ? y : z * Example: conditional(2,4,5) = 4 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 16 * Rating: 3 */ int conditional(int x, int y, int z) &#123; int px1 = !x; int px2 = !px1; int ty = ( (px2 &lt;&lt; 31) &gt;&gt; 31 ) &amp; y; int tz = ( (px1 &lt;&lt; 31) &gt;&gt; 31 ) &amp; z; return ty ^ tz; &#125; isLessOrEqualTo 最直观就是做差,判断\\(\\triangle\\)的符号位 然而当两个数异号的时候,他们的差会溢出,为了处理这种状况我们要先判掉异号的情况,这样同号运算就是在范围内的了 /* * isLessOrEqual - if x &lt;= y then return 1, else return 0 * Example: isLessOrEqual(4,5) = 1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 24 * Rating: 3 */ int isLessOrEqual(int x, int y) &#123; int d = x + (1 + (~y) ); int fx = (x &gt;&gt; 31) &amp; 1; int fy = (y &gt;&gt; 31) &amp; 1; return ( (!d) | ( (d &gt;&gt; 31) &amp; 1) | ( fx &amp; (!fy) ) ) &amp; !( (!fx) &amp; fy); &#125; logicalNeg 可以发现符号位不重要,第一步先去掉符号位得到\"绝对值\" 如果是0的话取反就会得到-1，否则都得不到-1 此时加1又可以得到0,即符号位为正,而其余情况得到的都是负数 这个性质可以判掉\"大部分\"非0数字,特例是-2147483648,它没有绝对值(或者说,\"绝对值\"是0)...所以特判一下就好了 //4 /* * logicalNeg - implement the ! operator, using all of * the legal operators except ! * Examples: logicalNeg(3) = 0, logicalNeg(0) = 1 * Legal ops: ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 4 */ int logicalNeg(int x) &#123; int tx = (~x) | (1 &lt;&lt; 31); return ( ~( ( ( tx + 1 ) | x ) &gt;&gt; 31 ) ) &amp; 1; &#125; howManyBits 先考虑正数,我们要找的就是最高位的1在哪(第几位) 负数的情况比较特殊,因为从符号位开始连续的1序列和单独的一个符号位1等价(回忆课堂上的Sign Extension),那么我们只需要保留一个符号位,也就是只需要找到最高位的0就可以了 于是负数就取反,找最高位的1可以用二分(魔幻吧),想了好久才想到... 先判断前16位是否有1,然后通过右移来调整下一次判断的区间,以此类推...就可以了 /* howManyBits - return the minimum number of bits required to represent x in * two&#39;s complement * Examples: howManyBits(12) = 5 * howManyBits(298) = 10 * howManyBits(-5) = 4 * howManyBits(0) = 1 * howManyBits(-1) = 1 * howManyBits(0x80000000) = 32 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 90 * Rating: 4 */ int howManyBits(int x) &#123; int t1, L1, t2, L2, t3, L3, t4, L4, t5, L5; int s = ~1 + 1, rx = x, cx = !x; int p = (x &gt;&gt; 31) &amp; 1, dx = !(rx ^ s); x ^= ~p + 1; t1 = s &amp; (x &gt;&gt; 16); L1 = ( (!!t1) &lt;&lt; 4); x &gt;&gt;= L1; t2 = s &amp; (x &gt;&gt; 8); L2 = ( (!!t2) &lt;&lt; 3); x &gt;&gt;= L2; t3 = (x &gt;&gt; 4) &amp; s; L3 = ( (!!t3) &lt;&lt; 2); x &gt;&gt;= L3; t4 = (x &gt;&gt; 2) &amp; s; L4 = ( (!!t4) &lt;&lt; 1); x &gt;&gt;= L4; t5 = (x &gt;&gt; 1) &amp; s; L5 = (!!t5); x &gt;&gt;= L5; return L1 + L2 + L3 + L4 + L5 + 2 + (1 + ~cx ) + (1 + ~dx); &#125; floatScale2 浮点数的编码很有意思 分类讨论。首先判掉NaN和INF,对于denorm的形式我们只要左移frac部分,对于norm形式我们只需要增加指数exp(why?) 这个例子大概是给你熟悉浮点数编码分类的 //float /* * floatScale2 - Return bit-level equivalent of expression 2*f for * floating point argument f. * Both the argument and result are passed as unsigned int&#39;s, but * they are to be interpreted as the bit-level representation of * single-precision floating point values. * When argument is NaN, return argument * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 */ unsigned floatScale2(unsigned uf) &#123; unsigned s = (uf &gt;&gt; 31) &amp; 1; unsigned e = (uf &gt;&gt; 23) &amp; 255; unsigned m = uf &amp; 8388607; if (e == 0) &#123; m = m * 2; &#125; else if (e != 255) &#123; e = e + 1; &#125; return (s &lt;&lt; 31) | (e &lt;&lt; 23) | m; &#125; floatFloat2Int 试了一下,C里面的强制类型转换会截掉小数点后的部分,除非某种类似1.9999999999999999999的例子,在这个例子下类型转换会变成2(why?) 事实上第二种情况我们不需要考虑,因此只需要把frac部分抠出来,前面添上1,按照exp-bias得到的指数位e偏移即可。很显然如果它是一个denorm/指数为负的norm的话答案就是0 /* * floatFloat2Int - Return bit-level equivalent of expression (int) f * for floating point argument f. * Argument is passed as unsigned int, but * it is to be interpreted as the bit-level representation of a * single-precision floating point value. * Anything out of range (including NaN and infinity) should return * 0x80000000u. * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 */ int floatFloat2Int(unsigned uf) &#123; int s = (uf &gt;&gt; 31) &amp; 1; int e = (uf &gt;&gt; 23) &amp; 255; int m = uf &amp; 8388607; int bias = 127, i = 22, r = 1; if (e == 255) &#123; return 0x80000000u; &#125; else if (e == 0) &#123; return 0; &#125; else &#123; e -= bias; if (e &lt; 0) return 0; for (; i &gt;= 0; i --) &#123; if ( (m &gt;&gt; i) &amp; 1) break; &#125; while (e &gt; 0) &#123; e -= 1; i -= 1; r &lt;&lt;= 1; if (i &gt; 0) r |= ( (m &gt;&gt; i) &amp; 1); if (r &lt; 0) return 0x80000000u; &#125; return s?(-r):r; &#125; &#125; floatPower2 这个也挺简单的... 从这个题可以看出单精度(32位)浮点数能表示的数字的范围 最大值是norm形式,exp为254(再大就是NaN和INF了),frac的每一位全为1(虽然在这题里不是这样),就能得到\\({\\left(2-\\epsilon\\right)}^{127}\\) 最小值是denorm形式,exp为0,frac为1,此时指数e是1-127,尾数额外提供了23位的指数,这样就得到\\(2^{-149}\\) 这样直接做就可以了 /* * floatPower2 - Return bit-level equivalent of the expression 2.0^x * (2.0 raised to the power x) for any 32-bit integer x. * * The unsigned value that is returned should have the identical bit * representation as the single-precision floating-point number 2.0^x. * If the result is too small to be represented as a denorm, return * 0. If too large, return +INF. * * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. Also if, while * Max ops: 30 * Rating: 4 */ unsigned floatPower2(int x) &#123; int s = 0, m = 0, e = 127; if (x &lt; 0) &#123; if (-x &gt; 149) return 0; else if (-x &lt;= 126) &#123; e = 1; &#125; else &#123; e = 0; m = (0x400000u) &gt;&gt; (-x - 126); &#125; &#125; else if (x &gt; 0) &#123; if (x + 127 &gt; 255) return 0x7f800000; else e = x + 127; &#125; return (s &lt;&lt; 31) | (e &lt;&lt; 23) | m; &#125;","tags":["CSAPP"]},{"title":"信息与计算科学导论02","path":"/2021/01/06/ICS-Intro02/","content":"这里的递归实际上也可以理解为递推 ##Karatsuba算法 计算界次为\\(n\\)的多项式乘积，\\(naive\\)做法需要计算\\(n^2\\)次 一个本科生(！！！)提出了这样的算法。考虑计算\\(\\left(ax+b\\right)\\left(cx+d\\right)\\) 展开就是\\(acx^2+\\left(ad+bc\\right)x+bd\\)，其中\\(ac\\)和\\(bd\\)无法避免，而\\(\\left(ad+bc\\right)=\\left(a+b\\right)\\left(c+d\\right)-ac-bd\\)，这样我们只需要计算\\(\\left(ad+bc\\right)\\)、\\(ac\\)和\\(bd\\)就可以了，这样就减少了\\(\\frac{1}{4}\\)的乘法次数 再套入递归中，可以得到这样一个乘法次数的计算公式\\(T\\left(n\\right)=3T\\left(\\lfloor\\frac{n}{2}\\rfloor\\right)\\)，注意这个不是时间复杂度，不然还要加上后面加法的n次循环(这样做只是为了方便引入递归方程的概念) 考虑怎么解这个东西。只需要不停地拆开就可以得到\\(T\\left(n\\right)=3^{\\log_{2}{n} }\\times T_0\\)，其中\\(T_0\\)是一个与n无关的常数，所以我们可以认为\\(T\\left(n\\right)=3^{\\log_{2}{n} }=n^{\\log_2{3} }\\)，等号的由来可以两边取对数 这种形式的的方程被称为递归方程(recurrence equation)，通常这类问题没有一般做法，最好的做法是guess and prove…… ##解的结构 这里讨论形如\\(T\\left(n\\right)=f\\left(n\\right)+\\sum\\limits_{i=1}^{k}{a_i T\\left(n-i\\right)}\\)的方程的解的结构，当然括号内部还可以更复杂，这里只是一个简单的情形 类比线性代数，递归方程也有所谓的常系数线性齐次递归方程(\\(f\\left(n\\right)=0\\)的时候)。方程的解同样可以表示为若干通解和特解的和的集合 通解：指令\\(f\\left(n\\right)=0\\)，得到的递归方程的所有解 特解：指带入原方程后满足递归方程的一个解 怎么证明呢？不妨设\\(T_0\\left(n\\right)\\)是一个通解，\\(T_1\\left(n\\right)\\)是一个特解 则\\(T_0\\left(n\\right)+T_1\\left(n\\right)=f\\left(n\\right)+\\sum\\limits_{i=1}^{k}{a_i\\left({T_0\\left(n-i\\right)+T_1\\left(n-i\\right)}\\right)}\\) 也满足这个递归方程，因此我们说\\(T_0\\left(n\\right)+T_1\\left(n\\right)\\)也是原方程的一个解。类似地我们可以得到特解加上任意一个通解都是原方程的解 反过来也是一样的，只需要做差就行了 ##特征方程求解 对于常系数线性齐次递推，我们(教材)猜测解的形式可以是等比数列(guess) 带入看看就是\\(V\\cdot p^n=V\\sum\\limits_{i=1}^{k}{a_i\\cdot p^{n-i} }\\)，这里\\(V\\)是任意常数 这是一个一元\\(k\\)次方程，也是它的特征方程，特征方程的根带入都满足常系数线性齐次幂递归方程。为了简便讨论这里假设没有重根 由解的结构我们知道，这k个通解加起来得到的仍然是原方程的解，并且每一项的系数\\(V\\)是不一样的 怎么证明这是唯一的解呢？假设存在另一个解\\(S\\left(n\\right)\\)，则可以列出线性方程组。这个方程组的系数矩阵是范德蒙矩阵，于是就完了。 对于有重根的情况，解可以写成\\(\\sum\\limits_{i=1}^{k-L}\\left({a_i\\cdot\\sum\\limits_{j=1}^{L_i}{\\left({n^{j-1}\\cdot x_i}\\right)} }\\right)\\) 其中\\(L\\)是不同根的数量，Li是重数 证明：考虑多项式\\(F\\left(x\\right)=x^n-\\sum\\limits_{i=1}^k{a_i\\cdot x^i}\\) 若\\(x_1\\)是多项式的二重根，则它是\\(F&#39;\\left(x\\right)\\)的单根，即\\(F&#39;\\left(x\\right)=nx^{n-1}-\\sum\\limits_{i=1}^k{a_i\\cdot i\\cdot x^{i-1} }\\) 两边同事乘上x得到\\(xF&#39;\\left(x\\right)=nx^n-\\sum\\limits_{i=1}^k{a_i\\cdot i\\cdot x^i}\\)，带入\\(x=x_1\\)恰好等于0 也就是说，\\(n\\cdot {x_1}^n\\)也是这个递归方程的解，解就可以写成类似\\(\\left(1+n\\right){x_1}^n\\)这种形式了 多重根也是类似的，这里就不推了(懒) 这种方法的难点一般在找一个特解和解特征方程。找特解需要一点智慧，而高次方程通常都很不好解…… ##生成函数 这是我最喜欢的方法;-P 考虑这样一个函数\\(F(x)=\\sum\\limits_{i=0}^{\\infty}{T\\left(i\\right)x^i}\\)，它包含了一个数列的所有信息，并且可以作为一个整体处理 这样的东西就是生成函数(Generating Function)，也被称为母函数/形式幂级数。在这里我们不关心它是否收敛，而只在乎它的系数，x只是一个占位符号 以斐波那契数列为例子，\\(F_n=F_{n-1}+F_{n-2}\\)且\\(F_0=F_1=1\\)就可以写成\\(G(x)=xG(x)+x^2G(x)+1\\) 化简就得到\\(G(x)=\\frac{1}{1-x-x^2}\\) 看起来很炫酷，但是你这一顿操作也没有得到数列公式啊？ 首先考虑一个等比数列\\(T(n)=p^n\\)，则很容易写出这个数列的生成函数\\(H(x)=\\sum\\limits_{i=0}^{\\infty}{\\left(px\\right)^{i} }\\) 既然是等比数列，那么就可以写成\\(H(x)=\\frac{1}{1-px}\\)，之所以略去了一项是因为我们可以令x任意取值使得这一项是无穷小 这提示我们可以把\\(G(x)\\)拆成等比数列之和，于是这个就很简单了 如果不能裂项怎么办？再考虑\\(P(x)=\\frac{1}{\\left(1-px\\right)^k}\\) 观察第l项的系数，这里是k个等比数列相乘，这k个项的未知数上指数之和为l，p的指数之和也为l，这里用隔板法就可以做了，系数就是\\(\\binom{l+k-1}{k-1}\\cdot p^l\\) 这里得出的结论和上面的方法是一致的，也就是我们总能得到若干个等比数列的线性组合，它满足递归方程 有一点非常重要的是，在使用生成函数方法时，要小心处理前k项的值，如果是未知的最好设出来，不然会有漏解的情况。不过有的时候用这种方法求一个特解也是极好的，结合上面的方法就很好啦~ 还有一些生成函数的运算法则，这个只需要知道两个生成函数相乘得到的是卷积就可以了，剩下的都比较简单 ##算子 这个听得比较懵逼，但是dl说它足够抽象，可以带来新的视角(囧)，先随便写写回来填坑 算子可以理解为是一个对数列的操作，即\\(opt(T(n))\\)得到得还是一个数列。我们把能使一个数列变成0的算子称为消去子(annihilator) 一个想法是，如果我们找到了把\\(T(n)\\)消成0的算子g，和g能消去的所有数列的集合\\(P\\)，那么我们就可以认为是找到了所有的解(饶舌) 这个想法看上去不那么直观，甚至有点刻意的味道…… 课件给出了两个最基本的算子\\(L\\)和\\(c\\)，分别表示把数列左移一位、每一项乘上常数c 习题中出现的两种算子\\(\\triangle\\)和\\(\\sum\\)分别表示差分和求和(可以理解为求导和积分)，类似的甚至有分步积分的形式 法则： 基础的两种消去子满足交换律、结合律(显然) 假如算子A消去了F，B消去了G，则AB能消去F+G 这里证明一下法则2：\\(AB(F+G)=ABF+ABG=B(AF)+A(BG)=0\\) 例子：解递归方程\\(T(n)=2T(\\sqrt{n})+\\log n\\) 注意到我们知道的递归方程解法都只和加减法有关，于是考虑变形。令\\(n=2^k\\),则有\\(T(2^k)=2T(2^{\\frac{k}{2} })+k\\) 记\\(t(k)=T(2^k)\\)，则有\\(t(k)=2t(\\frac{k}{2})+k\\)，故技重施再令\\(k=2^u\\)，则\\(t(2^u)=2t(2^{u-1})+2^u\\) 于是\\(t&#39;(u)=2t&#39;(u-1)+2^u\\) 先去掉\\(2^u\\)的项观察齐次的部分\\(t&#39;(u)=2t&#39;(u-1)\\)，这个东西的消去子是\\((L-2)\\) 把这个算子施加在数列上可以得到\\((L-2)t&#39;(u)=Lt&#39;(u)-2t&#39;(u)=t&#39;(u+1)-2t&#39;(u)=2^{u+1}\\)，这一步还是很好理解的 那么我们就得到了一个新的数列\\((L-2)t&#39;(u)=2^{u+1}\\)，这个东西的消去子又是\\((L-2)\\)，于是\\((L-2)^2t&#39;(u)=0\\) 这个消去子能消去哪些数列呢？类比消去子和特征方程，不难发现这个东西能消去形如\\((c_1+c_2n)\\cdot 2^n\\)的数列 因此有\\(t&#39;(u)=(c_1+c_2u)\\cdot 2^u\\)，即\\(t(k)=(c_1+c_2\\cdot\\log k)\\cdot k\\) 因此有\\(T(n)=\\log n\\cdot(c_1+c_2\\cdot \\log\\log n)\\) ##渐进解 很多时候(大部分时候)我们是不能得出递归方程的精确解的，通常我们只能得到一个渐进意义下的解(但这不意味着我们不关心确解，当然越精确越好~) 渐进意义下(asymptotically)有点类似数学分析里的等价无穷大/等价无穷小 我们称函数\\(f(x)\\)的增长速度在渐进意义下不超过\\(g(x)\\)，代表\\(\\exists N,M&gt;0\\)，当\\(n&gt;N\\)时，恒有\\(f(x)\\le M\\cdot g(x)\\)成立，即\\(\\lim\\limits_{n\\rightarrow \\infty}{\\frac{f(x)}{g(x)} }&lt; M\\)，其中\\(M\\) 是个常数。 如果这里的\\(M\\)是0，我们认为\\(f(x)\\)也是不超过\\(g(x)\\)的。 上述两种情况记作\\(f(x)=O(g(x))\\)，这实际上是一个不等号，包含了渐进意义下等价和小于两种情况(M是否为0) 需要注意的是，这里我们研究的函数可以认为都是非负的、定义域在正整数上的函数。因为这里的分析来源于算法的复杂度分析，倘若一个算法的复杂度和输入规模无关(或者说消耗时间为负数，它越跑越快雾)，那么我们就没有什么必要研究这个算法了…… 同时还有如下定义： \\(f(x)=\\Theta(g(x))\\iff f(x)=O(g(x))\\)且\\(g(x)=O(f(x))\\)，这样就排除了\\(M=0\\)的情况 \\(f(x)=o(g(x))\\iff g(x)=O(f(x))\\) 不难发现，这三种符号对应了不同的二元关系(联系2、3章)，所有的多项式(这里所说的多项式指数可以是实数，甚至可以带log等函数)都可以由一个最简单的形式代表 还有一个很有意思的结论：\\(\\sum\\limits_{i=1}^{k}{f_i(n)}\\le\\sum\\limits_{i=1}^{k}{f_{max}(n)}=k\\cdot f_{max}(n)=\\Theta(f_{max}(n))\\)，这里\\(k\\)是和n无关的常数。这个结论写出来就证明完了 ##Master Theorem 这个太强了！之前看的一直是简略版本，这里是一个更加详尽的版本 考虑形如\\(T\\left(n\\right)=aT\\left(\\lfloor\\frac{n}{b}\\rfloor\\right)+f(n)\\)的递归方程 分三种情况： 若\\(f(n)=\\Theta\\left(n^c\\right)\\)，且\\(c&lt;\\log_ba\\)，那么\\(T\\left(n\\right)=\\Theta\\left(n^{\\log_ba}\\right)\\) 若\\(f(n)=\\Theta\\left(n^c\\right)\\)，且\\(c&gt;\\log_ba\\)，且\\(f(n)\\)满足正则条件(regular condition)，那么\\(T\\left(n\\right)=\\Theta\\left(n^c\\right)\\) 若\\(f(n)=\\Theta\\left(n^{\\log_ba}\\log^k n\\right)\\)，且\\(k&gt; -1\\)，则\\(T\\left(n\\right)=\\Theta\\left(n^{\\log_ba}\\log^{k+1}n\\right)\\) 正则条件指的是对于函数\\(f(n)\\)，\\(\\exists d&lt; 1\\)，当n充分大时，满足\\(a\\cdot f\\left(\\frac{n}{b}\\right)&lt; d\\cdot f\\left(n\\right)\\) 注意到这里给出的都是\\(\\Theta\\)，这可比网上一搜一大堆的\\(O\\)强多了 一个通俗的理解是，考虑等号右边两项哪个更大，则主要的复杂度取决于哪个。如果要证明可以把递归拆开然后用等比数列求和的公式，讨论公比就可以得到这个结论了。实在不行归纳也可以证明 有的时候不能用注定理，比如说\\(f(n)=n^{1-\\frac{1}{n} }\\)，这里不存在任何一个\\(c=1-\\frac{1}{n}\\)","tags":["ICS Intro"]},{"title":"信息与计算科学导论03","path":"/2021/01/06/ICS-Intro03/","content":"##集合的大小 有限集合的大小很容易比较，只需要数一数，比一比就完了 而无限集不能这么做。我们在这里规定集合\\(A\\)与\\(B\\)大小相等当且仅当存在\\(f: A\\mapsto B\\)为双射 定理：无限集至少和它的一个真子集有双射 证明：考虑\\(A\\)，由选择公理，我们可以取出\\(B\\subset A\\)且\\(B\\)可数，那么\\(f:A\\mapsto A\\backslash B_0\\)就可以取\\(f(x)=\\left\\{\\begin{aligned}\\begin{equation}B_{i+1}, x=B_i\\\\x,x ot\\in B\\end{equation}\\end{aligned}\\right.\\) ##康托定理 集合\\(S\\)总是小于它的幂集\\(2^S\\)，定义\\(2^S=\\left\\{T|T\\subset S\\right\\}\\) 这里蕴含了一个幂集公理，即集合的幂集还是集合 证明：假设存在一个双射\\(f:S\\mapsto 2^S\\)，那么取\\(T=\\left\\{x|x ot\\in f(x)\\right\\}\\)，显然这个集合不同于任何双射中的值域，这就得到了一个矛盾 这个方法叫对角线法则，很好用~ ##可数与不可数 定义\\(S\\)可数(countable)当且仅当\\(\\exists f:\\mathbb N\\mapsto S\\)为双射 定理：\\(\\mathbb R\\)不可数 证明：这是别处看来的，觉得更好理解一些(虽然没有用到对角线法则) 这里先只证明\\([0,1]\\)不可数。反证法：假设可数，则存在一种列举方式使得我们能穷尽所有的实数，记这个数列为\\(\\left\\{a_n\\right\\}\\) 那么对于\\(a_0\\)，我们可以把区间划分为\\([0,\\frac{1}{3}],[\\frac{1}{3},\\frac{2}{3}],[\\frac{2}{3},1]\\)，则至多有两个区间包含了\\(a_0\\)，取剩下的那个区间为下一次的操作区间，重复上述过程 这样我们就得到了一系列区间套，最终会收敛到一个点\\(\\xi\\) \\(\\xi\\in\\mathbb R\\)，但是\\(\\forall i\\)都有\\(a_i eq \\xi\\)，这样就推出了矛盾 ##Cantor-Bernstein定理 其实还有一个名字的，不会写…… 这个定理很直观：若\\(|A|\\le|B|\\)且\\(|B|\\le|A|\\)则\\(|A|=|B|\\) 证明用到了巴拿赫定理 ##Banach定理 若存在\\(f:A\\mapsto B\\)和\\(g:B\\mapsto A\\)都是单射，则 存在\\(A_0,A_1\\)满足\\(A_0\\cap A_1=\\varnothing\\)，\\(A_0\\cup A_1=A\\) 存在\\(B_0,B_1\\)满足\\(B_0\\cap B_1=\\varnothing\\)，\\(B_0\\cup B_1=B\\) 使得\\(f\\left(A_0\\right)=B_1\\)，\\(g\\left(B_0\\right)=A_1\\) 证明有点长，先去吃个饭~ 剩下的内容可以看之前写过的集合大小比较的文章，差不多都齐了……","tags":["ICS Intro"]},{"title":"信息与计算科学导论01","path":"/2021/01/06/ICS-Intro01/","content":"信息与计算科学导论一 ##罗素悖论 考虑这么一个集合： \\(S=\\left\\{T|T ot\\in S\\right\\}\\) 考虑一个集合内的元素\\(x\\)，若\\(x\\in S\\)，则根据定义\\(x ot\\in S\\)，矛盾 若\\(x ot\\in S\\)，则根据定义有\\(x\\in S\\)，矛盾 我们找不到这样一个集合，这就是大名鼎鼎的罗素悖论 ##公理集合论 悖论的源头在于构建集合的描述性方法，我们在使用这个方法的时候出现了所谓“自引用”的情况 为了体系的和谐与自洽，数学家们提出了公理集合论，创造了“类”(class)的概念。 一个集合是一个类，但是所有的类不都是集合。对于那些不是集合的类我们称之为真类(proper class) 可以存在set的set但是不能存在class的class，这样就可以比较和谐地处理一些问题了 关于公理集合论的的具体内容可以看wiki，这里给了一个比较重要的公理——正则公理 若\\(S\\)为一集合，要么\\(S=\\varnothing\\)，要么\\(\\exists x\\in S\\)使得\\(x\\cap S=\\varnothing\\) 这排除了一些看起来是集合然而不太和谐的真类，比如说\\(\\left\\{\\left\\{\\left\\{\\dots\\right\\}\\right\\}\\right\\}\\) 例子：不存在集合\\(A\\)，\\(B\\)使得\\(A\\in B\\)且\\(B\\in A\\) 证明：假设存在集合，由集合公理得到\\(\\left\\{A,B\\right\\}\\)也是一个集合，这个集合与正则公理矛盾 例子：不存在\\(\\left\\{S_0,S_1,S_2,\\dots\\right\\}\\)使得\\(\\forall i\\in\\mathbb N\\)都有\\(S_i\\in S_{i+1}\\) 证明：和上面的例子类似，反证然后用正则公理推出矛盾 ##集合运算 然后介绍了差集(set difference)的概念，即\\(A\\backslash B=A\\backslash \\left(A\\cap B\\right)=A\\cap \\overline B\\) 差集没有交换律，考虑这个例子：\\(A\\cap B=\\varnothing\\)，显然\\(A\\backslash B eq B\\backslash A\\) 差集也没有结合律，考虑这个：\\(A\\cap B eq \\varnothing\\)，\\(A eq B=C\\)，则显然\\(A\\backslash\\left(B\\backslash C\\right) eq\\left(A\\backslash B\\right)\\backslash C\\) 然后介绍了De Morgan Law：\\(A\\backslash\\left(B\\cup C\\right)=\\left(A\\backslash B\\right)\\cap\\left(A\\backslash C\\right)\\) 证明：考虑用上面差集定义，那么\\(A\\backslash\\left(B\\cup C\\right)=A\\cap\\left({\\overline{B\\cup C} }\\right)=A\\cap\\left({\\overline B\\cap\\overline C}\\right)\\\\=\\left(A\\cap\\overline B\\right)\\cap\\left(A\\cap\\overline C\\right)=\\left(A\\backslash B\\right)\\cap\\left(A\\backslash C\\right)\\) 当然证明左右互相为对面的子集也是可以的 接下来引入了笛卡尔积(cartesian product)的概念 定义\\(A\\times B=\\left\\{\\left(x,y\\right)|x\\in A, y\\in B\\right\\}\\) 其中\\(\\left(x,y\\right)\\)是一个有序二元组(tuple)，它的集合定义是\\(\\left\\{x,\\left\\{x,y\\right\\}\\right\\}\\)或\\(\\left\\{\\left\\{\\varnothing, \\left\\{x\\right\\}\\right\\},\\left\\{\\left\\{y\\right\\}\\right\\}\\right\\}\\) 但是n元组却不能简单地类似定义，例如\\(\\left(x,\\left(y,z\\right)\\right)\\)和\\(\\left(\\left(x,y\\right),z\\right)\\)是等价的3元组，但是在这种嵌套方式下它们不等价，所以很多时候数学的表达方式都不是最严谨的。。(比如说zhongsheng老湿的lecture notes) ##关系 定义在集合\\(A\\)上的二元关系(binary relation)指\\(R=A\\times A=A^2\\)，类比还有n元关系 关系有一下四种性质，不同的关系满足其中一个或多个 自反性(reflexive)：若\\(x\\in A\\Rightarrow\\left(x,x\\right)\\in R\\)，则称这个关系具有自反性 对称性(symmetric)：若\\(\\left(x,y\\right)\\in R\\iff \\left(y,x\\right)\\in R\\) 反对称性(anti-symmetric)：若\\(\\left(x,y\\right)\\in R\\)且\\(\\left(y,x\\right)\\in R\\Rightarrow x=y\\) 强反对称性(strongly anti-symmetric)：若\\(\\left(x,y\\right)\\in R\\Rightarrow \\left(y,x\\right) ot\\in R\\) 传递性(transitive)：若\\(\\left(x,y\\right),\\left(y,z\\right)\\in R\\Rightarrow \\left(x,z\\right)\\in R\\) ##传递闭包 然后讲了传递闭包，也就是所谓的\\(Warshall- Floyd\\)算法，也就是最短路。。 证明思路留坑，反正也不难，来填坑了 \\(Warshall\\)算法的过程可以表述如下： 枚举一个中间元素\\(k\\) 枚举一个元素\\(i\\) 枚举一个元素\\(j\\) 若\\(\\left(i,k\\right)\\in R\\)且\\(\\left(k,j\\right)\\in R\\)，则将\\(\\left(i,j\\right)\\)加入\\(R\\) 最后得到的\\(R=R^\\star\\) 由于\\(k\\)是递增的，我们就可以根据\\(k\\)来归纳。我们说一条路径\\(&lt; a , b &gt;\\)指的是\\(\\left(a,c_1\\right),\\left(c_1,c_2\\right),\\dots,\\left(c_m,b\\right)\\)这样首尾相连的关系链，再定义\\(k\\)-路径为除了首尾以外的点编号都不超过k的路径，记为\\(&lt; a, b &gt;_k\\) 那么第0次循环时，所有的路径都是0-路径，这个很简单 假设做到了第k+1次循环，那么对于\\(R^\\star\\)中的关系有两种情况： 这条路径上的最大点小于k+1，那么这条路径已经在前面的循环中被找出来加入\\(R\\)了 这条路径上的最大点恰好等于k+1，那么此时从k+1处断开，剩下的两条链都已经在前面的循环中被加入了\\(R\\)，于是这里可以一步做完 大于k+1，什么都没有发生…… ##等价类的概念 如果一个集合\\(X\\)上的二元关系是自反的、对称的、传递的，那么这个关系就可以被称为一个等价关系(equivalence relation) 定理：集合\\(X\\)上的一个等价关系提供了\\(X\\)的一个划分(partition) 称\\(X\\)的一个划分为\\(Y\\)，当且仅当\\(\\forall x,y\\in Y\\)都有\\(x\\cap y=\\varnothing\\)，且\\(\\cup Y=X\\) 若两个元素有等价关系，则我们称它们属于同一个等价类(equivalent class) 证明：首先由自反性可知等价关系至少包含了所有元素，因此只需要证明这些等价类不相交就可以了 反证法：假设存在一个元素\\(z\\)同时属于等价类\\(X\\)和\\(Y\\)且\\(X eq Y\\) 任取\\(x\\in X,y\\in Y\\)，则\\(\\left(x,z\\right)\\in R\\)且\\(\\left(y,z\\right)\\in R\\)，由传递性可知\\(\\left(x,y\\right)\\in R\\)，即\\(X=Y\\)，矛盾 ##函数 接下来是函数基于二元关系的新定义。即\\(f: A\\mapsto D\\)可以理解为\\(A\\times D\\)的一个子集，满足每个A(定义域)中的元素只出现了一次 单射(injective)、双射(bijective)、满射(surjective)都很好理解，不说 函数的复合、求逆也很好理解，只需要注意复合运算的顺序就行了 再然后介绍了函数的函数(算子、泛函)的概念，提了一嘴函数式编程(functional programming)，只要知道lambda验算和图灵机是等价的应该就行了 需要注意的是，计算一个函数有时候是一件很难的事情(难以找到确定关系、复杂度不可接受) 还有非确定函数的概念(functionality)，这个在密码学中运用的比较广泛。比如给出一个输入\\(L\\)，输出一个长度为\\(L\\)的随机二进制串使得它满足一定的概率分布等等","tags":["ICS Intro"]},{"title":"数学分析01 实数完备性的六个定理","path":"/2021/01/01/Calculus01-实数完备性的六个定理/","content":"实数完备性的几个定理可以互相推导，这里给出了一个比较简单的完整推导链条 对于没有写到的推导可以通过旁敲侧击推导出这里的条件再继续（迂回战术） 1. 有界必有确界 如果\\(\\exists u\\)使得\\(\\forall x\\in S\\)都有\\(x\\le u\\),那么\\(S\\)有上确界 上确界：记\\(U=\\sup\\left\\{S\\right\\}\\)，则\\(\\forall x\\in S\\)都有\\(x\\le U\\)，且\\(\\forall \\epsilon&gt;0\\)，\\(\\exists x_0\\in S\\)使得\\(x_0&gt;U-\\epsilon\\) 用有限区间覆盖证明 \\(S\\)存在最大值的情况非常显然，它的上确界就是最大值；后文只讨论\\(S\\)不存在最大值的情况 反证法，假设\\(S\\)有界而没有上确界，记\\(S\\)上界的集合为\\(\\overline U\\) 则可以取\\(S\\)中的一个元素\\(L\\)，\\(\\overline U\\)中的一个元素\\(R\\)，得到一个闭区间\\(\\left[L,R\\right]\\) 考虑\\(x\\in [L,R]\\)，分成如下几种情况： \\(x\\in\\overline U\\) \\(x\\in S\\) \\(x ot\\in S\\)且\\(x ot\\in\\overline U\\) 对于情况1,由假设我们一定可以找到\\(x&#39;\\in\\overline U\\)且\\(x&#39;&lt; x\\)，使得\\(x&#39;\\)也是一个上界 此时我们为点\\(x\\)造一个开区间\\(\\left(x&#39;,2x-x&#39;\\right)\\)，这个区间内的点都是\\(S\\)的上界 对于情况2,由\\(S\\)不存在最大值可知我们一定能找到\\(x&#39;\\in S\\)且\\(x&#39;&gt;x\\) 此时我们为点x造一个开区间\\(\\left(2x-x&#39;,x&#39;\\right)\\)，这个区间内的点都不是\\(S\\)的上界 对于情况3,由x不是上界可知，必存在一个\\(x&#39; \\in S\\)使得 $x &lt; x' $，此时情况同2 于是我们为闭区间内的每一个点都配了一个开区间，这个开区间的集合覆盖了闭区间内的每一个点，由有限覆盖定理可知存在有限个开区间覆盖了\\([L,R]\\) 引理1:开覆盖中相邻两个开区间必相交 证明：假设存在不相交的开覆盖，则存在点未被覆盖，矛盾 由引理1可知\\([L,R]\\)上的开覆盖一定是环环相套的，从左起每一个开区间内的点都不是上界，从右起每一个开区间内的点都是上界，则可以推得中间存在一个开区间同时满足这两种情况(这是不可能的)，推得矛盾。于是原命题成立 2. 单调有界收敛 若数列\\(\\left\\{a_n\\right\\}\\)单调递增且有上界，则该数列收敛(存在极限) 用确界存在证明 有上界必有上确界，记\\(U=\\sup\\left\\{a_n\\right\\}\\)，则根据定义有\\(\\forall n\\left(U\\ge a_n\\right)\\)且\\(\\forall \\epsilon&gt;0,\\exists n_0\\)，有\\(U-\\epsilon &lt; a_{n_0}\\) 又\\(\\left\\{a_n\\right\\}\\)递增，于是取\\(N=n_0\\)，当\\(n\\ge N\\)，有\\(U-\\epsilon &lt; a_{n_0}\\le a_n \\le U &lt; U+\\epsilon\\)，这个就是数列收敛的定义，且恰好收敛于\\(U\\) 3. 闭区间套 考虑一个初始闭区间\\([L_0,R_0]\\)，我们取一系列闭区间\\([L_1,R_1],[L_2,R_2],\\dots\\)满足\\([L_1,R_1]\\subset[L_2,R_2]\\subset\\dots\\) 且有\\(\\lim\\limits_{i\\rightarrow +\\infty}{\\left(R_i-L_i\\right)}=0\\) 则\\(\\cap{[L_i,R_i]}=\\xi\\)，收敛于一个点 用单调有界收敛证明 由第一个条件可知，\\(\\left\\{L_n\\right\\}\\)单调递增，且有上界\\(R_0\\)，于是数列收敛 同理\\(\\left\\{R_n\\right\\}\\)也收敛，下面证明两个极限相等。 反证法：假设左右极限不相等，则\\(\\cap[L_i,R_i]=[\\sup\\left\\{L_n\\right\\},\\inf\\left\\{R_n\\right\\}]\\)，与条件2矛盾，故假设不成立 然后就做完了 4. 聚点 无穷项有界数列必有收敛子数列 用闭区间套证明 因为有界，必可以找到上下界，记值域区间为\\([L_0,R_0]\\) 取中点\\(M=\\frac{L_0+R_0}{2}\\)，则左右两个区间中，必存在至少一个区间包含了数列的无限项，记这个新的区间为\\([L_1,R_1]\\) 重复上述过程，则我们构造出了一个闭区间套，由闭区间套定理可知这个区间会收敛到一个点\\(\\xi\\)上，那么每次任意取\\(x_i\\in[L_i,R_i]\\)就可以得到一个收敛的子数列 5. 有限覆盖 考虑一个由若干开区间构成的集合\\(I\\)，若\\([L,R]\\subset\\cup I\\)，则一定可以从\\(I\\)中取出有限个开区间覆盖整个闭区间\\([L,R]\\) 用闭区间套证明 反证法：假设不能用有限个开区间覆盖\\([L,R]\\)，则取\\(M=\\frac{L+R}{2}\\)，左右两半至少有一个闭区间被无限个开区间覆盖 反复上述操作，则我们构造了一个闭区间套。且这个闭区间的长度可以任意小。而开区间集合中任意一个覆盖了\\(\\xi\\)的开区间长度都确定，得到矛盾，故假设不成立 6. 柯西收敛 数列收敛的充要条件是 \\(\\forall \\epsilon &gt; 0\\)，\\(\\exists N\\)，\\(\\forall x,y\\ge N\\)都有\\(|a_x-a_y|\\le \\epsilon\\) 可以发现这个判别法则和具体的极限无关，只关心数列本身的性质 用聚点证明 必要性比较简单，这里只证明充分性 先证明柯西数列有界。取\\(\\epsilon=1\\)，则\\(\\max\\left\\{a_1,a_2,\\dots,a_{N_{\\epsilon} },a_{N_{\\epsilon} }+1\\right\\}\\)是数列的一个上界，下界同理 有界数列必有收敛子数列，记子数列为\\(a_{n_1},a_{n_2},a_{n_3},\\dots\\)，其极限为\\(A\\)， 则\\(\\forall \\epsilon &gt;0\\), \\(\\exists K &gt;0\\)，当\\(k&gt; K\\)时\\(|a_{n_k}-A|\\le \\epsilon\\) 根据定义，取\\(N&#39;=\\max\\left\\{n_K,N\\right\\}\\)，令\\(x=N&#39;\\)，则\\(\\forall y&gt; N&#39;\\)都有\\(|a_y-A|=|a_y-a_x+a_x-A|\\le|a_y-a_x|+|a_x-A|\\le2\\epsilon\\)","tags":["数学分析"]},{"title":"Compiler01 Introduction","path":"/2020/12/26/Compiler01-Introduction/","content":"看的是英文版的龙书,各种翻译看看就好... 大概前半段是课本，后半段是网课的笔记。因为课程好像都不讲Chap. 2，所以下一节的笔记大概就没有课堂内容了 1.1 Language Processors 词汇: intermediate 中间的 compile 编译 interpret 解释 assemble 汇编 relocatable 可重定位的 编译器指的是将源代码翻译成等价的另一种语言的程序的程序(饶舌) 编译器的一项重要职责是检测error 编译语言的编译和执行是分开的: 源代码-&gt;[编译]-&gt;程序 输入-&gt;[执行程序]-&gt;输出 解释器是另一种语言处理器,解释语言没有编译过程: 源代码+输入-&gt;[解释]-&gt;输出 通常来讲编译器要比解释器快得多(显然) 而解释器通常能更好的检测error(解释器一步步执行) Java语言结合了翻译(Translate)和解释(Interpret): 源代码-&gt;[翻译]-&gt;字节码(bytecode) 字节码+输入-&gt;[虚拟机]-&gt;输出 这里的字节码是一种中间程序(intermediate program),其好处在于可以在编译后跨平台解释 有些java编译器直接把字节码翻译成机器语言来加速 从源代码到可执行文件之间不止需要编译器一个程序(过程) 源代码(source code)-&gt;[预处理器(preprocessor)]-&gt;修改过的源代码(modified sourcecode)-&gt;[编译器(compiler)]-&gt;汇编语言(assembly program)-&gt;[汇编(assembler)]-&gt;可重定位机器码(relocatable machine code)-&gt;[链接(Linker/Loader)]-&gt;目标机器码(target machine code) 通常情况下源代码有多个,它们先被预处理器处理(宏展开 代码替换等)得到修改过的源代码 修改过的源代码然后被给了编译器,编译后得到汇编程序(较抽象 方便调试) 汇编程序传给汇编器后得到relocatable machine code.编译过程中可能会有多个源代码,这时候就要把它们(还有一些需要的部分)链接起来,最后才得到可执行的程序(executable program) Exercise for 1.1 编译器和解释器的区别？ 笔记里有 编译器和解释器的优劣？ 笔记里有 编译器为什么产生汇编程序而不直接得到机器语言？ Assembly language is easier to produce as output It is easier to debug (there may be more than one source codes) 把一种高级语言翻译成另一种高级语言的的编译器叫做“source-to-source translator”.那么用C作为目标语言的编译器有什么好处？ C语言比较通用、易于阅读(compared to other intermediate languages)...C语言的优势都算 描述一下assembler的任务 笔记里有 1.2 The Structure of a Compiler 词汇: grammar 语法 lexical 词法的 syntax syntactical 句法 句法的 semantic 语义的 synthesis 合成 constituent 组成的 impose 施加 sound 合理的 coercions 强制类型转换 judicious 明智的 phase 阶段 pass 通路 syntax-directed translation 前面讲了整个编译的过程,后面主要讲Compiler这一部分 编译可以分成两个部分:分析(Analysis)和合成(Synthesis) Analysis: compiler的前端(front end) 把源代码分解成若干小部分,再用这些部分构建语法结构,并利用语法结构来建立一种中间表达(Intermediate Representation = IR) 如果Analysis过程中检测到了error(语法错误、语义错误)compiler能够及时报错 同时Analysis过程中compiler会收集一些信息储存在symbol table(一种数据结构)中,并连同IR一起传递给Synthesis过程 ##　Synthesis: compiler的后端(back end) 接收IR和symbol table,并据此产生目标程序 compiler的流程一套下来大概是这样的: characters-&gt;lexical analysis-&gt;tokens-&gt;syntax analysis-&gt;syntax tree(语法树)-&gt;semantic analysis-&gt;decorated syntax tree-&gt;Intermediate code generator-&gt;IR-&gt;optimizer1-&gt;IR-&gt;code generator-&gt;machine code-&gt;optimizer2-&gt;machine code 其中symbol table全程在线,随时可用 流程里的optimizer不一定都有,不一定没有,都属于可选项 Lexical Analysis 这是compiler的第一部分,这个部分compiler会读取字符流中的字符,然后得到若干词语(lexeme)组成的序列 每个lexeme都可以表示为形式如下的token: &lt;token-name, value&gt; 书中举了这样一个例子: position = initial + rate * 60; 就会被翻译成类似 &lt;id, 1&gt; &lt;=&gt; &lt;id, 2&gt; &lt;+&gt; &lt;id, 3&gt; &lt;*&gt; &lt;num, 60&gt; 这样的语句.此时symbol table就会变成这个样子 1 position 20 2 initial 12 3 rate 0.25 我们看到的 &lt;id, x&gt; 就可以理解成从symbol table中的第x个元素,其余的类似(如果学过汇编可能会理解得好一点) 如果出现了无法识别的字符/词语,那么编译器就会报错 否则Lexical Analysis就会把若干这样的tokens传递给下一个环节 Syntax Analysis/Parsing 在这一环节parser会利用前面的tokens建立一个树形结构来表示语法结构 一个例子是语法树(syntax tree).语法树的每一个节点都代表一个操作(operator),节点的儿子是操作的对象. 语法树的结构有利于维持传统运算的优先级顺序,也就是说它直观上是很好理解的 Semantic Analysis 这一环节semantic analyzer会根据语言的规范，利用语法树和symbol table中的信息来检查源代码的语义.一个具体的检查例子就是typecheck 有的时候语言还有隐式的类型转换，这种时候compiler就需要判断出转换的类型，一个简单的例子是(int x = 60.0 * 2;) 同时，这一阶段还会收集变量的类型等各种信息用于后续过程 IR Generation 在编译过程中可能出现不止一种、不止一次IR，语法树只是其中常用的一种 在词法分析、语法分析、语义分析后，compiler通常会产生一种特殊的IR 它较低级(low-level)，靠近机器语言(machine-like)，所以 容易生成 容易翻译成目标机器语言 3AC 3AC(3-adress code)是IR一个例子 (这个东西的形式很像寄存器的各种操作，通常有各种中间变量) 一条常见的3AC指令形如: A = B opt C 至多一个操作符(可以没有:赋值操作)，至多两个操作变量 所有操作按固定顺序执行 Opimization 喜闻乐见的优化环节 优化分两种，机器无关(machine dependent)/机器相关(machine independent)，通常指的是运行速度优化 优化还可能包括(更少的能耗/更小的程序) 对IR的优化属于机器无关的优化(为什么?)，其意图在于减少语句的同时保证所有的语句仍然符合IR规范 现代编译器的区别主要在于optimization环节，致力于optimization的compiler被称为\"optimizing compilers\" Code Generation 这个环节compiler将IR转变为机器代码(直接操作寄存器、内存)的序列 这一步的重点在于明智地分配寄存器和高效代码的生成 本书到目前为止忽略了identifier(变量)的储存分配，现在只需要知道这在生成IR的环节或这一环节完成就行 Symbol Table Management 这个数据结构记录了变量、变量类型、使用位置、函数等等信息，并且应该支持快速的查找和修改各种信息 (怎么看怎么像BST囧,哪来的table) Grouping Phases into Passes 一个流程的各逻辑阶段叫做Phases。在实际操作中可能会把多个Phases同时实现(例如同时实现词法和语法分析) 一个读入一段文件并输出一段文件的流程叫做Pass，是流程的实际阶段分割 这一段讲的是compiler的组织结构和模块化 在上面把compiler分为front end和back end的好处在于 可以用一个语言的前端配上不同机器的后端 可以用不同语言的前端配上特定机器的后端 语言发展史 机器语言-&gt;汇编-&gt;Fortran、Cobol、Lisp、C、C++-&gt;NOMAD、SQL、Postscript-&gt;Prolog、OPS5 一些概念(哪哪都是这几位) 命令式语言(Imperative) 描述操作过程的语言 声明式语言(Declarative) 描述操作目的的语言 冯诺依曼语言 所有以冯诺依曼体系结构为基础的语言都是。基于lambda验算的语言则不是 脚本语言 逐行执行，无需全文编译即可运行的语言 Name&amp;Variable&amp;Identifier Identifier是一段字符，用来表示某个实体(entity) Name指的是代码中的某个静态实体，可以是函数/变量/其它。所有的Id都是Name，但Name不都是Id(例如Id.Id也可以表示一个Name) Variable用来指代运行时Name的动态含义(某个Name究竟指内存中的哪段数据)。 静态/动态 如果一个问题需要运行程序才能得到答案，那么这个问题就是\"动态的\"，否则是\"静态的\" 一个经典的问题就是静态/动态的作用域。作用域设计的问题是：在某处提及的Name x究竟是在何处被声明的 x？ 静态的作用域很好理解；动态作用域的一个例子出现在OOP中。例如对于父类和子类都实现了的方法method()，x.method();究竟dispatch到哪一个，就依赖于x动态指向的对象。 Environment&amp;State 环境：从Name到储存位置的映射(左值) 状态：从储存位置到值的映射(右值) 那么上面说到的\"OOP中函数的作用域由环境决定\"的说法就可以理解了，即类中函数的作用域由receiver object在储存中的位置决定。 从这个角度看，Allocation Site Abstraction就是在用Name对Variable建模","tags":["Compiler"]},{"title":"几个关于集合的有趣证明","path":"/2020/11/23/几个关于集合的有趣证明/","content":"在离散数学的第一堂课就被介绍了Set和Proper Class的区别。 与高中内容不同，现代数学中并不是任取一些元素都能组成一个Set，某些东西归在一起只能形成Proper Class（因为这个东西不满足集合的一些性质） 根据ZFC公理系统，我们可以知道关于集合的若干条公理，也就是说满足这些的才是集合，而推断某个东西不是集合常用反证的方法，下面来看几个栗子 Prove that the set of all sets is a proper class. 这个比较简单。假设这东西是一个集合，记为 \\(S\\)，那么根据Cantor's Theorem我们有 \\(|S|&lt;|2^S|\\)。 而根据幂集公理(Axiom of power set)可知，\\(2^S\\) 也是一个集合，于是有 \\(2^S\\subset S\\)，也就是 \\(|2^S|\\le |S|\\)，矛盾 Prove that the set of all cardinals is a proper class. 这周的作业，想了很久。。 首先要知道任意多个集合的并、一个集合的幂集都是集合，并且任意集合都唯一对应着一个cardinal，知道这些就比较好做了。 和上面的方法类似，假设这是一个集合 \\(S\\)，那么根据并集公理(Axiom of union)我们有 \\(T=\\cup S\\) 也是一个集合，于是 \\(|T|\\in S\\)。 因为 \\(\\forall x\\in S\\) 都有 \\(x\\subset T\\)，于是得到 \\(\\forall x\\in S\\ \\ \\ |x|\\le |T|\\) 然而 \\(|T|&lt;|2^T|\\in S\\)，这就推出了一个矛盾。 关于the set of all ordinals是一个 proper class的证明和这个类似 Prove that for cardinals \\(A\\) and \\(B\\)，\\(A+B=\\max(A,B)\\) where at least one of them is infinite 网上有很多证明，然而我都看不太懂，希望有别的做法的朋友可以交流交流;-P 不妨设\\(A\\ge B\\)，那么显然有\\(A\\le A+B\\le A+A\\)，我们只需要证明\\(A=A+A\\)即可，注意这里的A是cardinal 由于A是一个cardinal，那么它同时也是一个ordinal。对于任意ordinal \\(x\\)，\\(\\exist \\alpha,\\beta\\ \\ s.t.\\ \\ x=\\alpha+\\beta\\)，其中\\(\\alpha\\)是一个limit ordinal，\\(\\beta\\in\\mathbb N\\) 考虑这样一个映射\\(f(\\alpha+\\beta)=\\left\\{\\begin{aligned}\\left(0,\\alpha+\\beta\\right)&amp;,&amp;\\beta &amp;=2k\\\\\\left(1,\\alpha+\\beta\\right)&amp;,&amp;\\beta&amp;=2k+1\\end{aligned}\\right. \\left(k\\in\\mathbb Z\\right)\\) 显然\\(\\forall x\\in A\\)，\\(x\\) is an ordinal. 那么\\(A=A+A\\)等价于证明\\(|A|=|\\left(\\left\\{0\\right\\}\\times A\\right)\\cup\\left(\\left\\{1\\right\\}\\times A\\right)|\\) 不难发现\\(f:A\\mapsto\\left(\\left\\{0\\right\\}\\times A\\right)\\cup\\left(\\left\\{1\\right\\}\\times A\\right)\\)是一个双射，于是就证明完了。","tags":["Eureka Moments"]},{"title":"有关集合大小的比较","path":"/2020/10/28/有关集合大小的比较/","content":"写在前面 今天上的离散数学做了一些有意思的证明，这里放一下 集合的大小，我知道 在对付有限集合时，我们很容易就能比较两个集合的大小（只需要数一数各自有多少个元素就行了）。但是当这个问题拓展到无限集合时，我们往往不能简单地给出答案。原因是什么呢？ 问题1：证明\\(|\\mathbb{N}|\\)=\\(|\\mathbb{E}|\\)，其中\\(\\mathbb{E}=\\left\\{2n|n\\in\\mathbb{N}\\right\\}\\) 定义映射\\(f:\\mathbb{N}\\mapsto\\mathbb{E}\\)为\\(f(x)=2x\\)，通过这样一个操作我们可以发现\\(\\forall x\\in\\mathbb{N},\\exist y\\in\\mathbb{E},f(x)=y. And \\ \\forall y\\in\\mathbb{E},\\exist x\\in\\mathbb{N},f(x)=y\\) 也就是说，我们建立起了一个从自然数到偶数的双射，任意一个偶数都唯一确定对应一个整数，反之亦然。 这时，我们就说这两个集合的大小相等，也称他们是等势的 上面的这个例子比较简单，我们可以接着看下一个例子 问题2：证明\\(|\\mathbb{R}|=|[0,1]|\\) 这个问题乍一看是反直觉的，但是有了上面问题的铺垫我们可以知道一件事情：一个无限集是可以和它的一个真子集等势的。 先说说我的做法。一个有趣的证明是这样的： 我们把区间\\([0,1]\\)卷起来形成一个半圆，不难得到它的半径\\(r=\\frac{1}{\\pi}\\) 在数轴上取一个点，不妨就取0，过0作一直线垂直于数轴，在这条直线上取一点\\(A\\)使得点到数轴的距离恰好为r 以点A为圆心，作半圆与数轴相切，不难得到这个半圆的长度恰好为1 在数轴上任取一点\\(D(x,0)\\)，连接\\(D\\)、圆心\\(A\\)，交半圆于\\(E\\) 如上图，可以发现，数轴上的任意一点都唯一对应着半圆上的一个点，而半圆上的任意一点（除去两端）也恰好唯一地对应了数轴上的一点 因此我们断言\\(|(0,1)|=|\\mathbb{R}|\\) 等等，你不是要证明\\(|[0,1]|=|\\mathbb{R}|\\)吗 是的，所以我们接下来要做的是证明\\(|[0,1]|=|(0,1)|\\) 如何下手？考虑这样集合\\(A=\\left\\{\\frac{1}{2},\\frac{1}{4}...\\frac{1}{2^n}...\\right\\}\\)和\\(B=\\left\\{0,1,\\frac{1}{2},\\frac{1}{4}...\\frac{1}{2^n}...\\right\\}\\) 注意！这里的\\(A\\)和\\(B\\)都是无限集 不难发现，我们将A和B分别一一列举了出来，也就是说他们是可列集合，也叫可数集，这样的两个集合是可以一一对应的，我们只需要把\\(A\\)中的第i个元素映射到\\(B\\)中的第i个元素，这样就造出了两个集合间的一个双射 我们又注意到\\([0,1]\\backslash B=(0,1)\\backslash A\\)，这一点是非常显然的，那么就有\\(|[0,1]\\backslash B|=|(0,1)\\backslash A|\\) 这样我们把A和B分别劈成了两半，使得这两半之间存在双射，所以只需要把两个互不相干的映射并在一起就可以得到\\(f:(0,1)\\mapsto[0,1]\\) 故\\(|\\mathbb{R}|=|[0,1]|\\)，并且易得\\(\\forall x,y\\in\\mathbb{R} 若x eq y,|[x,y]|=|(x,y)|=|(x,y]|=|[x,y)|=|\\mathbb{R}|\\) 这个证明看起来非常直观，但是关键步骤缺乏严密性。毕竟我们无法严谨地说明把线段弯曲后它和原来仍然是一样的（笑 但是观察这个证明，我们发现这等价于利用函数\\(f(x)=\\tan\\left(\\pi \\left(x-\\frac{1}{2}\\right)\\right)\\)，将\\(\\mathbb{R}\\)映射到\\((0,1)\\)，并且由单调性可知这样的映射是双射，所以上述证明可以用更加严谨的方式进行说明，是没有问题的。 问题3：证明\\(|\\mathbb{R}^2|=|\\mathbb{R}|\\) 这个平方指集合和他自身的笛卡尔积，即设\\(A\\)为一非空集合，则\\(A^2=\\left\\{(x,y)|x\\in A,y\\in A\\right\\}\\) 由前一题可知，\\(|\\mathbb R|=|[0,1)|\\) 于是我们只需要证明\\(|[0,1)|=|[0,1)^2|\\) 观察可知，\\([0,1)\\)中的实数都是整数部分为0的小数，我们可以构造这样一个映射 \\[\\begin{aligned} f(a,b)=\\sum_{i=1}^{+\\infin}\\left(\\frac{[a\\times 10^{2i-1}]}{10^{2i-1} }+\\frac{[b\\times 10^{2i}]}{10^{2i} }\\right) \\end{aligned}\\] 其中\\([x]\\)表示取\\(x\\)的整数部分，不难发现这样就是在把\\(a、b\\)的每一位交替插在一起，形成一个新的实数 这个构造过程是可逆的，并且是一个双射 这个结论能说明什么呢？我们在二维平面上的点和数轴上一样多！很奇妙 并且这个结论是可以推广的。也就是\\(\\forall n\\in \\mathbb{N},|{\\mathbb R}^n|=|\\mathbb R|\\) 而\\(|{\\mathbb R}^{+\\infin}|=|\\mathbb R|\\)也是可以得出的，我们只需要按行列出实数的每一位，然后沿着反对角线取数字构造就可以了 问题4：证明\\(|2^{\\mathbb{N} }|=|\\mathbb{R}|\\) 这个幂次的含义是自然数的幂集，也就是自然数所有子集形成的集合。 由上一问可知，我们只需要证明\\(|2^{\\mathbb{N} }|=|[0,1)|\\) 而对于任意一个\\(\\mathbb N\\)的子集\\(S\\)，我们都可以用一个无限长度的二进制串来唯一表示，其中若第i位为0，代表\\(i ot\\in S\\)，否则\\(i\\in S\\) 于是立刻就可以想到，我们只需要把\\([0,1)\\)中的实数化成二进制小数，就可以获得无线多个互不相同的任意长的二进制串（取小数点后的部分） 例如，一个实数\\(a=0.1101100000...\\)，唯一地表达了集合\\(S=\\left\\{1,2,4,5\\right\\}\\) 但是，这样的表示是完美的吗？ 并不！考虑一个无限循环小数\\(a=0.011000\\dot{1}\\)，我们可以发现它其实等价于有限小数\\(b=0.011001\\)，也就是说，我们这样存在一个实数映射到了两个不同的\\(\\mathbb N\\)的子集。当然这个很好解决，我们只需要规定若一个数能表示成有限小数，那么我们绝不考虑它的无限小数表示法。但是，我们无法通过这样的映射找到一个逆映射，使得\\(\\mathbb N\\)中的每一个子集都能在\\([0,1)\\)中找到唯一对应的实数（例如，我们把从9开始往后的所有正整数都选出来，组成的集合就不能映射到一个实数上，因为这是一个无限小数，而前面我们规定了不取这种写法） 怎么办呢？这里有一个比较奇妙的做法。考虑这样一个事实，我们在第一问证明了\\(|\\mathbb N|=|\\mathbb E|\\)，因此我们似乎只需要证明\\(|\\mathbb E|=|2^{\\mathbb N}|\\)就可以了！ 观察\\(\\mathbb E\\)的性质，我们可以发现对它做上述构造后，是不存在与另一个有限小数相等的无限循环小数的！为什么呢？因为这里都是偶数，因此最多出现\\(0.00010101010101...\\)这样的循环小数 因此这个构造就成立了！我们就成功地证明了\\(|2^{\\mathbb N}|=|2^{\\mathbb E}|=|[0,1)|=|\\mathbb R|\\) 小小的总结 离散数学这门课听起来还是很有意思的，但是真正要做题的时候就很懵逼了，算是很需要idea的学科 经过了几次课大概可以感受到离散数学主要研究的问题无非就是围绕着集合开展，并且多和无限扯上关系。因为和无限扯上了关系，我们初等数学中的一些结论就变得不那么显然和可拓展了，原本的直觉也会失效。 希望能活过大一的离散数学（一），然后看看能不能活过离散（二）吧(希望)","tags":["Eureka Moments"]},{"title":"2020 ICPC 小米邀请赛 部分题解","path":"/2020/10/26/2020-ICPC-小米邀请赛-部分题解/","content":"这是oi退役以来第五场比赛，也是和队友打的第三场，前面零星有几次个人赛。 水平不太够，不过个人比较佛系，希望把能看出来的题目肝出来就行了，目标就是争取不做演员;-P A A是我写的 给n个数，取出最大的子集，使得子集中任意两个数a,b满足要么a|b，要么b|a 这题卡了很久，主要是一开始就想到了和n相关的做法和和值域相关的做法，但是两个单独都过不了 最后想起来这个是可以讨论的，把两种做法一结合就过了 考虑设f[x]表示x为最大数的最大集合大小，那么新加入的数字必须是x的整数倍，我们枚举x的倍数就可以了，这里是1e7log1e7的 同理，设g[x]表示第x大的数字为最大数的最大集合大小，那么它一定由前n-1个状态转移来，所以就是n^2的 B B是我写的 给定一个二维平面，给k个线段，问从起点坐标走到终点坐标的最短路，行走时不能跨越线段 想象我们用弹力绳从起点拉，一直拉到终点，那么绳子会收缩，此时绳子的长度就是答案，而且绳子一定经过了若干线段的端点 所以把线段拆成两个点，然后n^2连边，跑最短路就可以了。因为INF开小了挂了，这是我的锅，得背 C sb题目，一整段长为n的w，答案就是2n-1，数数就好了 D 这个是队友写的 考虑一个点什么情况下删掉会增加连通块。不难发现当一个点是一个连通分量连出去的点时，我们删掉他会新增连通块。也就是说，这个点的所有儿子的low都不比他小。 E 留坑 F 做的时候看错题了，我的锅 不难发现答案可以二分。先排除全选ri都不能到L和全选li都过了R的两种情况 假设我们二分到的答案是mid，那么每类题目我们至少需要mid*li个，并且剩余max(L,sum[li])-mid*li道题目需要去凑齐。这个直接看看sum[ri-li]够不够就行了 这题LL不够，看了别人的题解用__int128才够 G 留坑 H 留坑 I 水题，模拟就好了 J 我写的 考虑从左上到右下找第一个不为零的位置，我们对这个点为左上角的矩阵不停-1，这样能保证不会遗漏 然后我们check是否最后都变成了0，或者减成了负数 这是一个区间加，单点求的问题，我们差分然后二维树状数组就好了 K 留坑待填","tags":["XCPC"]},{"title":"Hello World!","path":"/2020/10/26/Hello-World/","content":"脱离了高中生活，进入大学的也接触了一些有意思的事情。 希望这个博客的存在意义在于写一些有趣的东西，记录一些好玩的事情，而不仅仅只是放一些算法题解 目前想法还有很多，最近这一个月遇到的人和事情都给了我很多灵感，根据某学长“想法不记录下来就等于没想”的说法， 我决定记录一下，写点东西，有点输出，这样也好回头看看自己的成长历程。 那就这样吧！Hello World！我的新博客","tags":["Eureka Moments"]},{"title":"tags","path":"/about/index.html","content":"About me 喜欢PL，爱写代码，愿意讲废话，偶尔愿意听人讲废话 只要你愿意看我的文字，我就会很高兴 Education 2024.09 - today: M.Sc. in Computer Science, Nanjing University. 2020.09 - 2024.06: B.S. in Information and Computing Science, Nanjing University."},{"title":"friends","path":"/friends/index.html","content":""},{"title":"tags","path":"/tags/index.html","content":""}]